share_vocab: true
src_vocab: "/root/projects/zindi/nmt_train/models/dictionary2.txt"
src_words_min_frequency: 1
src_vocab_size: 270987
tgt_vocab: "/root/projects/zindi/nmt_train/models/dictionary2.txt"
tgt_words_min_frequency: 1
tgt_vocab_size: 270987
vocab_size_multiple: 1
decoder_start_token: '</s>'
#### Subword
src_subword_model: "/root/projects/zindi/nmt_train/models/flores200_sacrebleu_tokenizer_spm2.model"
tgt_subword_model: "/root/projects/zindi/nmt_train/models/flores200_sacrebleu_tokenizer_spm2.model"
src_subword_nbest: 1
src_subword_alpha: 0.0
tgt_subword_nbest: 1
tgt_subword_alpha: 0.0
# Corpus opts:
# Training files
data:
    corpus_1:
        path_src: "/root/projects/zindi/nmt_train/data/all_dyu.txt"
        path_tgt: "/root/projects/zindi/nmt_train/data/all_fr.txt"
        transforms: [sentencepiece, prefix, suffix, filtertoolong]
        weight: 10
        src_prefix: "</s> dyu_Latn"
        tgt_prefix: "fra_Latn"
        src_suffix: ""
        tgt_suffix: ""
    # valid:
    #     path_src: /content/drive/MyDrive/zindi_opt/nmt/training/MT-Preparation/data/all_dyu.txt-filtered.dyu
    #     path_tgt: /content/drive/MyDrive/zindi_opt/nmt/training/MT-Preparation/data/all_fr.txt-filtered.fr
    #     transforms: [sentencepiece, prefix, suffix, filtertoolong]
    #     weight: 10
    #     src_prefix: "</s> dyu_Latn"
    #     tgt_prefix: "fra_Latn"
    #     src_suffix: ""
    #     tgt_suffix: ""
update_vocab: true
train_from: "/root/projects/zindi/nmt_train/models/nllb-200-600M-zindi_V3_step_1000.pt"
reset_optim: all
save_data: "/root/projects/zindi/nmt_train/train"
save_model: "/root/projects/zindi/nmt_train/models/nllb-200-600M-zindi_V4"
log_file: "/root/projects/zindi/nmt_train/trainnllb-200-600M-zindi_V4.log"
keep_checkpoint: 3
save_checkpoint_steps: 1000
average_decay: 0.0005
seed: 1234
report_every: 10
train_steps: 10000
# valid_steps: 100
# Batching
bucket_size: 256
num_workers: 4
prefetch_factor: 2
world_size: 1
gpu_ranks: [0]
batch_type: "tokens"
batch_size: 1024
# valid_batch_size: 8
batch_size_multiple: 1
accum_count: [8, 8, 8]
accum_steps: [0, 15000, 30000]
# Optimization
model_dtype: "fp16"
optim: "sgd"
learning_rate: 30
warmup_steps: 100
decay_method: "noam"
adam_beta2: 0.98
max_grad_norm: 0.5
label_smoothing: 0.1

param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
override_opts: true
encoder_type: transformer
decoder_type: transformer
enc_layers: 24
dec_layers: 24
heads: 16
hidden_size: 1024
word_vec_size: 1024
transformer_ff: 4096
add_qkvbias: true
add_ffnbias: true
dropout_steps: [0, 15000, 30000]
dropout: [0.1, 0.1, 0.1]
attention_dropout: [0.1, 0.1, 0.1]
share_decoder_embeddings: true
share_embeddings: true
position_encoding: true
position_encoding_type: 'SinusoidalConcat'

# Filter out source/target longer than n if [filtertoolong] enabled
src_seq_length: 120
src_seq_length: 120

# Stop training if it does not imporve after n validations
early_stopping: 10

# Use standard scaled dot-product attention
self_attn_type: scaled-dot