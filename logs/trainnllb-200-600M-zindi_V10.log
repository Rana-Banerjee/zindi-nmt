[2024-07-29 10:07:37,444 INFO] Parsed 1 corpora from -data.
[2024-07-29 10:07:37,444 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 10:07:38,893 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-29 10:07:38,894 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 10:07:38,934 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:07:38,934 INFO] The decoder start token is: </s>
[2024-07-29 10:07:38,971 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 10:07:38,971 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 10:07:39,003 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:07:39,003 INFO] The decoder start token is: </s>
[2024-07-29 10:07:39,006 INFO] Over-ride model option set to true - use with care
[2024-07-29 10:07:39,006 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 10:07:39,006 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 10:07:39,006 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 10:07:39,006 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 10:07:39,006 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:07:39,006 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:07:39,007 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:07:39,007 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:07:39,007 INFO] Option: src_seq_length , value: 120 overriding model: 150
[2024-07-29 10:07:39,007 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 10:07:39,007 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:07:39,007 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:07:39,007 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 10:07:39,007 INFO] Option: enc_layers , value: 24 overriding model: 12
[2024-07-29 10:07:39,007 INFO] Option: dec_layers , value: 24 overriding model: 12
[2024-07-29 10:07:39,007 INFO] Option: bucket_size , value: 1048576 overriding model: 262144
[2024-07-29 10:07:39,007 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V10 overriding model: nllb
[2024-07-29 10:07:39,007 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 10:07:39,007 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 10:07:39,007 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 10:07:39,007 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 10:07:39,007 INFO] Option: batch_size , value: 2048 overriding model: 8192
[2024-07-29 10:07:39,007 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 10:07:39,007 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:07:39,007 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 10:07:39,007 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 10:07:39,007 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 10:07:39,007 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 10:07:39,007 INFO] Option: dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:07:39,007 INFO] Option: attention_dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:07:39,008 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:07:39,008 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 10:07:39,008 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 10:07:39,008 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 10:07:39,008 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 10:07:39,008 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V10.log overriding model: 
[2024-07-29 10:07:39,008 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 10:07:39,008 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'filtertoolong', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-29 10:07:39,008 INFO] Building model...
[2024-07-29 10:07:45,084 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 10:07:45,084 INFO] Non quantized layer compute is fp16
[2024-07-29 10:07:45,084 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 10:07:45,292 INFO] src: 14783 new tokens
[2024-07-29 10:07:45,630 INFO] tgt: 14783 new tokens
[2024-07-29 10:07:46,558 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-23): 24 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 10:07:46,566 INFO] encoder: 326900736
[2024-07-29 10:07:46,566 INFO] decoder: 403146189
[2024-07-29 10:07:46,566 INFO] * number of parameters: 730046925
[2024-07-29 10:07:46,569 INFO] Trainable parameters = {'torch.float32': 730046925, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:07:46,569 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:07:46,569 INFO]  * src vocab size = 24013
[2024-07-29 10:07:46,569 INFO]  * tgt vocab size = 24013
[2024-07-29 10:07:46,912 INFO] Starting training on GPU: [0]
[2024-07-29 10:07:46,912 INFO] Start training loop without validation...
[2024-07-29 10:07:46,912 INFO] Scoring with: None
[2024-07-29 10:13:21,404 INFO] Parsed 1 corpora from -data.
[2024-07-29 10:13:21,404 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 10:13:23,817 WARNING] configured transforms is different from checkpoint: +{'prefix', 'sentencepiece', 'suffix'}
[2024-07-29 10:13:23,817 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:13:23,856 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:13:23,857 INFO] The decoder start token is: </s>
[2024-07-29 10:13:23,889 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 10:13:23,890 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:13:23,932 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:13:23,932 INFO] The decoder start token is: </s>
[2024-07-29 10:13:23,936 INFO] Over-ride model option set to true - use with care
[2024-07-29 10:13:23,936 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 10:13:23,936 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 10:13:23,936 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 10:13:23,936 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 10:13:23,936 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:13:23,936 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:13:23,936 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:13:23,936 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:13:23,936 INFO] Option: src_seq_length , value: 120 overriding model: 150
[2024-07-29 10:13:23,936 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 10:13:23,936 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:13:23,936 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:13:23,937 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 10:13:23,937 INFO] Option: enc_layers , value: 24 overriding model: 12
[2024-07-29 10:13:23,937 INFO] Option: dec_layers , value: 24 overriding model: 12
[2024-07-29 10:13:23,937 INFO] Option: bucket_size , value: 524288 overriding model: 262144
[2024-07-29 10:13:23,937 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V10 overriding model: nllb
[2024-07-29 10:13:23,937 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 10:13:23,937 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 10:13:23,937 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 10:13:23,937 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 10:13:23,937 INFO] Option: batch_size , value: 2048 overriding model: 8192
[2024-07-29 10:13:23,937 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 10:13:23,937 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:13:23,937 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 10:13:23,937 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 10:13:23,937 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 10:13:23,937 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 10:13:23,937 INFO] Option: dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:13:23,937 INFO] Option: attention_dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:13:23,937 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:13:23,937 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 10:13:23,937 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 10:13:23,938 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 10:13:23,938 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 10:13:23,938 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V10.log overriding model: 
[2024-07-29 10:13:23,938 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 10:13:23,938 INFO] Option: _all_transform , value: {'prefix', 'sentencepiece', 'suffix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-29 10:13:23,938 INFO] Building model...
[2024-07-29 10:13:29,819 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 10:13:29,819 INFO] Non quantized layer compute is fp16
[2024-07-29 10:13:29,819 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 10:13:29,998 INFO] src: 14783 new tokens
[2024-07-29 10:13:30,309 INFO] tgt: 14783 new tokens
[2024-07-29 10:13:31,277 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-23): 24 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 10:13:31,286 INFO] encoder: 326900736
[2024-07-29 10:13:31,287 INFO] decoder: 403146189
[2024-07-29 10:13:31,287 INFO] * number of parameters: 730046925
[2024-07-29 10:13:31,289 INFO] Trainable parameters = {'torch.float32': 730046925, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:13:31,289 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:13:31,289 INFO]  * src vocab size = 24013
[2024-07-29 10:13:31,289 INFO]  * tgt vocab size = 24013
[2024-07-29 10:13:31,614 INFO] Starting training on GPU: [0]
[2024-07-29 10:13:31,614 INFO] Start training loop without validation...
[2024-07-29 10:13:31,614 INFO] Scoring with: None
[2024-07-29 10:17:35,396 INFO] Step 10/10000; acc: 23.7; ppl: 478.0; xent: 6.2; lr: 0.00069; sents:   16394; bsz:  998/1273/51; 1311/1671 tok/s;    244 sec;
[2024-07-29 10:19:37,461 INFO] Step 20/10000; acc: 33.8; ppl: 171.1; xent: 5.1; lr: 0.00131; sents:   16171; bsz: 1012/1291/51; 2653/3384 tok/s;    366 sec;
[2024-07-29 10:21:46,836 INFO] Parsed 1 corpora from -data.
[2024-07-29 10:21:46,836 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 10:21:48,512 WARNING] configured transforms is different from checkpoint: +{'suffix', 'prefix', 'sentencepiece'}
[2024-07-29 10:21:48,512 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', 'dyu_Latn', ''], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:21:48,546 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:21:48,546 INFO] The decoder start token is: </s>
[2024-07-29 10:21:48,572 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 10:21:48,572 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', 'dyu_Latn', ''], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:21:48,605 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:21:48,605 INFO] The decoder start token is: </s>
[2024-07-29 10:21:48,609 INFO] Over-ride model option set to true - use with care
[2024-07-29 10:21:48,609 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 10:21:48,609 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 10:21:48,609 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 10:21:48,609 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 10:21:48,609 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:21:48,609 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:21:48,610 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:21:48,610 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:21:48,610 INFO] Option: src_seq_length , value: 120 overriding model: 150
[2024-07-29 10:21:48,610 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 10:21:48,610 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:21:48,610 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:21:48,610 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 10:21:48,610 INFO] Option: enc_layers , value: 24 overriding model: 12
[2024-07-29 10:21:48,610 INFO] Option: dec_layers , value: 24 overriding model: 12
[2024-07-29 10:21:48,610 INFO] Option: bucket_size , value: 524288 overriding model: 262144
[2024-07-29 10:21:48,610 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V10 overriding model: nllb
[2024-07-29 10:21:48,610 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 10:21:48,610 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 10:21:48,610 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 10:21:48,610 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 10:21:48,610 INFO] Option: batch_size , value: 2048 overriding model: 8192
[2024-07-29 10:21:48,610 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 10:21:48,610 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:21:48,610 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 10:21:48,610 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 10:21:48,611 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 10:21:48,611 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 10:21:48,611 INFO] Option: dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:21:48,611 INFO] Option: attention_dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:21:48,611 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:21:48,611 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 10:21:48,611 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 10:21:48,611 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 10:21:48,611 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 10:21:48,611 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V10.log overriding model: 
[2024-07-29 10:21:48,611 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 10:21:48,611 INFO] Option: _all_transform , value: {'suffix', 'prefix', 'sentencepiece', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-29 10:21:48,611 INFO] Building model...
[2024-07-29 10:21:54,866 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 10:21:54,866 INFO] Non quantized layer compute is fp16
[2024-07-29 10:21:54,866 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 10:21:55,082 INFO] src: 14783 new tokens
[2024-07-29 10:21:55,457 INFO] tgt: 14783 new tokens
[2024-07-29 10:21:56,484 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-23): 24 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 10:21:56,493 INFO] encoder: 326900736
[2024-07-29 10:21:56,493 INFO] decoder: 403146189
[2024-07-29 10:21:56,493 INFO] * number of parameters: 730046925
[2024-07-29 10:21:56,496 INFO] Trainable parameters = {'torch.float32': 730046925, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:21:56,496 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:21:56,496 INFO]  * src vocab size = 24013
[2024-07-29 10:21:56,496 INFO]  * tgt vocab size = 24013
[2024-07-29 10:21:56,826 INFO] Starting training on GPU: [0]
[2024-07-29 10:21:56,826 INFO] Start training loop without validation...
[2024-07-29 10:21:56,826 INFO] Scoring with: None
[2024-07-29 10:23:31,664 INFO] Parsed 1 corpora from -data.
[2024-07-29 10:23:31,665 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 10:23:33,082 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-29 10:23:33,082 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', 'dyu_Latn', ''], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 10:23:33,116 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:23:33,116 INFO] The decoder start token is: </s>
[2024-07-29 10:23:33,157 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 10:23:33,157 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', 'dyu_Latn', ''], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 10:23:33,190 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:23:33,191 INFO] The decoder start token is: </s>
[2024-07-29 10:23:33,194 INFO] Over-ride model option set to true - use with care
[2024-07-29 10:23:33,194 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 10:23:33,194 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 10:23:33,194 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 10:23:33,194 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 10:23:33,194 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:23:33,194 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:23:33,194 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:23:33,194 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:23:33,194 INFO] Option: src_seq_length , value: 120 overriding model: 150
[2024-07-29 10:23:33,194 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 10:23:33,194 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:23:33,194 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:23:33,194 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 10:23:33,194 INFO] Option: enc_layers , value: 24 overriding model: 12
[2024-07-29 10:23:33,194 INFO] Option: dec_layers , value: 24 overriding model: 12
[2024-07-29 10:23:33,195 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V10 overriding model: nllb
[2024-07-29 10:23:33,195 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 10:23:33,195 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 10:23:33,195 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 10:23:33,195 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 10:23:33,195 INFO] Option: batch_size , value: 2064 overriding model: 8192
[2024-07-29 10:23:33,195 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 10:23:33,195 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:23:33,195 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 10:23:33,195 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 10:23:33,195 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 10:23:33,195 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 10:23:33,195 INFO] Option: dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:23:33,195 INFO] Option: attention_dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:23:33,195 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:23:33,195 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 10:23:33,195 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 10:23:33,195 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 10:23:33,195 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 10:23:33,195 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V10.log overriding model: 
[2024-07-29 10:23:33,195 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 10:23:33,195 INFO] Option: _all_transform , value: {'sentencepiece', 'filtertoolong', 'suffix', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-29 10:23:33,195 INFO] Building model...
[2024-07-29 10:23:39,440 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 10:23:39,440 INFO] Non quantized layer compute is fp16
[2024-07-29 10:23:39,440 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 10:23:39,657 INFO] src: 14783 new tokens
[2024-07-29 10:23:40,044 INFO] tgt: 14783 new tokens
[2024-07-29 10:23:41,072 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-23): 24 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 10:23:41,080 INFO] encoder: 326900736
[2024-07-29 10:23:41,080 INFO] decoder: 403146189
[2024-07-29 10:23:41,080 INFO] * number of parameters: 730046925
[2024-07-29 10:23:41,083 INFO] Trainable parameters = {'torch.float32': 730046925, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:23:41,083 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:23:41,083 INFO]  * src vocab size = 24013
[2024-07-29 10:23:41,083 INFO]  * tgt vocab size = 24013
[2024-07-29 10:23:41,408 INFO] Starting training on GPU: [0]
[2024-07-29 10:23:41,408 INFO] Start training loop without validation...
[2024-07-29 10:23:41,408 INFO] Scoring with: None
[2024-07-29 10:27:13,662 INFO] Step 10/10000; acc: 24.7; ppl: 429.0; xent: 6.1; lr: 0.00069; sents:   16398; bsz: 1005/1282/51; 1516/1933 tok/s;    212 sec;
[2024-07-29 10:27:41,220 INFO] Parsed 1 corpora from -data.
[2024-07-29 10:27:41,221 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 10:27:42,647 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'prefix', 'suffix'}
[2024-07-29 10:27:42,647 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:27:42,681 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:27:42,681 INFO] The decoder start token is: </s>
[2024-07-29 10:27:42,712 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 10:27:42,712 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:27:42,745 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:27:42,746 INFO] The decoder start token is: </s>
[2024-07-29 10:27:42,749 INFO] Over-ride model option set to true - use with care
[2024-07-29 10:27:42,749 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 10:27:42,749 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 10:27:42,749 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 10:27:42,749 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 10:27:42,749 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:27:42,750 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:27:42,750 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:27:42,750 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:27:42,750 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 10:27:42,750 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:27:42,750 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:27:42,750 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 10:27:42,750 INFO] Option: enc_layers , value: 24 overriding model: 12
[2024-07-29 10:27:42,750 INFO] Option: dec_layers , value: 24 overriding model: 12
[2024-07-29 10:27:42,750 INFO] Option: bucket_size , value: 131072 overriding model: 262144
[2024-07-29 10:27:42,750 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V10 overriding model: nllb
[2024-07-29 10:27:42,750 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 10:27:42,750 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 10:27:42,750 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 10:27:42,750 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 10:27:42,750 INFO] Option: batch_size , value: 2064 overriding model: 8192
[2024-07-29 10:27:42,750 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 10:27:42,750 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:27:42,750 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 10:27:42,750 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 10:27:42,750 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 10:27:42,750 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 10:27:42,750 INFO] Option: dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:27:42,750 INFO] Option: attention_dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:27:42,751 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:27:42,751 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 10:27:42,751 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 10:27:42,751 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 10:27:42,751 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 10:27:42,751 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V10.log overriding model: 
[2024-07-29 10:27:42,751 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 10:27:42,751 INFO] Option: _all_transform , value: {'sentencepiece', 'filtertoolong', 'prefix', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-29 10:27:42,751 INFO] Building model...
[2024-07-29 10:27:48,643 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 10:27:48,644 INFO] Non quantized layer compute is fp16
[2024-07-29 10:27:48,644 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 10:27:48,849 INFO] src: 14783 new tokens
[2024-07-29 10:27:49,154 INFO] tgt: 14783 new tokens
[2024-07-29 10:27:50,176 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-23): 24 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 10:27:50,185 INFO] encoder: 326900736
[2024-07-29 10:27:50,185 INFO] decoder: 403146189
[2024-07-29 10:27:50,185 INFO] * number of parameters: 730046925
[2024-07-29 10:27:50,188 INFO] Trainable parameters = {'torch.float32': 730046925, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:27:50,188 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:27:50,188 INFO]  * src vocab size = 24013
[2024-07-29 10:27:50,188 INFO]  * tgt vocab size = 24013
[2024-07-29 10:27:50,520 INFO] Starting training on GPU: [0]
[2024-07-29 10:27:50,520 INFO] Start training loop without validation...
[2024-07-29 10:27:50,520 INFO] Scoring with: None
[2024-07-29 10:28:11,859 INFO] Parsed 1 corpora from -data.
[2024-07-29 10:28:11,859 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 10:28:13,255 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-29 10:28:13,255 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:28:13,289 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:28:13,289 INFO] The decoder start token is: </s>
[2024-07-29 10:28:13,331 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 10:28:13,331 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 10:28:13,363 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 10:28:13,364 INFO] The decoder start token is: </s>
[2024-07-29 10:28:13,367 INFO] Over-ride model option set to true - use with care
[2024-07-29 10:28:13,367 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 10:28:13,367 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 10:28:13,367 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 10:28:13,367 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 10:28:13,367 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:28:13,367 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 10:28:13,367 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:28:13,367 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 10:28:13,367 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 10:28:13,367 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:28:13,367 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 10:28:13,367 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 10:28:13,367 INFO] Option: enc_layers , value: 24 overriding model: 12
[2024-07-29 10:28:13,367 INFO] Option: dec_layers , value: 24 overriding model: 12
[2024-07-29 10:28:13,367 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 10:28:13,367 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V10 overriding model: nllb
[2024-07-29 10:28:13,367 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 10:28:13,367 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 10:28:13,368 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 10:28:13,368 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 10:28:13,368 INFO] Option: batch_size , value: 2064 overriding model: 8192
[2024-07-29 10:28:13,368 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 10:28:13,368 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:28:13,368 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 10:28:13,368 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 10:28:13,368 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 10:28:13,368 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 10:28:13,368 INFO] Option: dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:28:13,368 INFO] Option: attention_dropout , value: [0.1, 0.1, 0.1] overriding model: [0.1]
[2024-07-29 10:28:13,368 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 10:28:13,368 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 10:28:13,368 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 10:28:13,368 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 10:28:13,368 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 10:28:13,368 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V10.log overriding model: 
[2024-07-29 10:28:13,368 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 10:28:13,368 INFO] Option: _all_transform , value: {'prefix', 'suffix', 'sentencepiece', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-29 10:28:13,368 INFO] Building model...
[2024-07-29 10:28:19,224 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 10:28:19,224 INFO] Non quantized layer compute is fp16
[2024-07-29 10:28:19,225 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 10:28:19,420 INFO] src: 14783 new tokens
[2024-07-29 10:28:19,719 INFO] tgt: 14783 new tokens
[2024-07-29 10:28:20,724 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): ModuleList(
      (0-23): 24 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-23): 24 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.1, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 10:28:20,734 INFO] encoder: 326900736
[2024-07-29 10:28:20,734 INFO] decoder: 403146189
[2024-07-29 10:28:20,734 INFO] * number of parameters: 730046925
[2024-07-29 10:28:20,738 INFO] Trainable parameters = {'torch.float32': 730046925, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:28:20,738 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 10:28:20,738 INFO]  * src vocab size = 24013
[2024-07-29 10:28:20,738 INFO]  * tgt vocab size = 24013
[2024-07-29 10:28:21,083 INFO] Starting training on GPU: [0]
[2024-07-29 10:28:21,084 INFO] Start training loop without validation...
[2024-07-29 10:28:21,084 INFO] Scoring with: None
[2024-07-29 10:30:14,908 INFO] Step 10/10000; acc: 24.6; ppl: 433.3; xent: 6.1; lr: 0.00069; sents:   16393; bsz: 1000/1276/51; 2811/3587 tok/s;    114 sec;
[2024-07-29 10:31:37,220 INFO] Step 20/10000; acc: 34.6; ppl: 164.8; xent: 5.1; lr: 0.00131; sents:   16192; bsz: 1011/1290/51; 3931/5014 tok/s;    196 sec;
[2024-07-29 10:32:58,893 INFO] Step 30/10000; acc: 45.0; ppl:  79.2; xent: 4.4; lr: 0.00194; sents:   16212; bsz: 1015/1286/51; 3977/5038 tok/s;    278 sec;
[2024-07-29 10:34:19,707 INFO] Step 40/10000; acc: 52.5; ppl:  45.8; xent: 3.8; lr: 0.00256; sents:   16709; bsz: 1001/1278/52; 3963/5060 tok/s;    359 sec;
[2024-07-29 10:35:40,508 INFO] Step 50/10000; acc: 60.3; ppl:  28.1; xent: 3.3; lr: 0.00319; sents:   15439; bsz: 1002/1281/48; 3967/5071 tok/s;    439 sec;
[2024-07-29 10:37:01,549 INFO] Step 60/10000; acc: 67.2; ppl:  19.3; xent: 3.0; lr: 0.00381; sents:   17138; bsz: 1019/1298/54; 4024/5126 tok/s;    520 sec;
[2024-07-29 10:38:23,294 INFO] Step 70/10000; acc: 74.7; ppl:  13.3; xent: 2.6; lr: 0.00444; sents:   15470; bsz: 1013/1284/48; 3965/5027 tok/s;    602 sec;
[2024-07-29 10:39:44,576 INFO] Step 80/10000; acc: 78.7; ppl:  10.7; xent: 2.4; lr: 0.00506; sents:   15782; bsz:  998/1280/49; 3930/5041 tok/s;    683 sec;
[2024-07-29 10:41:05,781 INFO] Step 90/10000; acc: 83.7; ppl:   8.5; xent: 2.1; lr: 0.00569; sents:   17365; bsz:  998/1272/54; 3934/5014 tok/s;    765 sec;
[2024-07-29 10:42:27,185 INFO] Step 100/10000; acc: 87.3; ppl:   7.2; xent: 2.0; lr: 0.00622; sents:   16149; bsz: 1017/1295/50; 4000/5089 tok/s;    846 sec;
[2024-07-29 10:42:27,200 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V10_step_100.pt
[2024-07-29 10:43:54,613 INFO] Step 110/10000; acc: 90.0; ppl:   6.4; xent: 1.9; lr: 0.00593; sents:   16279; bsz:  989/1277/51; 3621/4675 tok/s;    934 sec;
