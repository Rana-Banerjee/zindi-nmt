[2024-07-29 11:14:54,889 INFO] Parsed 1 corpora from -data.
[2024-07-29 11:14:54,889 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 11:14:56,463 WARNING] configured transforms is different from checkpoint: +{'prefix', 'sentencepiece', 'suffix'}
[2024-07-29 11:14:56,463 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 11:14:56,496 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:14:56,496 INFO] The decoder start token is: </s>
[2024-07-29 11:14:56,525 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 11:14:56,525 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 11:14:56,560 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:14:56,560 INFO] The decoder start token is: </s>
[2024-07-29 11:14:56,564 INFO] Over-ride model option set to true - use with care
[2024-07-29 11:14:56,564 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 11:14:56,564 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 11:14:56,564 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 11:14:56,564 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 11:14:56,564 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:14:56,564 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:14:56,564 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:14:56,564 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:14:56,564 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 11:14:56,564 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:14:56,564 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:14:56,564 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 11:14:56,565 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 11:14:56,565 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V12 overriding model: nllb
[2024-07-29 11:14:56,565 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 11:14:56,565 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 11:14:56,565 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 11:14:56,565 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 11:14:56,565 INFO] Option: batch_size , value: 2064 overriding model: 8192
[2024-07-29 11:14:56,565 INFO] Option: accum_count , value: [32, 32, 32] overriding model: [4]
[2024-07-29 11:14:56,565 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:14:56,565 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 11:14:56,565 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 11:14:56,565 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 11:14:56,565 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 11:14:56,565 INFO] Option: dropout , value: [0.3, 0.3, 0.3] overriding model: [0.1]
[2024-07-29 11:14:56,565 INFO] Option: attention_dropout , value: [0.2, 0.2, 0.2] overriding model: [0.1]
[2024-07-29 11:14:56,565 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:14:56,565 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 11:14:56,565 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 11:14:56,565 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 11:14:56,565 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 11:14:56,565 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V12.log overriding model: 
[2024-07-29 11:14:56,565 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 11:14:56,566 INFO] Option: _all_transform , value: {'prefix', 'filtertoolong', 'sentencepiece', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-29 11:14:56,566 INFO] Building model...
[2024-07-29 11:15:00,070 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 11:15:00,071 INFO] Non quantized layer compute is fp16
[2024-07-29 11:15:00,071 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 11:15:00,302 INFO] src: 14783 new tokens
[2024-07-29 11:15:00,661 INFO] tgt: 14783 new tokens
[2024-07-29 11:15:01,424 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=4096, bias=True)
          (w_2): Linear(in_features=4096, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 11:15:01,431 INFO] encoder: 175746048
[2024-07-29 11:15:01,431 INFO] decoder: 201586125
[2024-07-29 11:15:01,431 INFO] * number of parameters: 377332173
[2024-07-29 11:15:01,433 INFO] Trainable parameters = {'torch.float32': 377332173, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:15:01,433 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:15:01,433 INFO]  * src vocab size = 24013
[2024-07-29 11:15:01,434 INFO]  * tgt vocab size = 24013
[2024-07-29 11:15:01,759 INFO] Starting training on GPU: [0]
[2024-07-29 11:15:01,760 INFO] Start training loop without validation...
[2024-07-29 11:15:01,760 INFO] Scoring with: None
[2024-07-29 11:16:51,406 INFO] Step 10/10000; acc: 14.5; ppl: 1097.3; xent: 7.0; lr: 0.00069; sents:   16393; bsz: 1000/1276/51; 2918/3724 tok/s;    110 sec;
[2024-07-29 11:17:34,795 INFO] Step 20/10000; acc: 21.7; ppl: 429.5; xent: 6.1; lr: 0.00131; sents:   16192; bsz: 1011/1290/51; 7457/9511 tok/s;    153 sec;
[2024-07-29 11:18:18,419 INFO] Step 30/10000; acc: 27.5; ppl: 237.0; xent: 5.5; lr: 0.00194; sents:   16212; bsz: 1015/1286/51; 7446/9431 tok/s;    197 sec;
[2024-07-29 11:19:01,969 INFO] Step 40/10000; acc: 35.7; ppl: 125.9; xent: 4.8; lr: 0.00256; sents:   16709; bsz: 1001/1278/52; 7353/9389 tok/s;    240 sec;
[2024-07-29 11:19:45,485 INFO] Step 50/10000; acc: 40.1; ppl:  84.6; xent: 4.4; lr: 0.00319; sents:   15439; bsz: 1002/1281/48; 7366/9416 tok/s;    284 sec;
[2024-07-29 11:20:28,929 INFO] Step 60/10000; acc: 44.9; ppl:  60.2; xent: 4.1; lr: 0.00381; sents:   17138; bsz: 1019/1298/54; 7507/9562 tok/s;    327 sec;
[2024-07-29 11:21:12,341 INFO] Step 70/10000; acc: 49.0; ppl:  45.2; xent: 3.8; lr: 0.00444; sents:   15470; bsz: 1013/1284/48; 7466/9466 tok/s;    371 sec;
[2024-07-29 11:21:55,671 INFO] Step 80/10000; acc: 53.4; ppl:  34.2; xent: 3.5; lr: 0.00506; sents:   15782; bsz:  998/1280/49; 7373/9456 tok/s;    414 sec;
[2024-07-29 11:22:38,818 INFO] Step 90/10000; acc: 57.2; ppl:  26.9; xent: 3.3; lr: 0.00569; sents:   17365; bsz:  998/1272/54; 7403/9436 tok/s;    457 sec;
[2024-07-29 11:23:22,199 INFO] Step 100/10000; acc: 61.1; ppl:  21.0; xent: 3.0; lr: 0.00622; sents:   16149; bsz: 1017/1295/50; 7506/9550 tok/s;    500 sec;
[2024-07-29 11:23:22,210 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V12_step_100.pt
[2024-07-29 11:24:08,322 INFO] Step 110/10000; acc: 66.1; ppl:  16.3; xent: 2.8; lr: 0.00593; sents:   16279; bsz:  989/1277/51; 6863/8862 tok/s;    547 sec;
[2024-07-29 11:24:51,700 INFO] Step 120/10000; acc: 72.2; ppl:  12.5; xent: 2.5; lr: 0.00568; sents:   16220; bsz: 1031/1289/51; 7607/9508 tok/s;    590 sec;
[2024-07-29 11:25:34,894 INFO] Step 130/10000; acc: 76.7; ppl:  10.4; xent: 2.3; lr: 0.00546; sents:   16195; bsz:  987/1264/51; 7313/9367 tok/s;    633 sec;
[2024-07-29 11:26:18,081 INFO] Step 140/10000; acc: 80.8; ppl:   8.8; xent: 2.2; lr: 0.00526; sents:   16747; bsz: 1007/1306/52; 7461/9680 tok/s;    676 sec;
[2024-07-29 11:27:01,345 INFO] Step 150/10000; acc: 84.6; ppl:   7.6; xent: 2.0; lr: 0.00509; sents:   15972; bsz: 1023/1273/50; 7567/9415 tok/s;    720 sec;
[2024-07-29 11:27:44,320 INFO] Step 160/10000; acc: 86.5; ppl:   7.1; xent: 2.0; lr: 0.00493; sents:   17062; bsz: 1011/1297/53; 7527/9661 tok/s;    763 sec;
[2024-07-29 11:28:27,679 INFO] Step 170/10000; acc: 88.9; ppl:   6.5; xent: 1.9; lr: 0.00478; sents:   15613; bsz:  992/1264/49; 7321/9329 tok/s;    806 sec;
[2024-07-29 11:29:10,932 INFO] Step 180/10000; acc: 90.7; ppl:   6.1; xent: 1.8; lr: 0.00465; sents:   17136; bsz:  999/1284/54; 7392/9501 tok/s;    849 sec;
[2024-07-29 11:29:54,437 INFO] Step 190/10000; acc: 92.4; ppl:   5.7; xent: 1.7; lr: 0.00452; sents:   15923; bsz: 1021/1290/50; 7511/9487 tok/s;    893 sec;
[2024-07-29 11:30:37,868 INFO] Step 200/10000; acc: 93.5; ppl:   5.5; xent: 1.7; lr: 0.00441; sents:   15760; bsz: 1004/1284/49; 7395/9460 tok/s;    936 sec;
[2024-07-29 11:30:37,877 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V12_step_200.pt
[2024-07-29 11:31:24,556 INFO] Step 210/10000; acc: 94.1; ppl:   5.3; xent: 1.7; lr: 0.00430; sents:   17614; bsz: 1000/1286/55; 6853/8816 tok/s;    983 sec;
[2024-07-29 11:32:07,940 INFO] Step 220/10000; acc: 95.4; ppl:   5.1; xent: 1.6; lr: 0.00420; sents:   15362; bsz: 1014/1275/48; 7480/9404 tok/s;   1026 sec;
[2024-07-29 11:32:51,106 INFO] Step 230/10000; acc: 95.6; ppl:   5.1; xent: 1.6; lr: 0.00411; sents:   17002; bsz: 1002/1288/53; 7426/9550 tok/s;   1069 sec;
[2024-07-29 11:33:34,589 INFO] Step 240/10000; acc: 96.4; ppl:   4.9; xent: 1.6; lr: 0.00403; sents:   15577; bsz: 1001/1274/49; 7367/9376 tok/s;   1113 sec;
[2024-07-29 11:34:17,812 INFO] Step 250/10000; acc: 96.6; ppl:   4.9; xent: 1.6; lr: 0.00394; sents:   16655; bsz: 1002/1288/52; 7416/9533 tok/s;   1156 sec;
[2024-07-29 11:35:01,234 INFO] Step 260/10000; acc: 97.2; ppl:   4.8; xent: 1.6; lr: 0.00387; sents:   16294; bsz: 1021/1291/51; 7525/9513 tok/s;   1199 sec;
