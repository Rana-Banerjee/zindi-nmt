[2024-07-29 14:55:37,743 INFO] Parsed 1 corpora from -data.
[2024-07-29 14:55:37,743 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 14:55:39,207 WARNING] configured transforms is different from checkpoint: +{'suffix', 'sentencepiece', 'prefix'}
[2024-07-29 14:55:39,207 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 14:55:39,248 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 14:55:39,248 INFO] The decoder start token is: </s>
[2024-07-29 14:55:39,283 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 14:55:39,283 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 14:55:39,322 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 14:55:39,323 INFO] The decoder start token is: </s>
[2024-07-29 14:55:39,326 INFO] Over-ride model option set to true - use with care
[2024-07-29 14:55:39,326 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 14:55:39,326 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 14:55:39,326 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 14:55:39,326 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 14:55:39,326 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 14:55:39,326 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 14:55:39,326 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 14:55:39,326 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 14:55:39,327 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 14:55:39,327 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 14:55:39,327 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 14:55:39,327 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 14:55:39,327 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 14:55:39,327 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 14:55:39,327 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 14:55:39,327 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 14:55:39,327 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 14:55:39,327 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 14:55:39,327 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 14:55:39,327 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 14:55:39,327 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 14:55:39,327 INFO] Option: batch_size , value: 17536 overriding model: 8192
[2024-07-29 14:55:39,327 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 14:55:39,327 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 14:55:39,327 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 14:55:39,327 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 14:55:39,327 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 14:55:39,327 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 14:55:39,328 INFO] Option: dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 14:55:39,328 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 14:55:39,328 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 14:55:39,328 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 14:55:39,328 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 14:55:39,328 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 14:55:39,328 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 14:55:39,328 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 14:55:39,328 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 14:55:39,328 INFO] Option: _all_transform , value: {'suffix', 'filtertoolong', 'sentencepiece', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-29 14:55:39,328 INFO] Building model...
[2024-07-29 14:55:41,678 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 14:55:41,678 INFO] Non quantized layer compute is fp16
[2024-07-29 14:55:41,678 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 14:55:41,909 INFO] src: 14783 new tokens
[2024-07-29 14:55:42,255 INFO] tgt: 14783 new tokens
[2024-07-29 14:55:42,727 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.75, inplace=False)
          (dropout_2): Dropout(p=0.75, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.75, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.75, inplace=False)
          (dropout_2): Dropout(p=0.75, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.75, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 14:55:42,733 INFO] encoder: 100211712
[2024-07-29 14:55:42,733 INFO] decoder: 126051789
[2024-07-29 14:55:42,733 INFO] * number of parameters: 226263501
[2024-07-29 14:55:42,735 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 14:55:42,735 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 14:55:42,735 INFO]  * src vocab size = 24013
[2024-07-29 14:55:42,735 INFO]  * tgt vocab size = 24013
[2024-07-29 14:55:43,103 INFO] Starting training on GPU: [0]
[2024-07-29 14:55:43,103 INFO] Start training loop without validation...
[2024-07-29 14:55:43,103 INFO] Scoring with: None
[2024-07-29 14:56:35,681 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:56:35,792 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:56:35,811 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:56:35,846 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:56:46,889 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:56:47,001 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:56:47,060 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:56:47,083 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:56:48,019 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,098 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,117 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,171 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,218 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,265 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,297 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:56:48,598 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,630 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,659 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,715 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,759 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,843 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,874 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:56:48,952 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:57:23,587 INFO] Parsed 1 corpora from -data.
[2024-07-29 14:57:23,587 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 14:57:24,948 WARNING] configured transforms is different from checkpoint: +{'prefix', 'sentencepiece', 'suffix'}
[2024-07-29 14:57:24,948 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 14:57:24,981 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 14:57:24,981 INFO] The decoder start token is: </s>
[2024-07-29 14:57:25,008 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 14:57:25,008 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 14:57:25,040 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 14:57:25,040 INFO] The decoder start token is: </s>
[2024-07-29 14:57:25,043 INFO] Over-ride model option set to true - use with care
[2024-07-29 14:57:25,043 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 14:57:25,043 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 14:57:25,044 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 14:57:25,044 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 14:57:25,044 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 14:57:25,044 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 14:57:25,044 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 14:57:25,044 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 14:57:25,044 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 14:57:25,044 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 14:57:25,044 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 14:57:25,044 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 14:57:25,044 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 14:57:25,044 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 14:57:25,044 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 14:57:25,044 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 14:57:25,044 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 14:57:25,044 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 14:57:25,044 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 14:57:25,044 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 14:57:25,044 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 14:57:25,044 INFO] Option: batch_size , value: 17020 overriding model: 8192
[2024-07-29 14:57:25,044 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 14:57:25,044 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 14:57:25,044 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 14:57:25,044 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 14:57:25,045 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 14:57:25,045 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 14:57:25,045 INFO] Option: dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 14:57:25,045 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 14:57:25,045 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 14:57:25,045 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 14:57:25,045 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 14:57:25,045 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 14:57:25,045 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 14:57:25,045 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 14:57:25,045 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 14:57:25,045 INFO] Option: _all_transform , value: {'filtertoolong', 'prefix', 'sentencepiece', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-29 14:57:25,045 INFO] Building model...
[2024-07-29 14:57:27,253 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 14:57:27,253 INFO] Non quantized layer compute is fp16
[2024-07-29 14:57:27,253 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 14:57:27,442 INFO] src: 14783 new tokens
[2024-07-29 14:57:27,755 INFO] tgt: 14783 new tokens
[2024-07-29 14:57:28,181 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.75, inplace=False)
          (dropout_2): Dropout(p=0.75, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.75, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.75, inplace=False)
          (dropout_2): Dropout(p=0.75, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.75, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 14:57:28,186 INFO] encoder: 100211712
[2024-07-29 14:57:28,186 INFO] decoder: 126051789
[2024-07-29 14:57:28,186 INFO] * number of parameters: 226263501
[2024-07-29 14:57:28,188 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 14:57:28,188 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 14:57:28,188 INFO]  * src vocab size = 24013
[2024-07-29 14:57:28,188 INFO]  * tgt vocab size = 24013
[2024-07-29 14:57:28,519 INFO] Starting training on GPU: [0]
[2024-07-29 14:57:28,519 INFO] Start training loop without validation...
[2024-07-29 14:57:28,519 INFO] Scoring with: None
[2024-07-29 14:58:06,629 INFO] Parsed 1 corpora from -data.
[2024-07-29 14:58:06,629 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 14:58:07,988 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-29 14:58:07,988 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 14:58:08,021 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 14:58:08,021 INFO] The decoder start token is: </s>
[2024-07-29 14:58:08,049 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 14:58:08,049 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 14:58:08,082 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 14:58:08,082 INFO] The decoder start token is: </s>
[2024-07-29 14:58:08,085 INFO] Over-ride model option set to true - use with care
[2024-07-29 14:58:08,085 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 14:58:08,085 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 14:58:08,085 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 14:58:08,085 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 14:58:08,085 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 14:58:08,085 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 14:58:08,085 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 14:58:08,086 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 14:58:08,086 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 14:58:08,086 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 14:58:08,086 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 14:58:08,086 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 14:58:08,086 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 14:58:08,086 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 14:58:08,086 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 14:58:08,086 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 14:58:08,086 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 14:58:08,086 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 14:58:08,086 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 14:58:08,086 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 14:58:08,086 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 14:58:08,086 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 14:58:08,086 INFO] Option: batch_size , value: 17536 overriding model: 8192
[2024-07-29 14:58:08,086 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 14:58:08,086 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 14:58:08,086 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 14:58:08,086 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 14:58:08,086 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 14:58:08,087 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 14:58:08,087 INFO] Option: dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 14:58:08,087 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 14:58:08,087 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 14:58:08,087 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 14:58:08,087 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 14:58:08,087 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 14:58:08,087 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 14:58:08,087 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 14:58:08,087 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 14:58:08,087 INFO] Option: _all_transform , value: {'prefix', 'filtertoolong', 'suffix', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-29 14:58:08,087 INFO] Building model...
[2024-07-29 14:58:10,407 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 14:58:10,408 INFO] Non quantized layer compute is fp16
[2024-07-29 14:58:10,408 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 14:58:10,620 INFO] src: 14783 new tokens
[2024-07-29 14:58:11,025 INFO] tgt: 14783 new tokens
[2024-07-29 14:58:11,469 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.75, inplace=False)
          (dropout_2): Dropout(p=0.75, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.75, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.75, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.75, inplace=False)
          (dropout_2): Dropout(p=0.75, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.75, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 14:58:11,474 INFO] encoder: 100211712
[2024-07-29 14:58:11,474 INFO] decoder: 126051789
[2024-07-29 14:58:11,474 INFO] * number of parameters: 226263501
[2024-07-29 14:58:11,476 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 14:58:11,476 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 14:58:11,476 INFO]  * src vocab size = 24013
[2024-07-29 14:58:11,476 INFO]  * tgt vocab size = 24013
[2024-07-29 14:58:11,792 INFO] Starting training on GPU: [0]
[2024-07-29 14:58:11,792 INFO] Start training loop without validation...
[2024-07-29 14:58:11,792 INFO] Scoring with: None
[2024-07-29 14:59:02,980 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:59:03,096 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:59:03,120 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:59:03,157 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 14:59:14,200 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:59:14,314 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:59:14,378 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:59:14,405 INFO] Step 8, cuda OOM - batch removed
[2024-07-29 14:59:15,349 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,431 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,452 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,502 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,543 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,584 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,612 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 14:59:15,912 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:15,945 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:15,974 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:16,022 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:16,067 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:16,157 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:16,181 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 14:59:16,263 INFO] Step 10, cuda OOM - batch removed
[2024-07-29 15:00:57,002 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:00:57,002 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:00:58,338 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-29 15:00:58,338 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:00:58,372 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:00:58,372 INFO] The decoder start token is: </s>
[2024-07-29 15:00:58,405 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:00:58,405 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:00:58,439 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:00:58,439 INFO] The decoder start token is: </s>
[2024-07-29 15:00:58,442 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:00:58,443 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:00:58,443 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:00:58,443 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:00:58,443 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:00:58,443 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:00:58,443 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:00:58,443 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:00:58,443 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:00:58,443 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:00:58,443 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 15:00:58,443 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:00:58,443 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:00:58,443 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:00:58,443 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:00:58,443 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:00:58,443 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:00:58,443 INFO] Option: batch_size , value: 16512 overriding model: 8192
[2024-07-29 15:00:58,444 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:00:58,444 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:00:58,444 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:00:58,444 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:00:58,444 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:00:58,444 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:00:58,444 INFO] Option: dropout , value: [0.9, 0.9, 0.9] overriding model: [0.1]
[2024-07-29 15:00:58,444 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 15:00:58,444 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:00:58,444 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:00:58,444 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:00:58,444 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:00:58,444 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:00:58,444 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:00:58,444 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:00:58,444 INFO] Option: _all_transform , value: {'filtertoolong', 'prefix', 'suffix', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-29 15:00:58,444 INFO] Building model...
[2024-07-29 15:01:00,633 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:01:00,633 INFO] Non quantized layer compute is fp16
[2024-07-29 15:01:00,633 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:01:00,828 INFO] src: 14783 new tokens
[2024-07-29 15:01:01,135 INFO] tgt: 14783 new tokens
[2024-07-29 15:01:01,581 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:01:01,587 INFO] encoder: 100211712
[2024-07-29 15:01:01,587 INFO] decoder: 126051789
[2024-07-29 15:01:01,587 INFO] * number of parameters: 226263501
[2024-07-29 15:01:01,589 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:01:01,589 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:01:01,589 INFO]  * src vocab size = 24013
[2024-07-29 15:01:01,589 INFO]  * tgt vocab size = 24013
[2024-07-29 15:01:01,933 INFO] Starting training on GPU: [0]
[2024-07-29 15:01:01,933 INFO] Start training loop without validation...
[2024-07-29 15:01:01,933 INFO] Scoring with: None
[2024-07-29 15:01:41,287 INFO] Step 3, cuda OOM - batch removed
[2024-07-29 15:01:41,367 INFO] Step 3, cuda OOM - batch removed
[2024-07-29 15:01:41,384 INFO] Step 3, cuda OOM - batch removed
[2024-07-29 15:01:41,415 INFO] Step 3, cuda OOM - batch removed
[2024-07-29 15:01:41,447 INFO] Step 3, cuda OOM - batch removed
[2024-07-29 15:01:41,507 INFO] Step 3, cuda OOM - batch removed
[2024-07-29 15:01:45,935 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,027 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,050 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,101 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,122 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,172 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,194 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:01:46,242 INFO] Step 5, cuda OOM - batch removed
[2024-07-29 15:02:09,543 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:02:09,544 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:02:11,010 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-29 15:02:11,010 INFO] Get special vocabs from Transforms: {'src': ['</s>', '', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:02:11,044 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:02:11,044 INFO] The decoder start token is: </s>
[2024-07-29 15:02:11,091 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:02:11,091 INFO] Get special vocabs from Transforms: {'src': ['</s>', '', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:02:11,123 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:02:11,124 INFO] The decoder start token is: </s>
[2024-07-29 15:02:11,127 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:02:11,127 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:02:11,127 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:02:11,127 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:02:11,127 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:02:11,127 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:02:11,127 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:02:11,127 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:02:11,127 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:02:11,127 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:02:11,127 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:02:11,127 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:02:11,128 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:02:11,128 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:02:11,128 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:02:11,128 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:02:11,128 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 15:02:11,128 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:02:11,128 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:02:11,128 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:02:11,128 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:02:11,128 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:02:11,128 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:02:11,128 INFO] Option: batch_size , value: 15488 overriding model: 8192
[2024-07-29 15:02:11,128 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:02:11,128 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:02:11,128 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:02:11,128 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:02:11,128 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:02:11,128 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:02:11,128 INFO] Option: dropout , value: [0.9, 0.9, 0.9] overriding model: [0.1]
[2024-07-29 15:02:11,128 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 15:02:11,129 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:02:11,129 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:02:11,129 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:02:11,129 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:02:11,129 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:02:11,129 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:02:11,129 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:02:11,129 INFO] Option: _all_transform , value: {'suffix', 'sentencepiece', 'prefix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-29 15:02:11,129 INFO] Building model...
[2024-07-29 15:02:13,339 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:02:13,340 INFO] Non quantized layer compute is fp16
[2024-07-29 15:02:13,340 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:02:13,510 INFO] src: 14783 new tokens
[2024-07-29 15:02:13,827 INFO] tgt: 14783 new tokens
[2024-07-29 15:02:14,300 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:02:14,306 INFO] encoder: 100211712
[2024-07-29 15:02:14,306 INFO] decoder: 126051789
[2024-07-29 15:02:14,306 INFO] * number of parameters: 226263501
[2024-07-29 15:02:14,307 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:02:14,307 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:02:14,307 INFO]  * src vocab size = 24013
[2024-07-29 15:02:14,307 INFO]  * tgt vocab size = 24013
[2024-07-29 15:02:14,617 INFO] Starting training on GPU: [0]
[2024-07-29 15:02:14,617 INFO] Start training loop without validation...
[2024-07-29 15:02:14,617 INFO] Scoring with: None
[2024-07-29 15:03:17,008 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 15:03:17,033 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 15:03:17,056 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 15:03:17,123 INFO] Step 9, cuda OOM - batch removed
[2024-07-29 15:03:21,243 INFO] Step 10/10000; acc: 0.0; ppl: 393694.2; xent: 12.9; lr: 0.00069; sents:   19513; bsz: 6041/7137/257; 6890/8142 tok/s;     67 sec;
[2024-07-29 15:03:51,956 INFO] Step 19, cuda OOM - batch removed
[2024-07-29 15:03:51,979 INFO] Step 19, cuda OOM - batch removed
[2024-07-29 15:03:52,009 INFO] Step 19, cuda OOM - batch removed
[2024-07-29 15:03:52,038 INFO] Step 19, cuda OOM - batch removed
[2024-07-29 15:03:56,632 INFO] Step 20/10000; acc: 0.5; ppl: 110623.1; xent: 11.6; lr: 0.00131; sents:   21060; bsz: 5139/6210/277; 11037/13337 tok/s;    102 sec;
[2024-07-29 15:04:17,351 INFO] Step 26, cuda OOM - batch removed
[2024-07-29 15:04:17,378 INFO] Step 26, cuda OOM - batch removed
[2024-07-29 15:04:17,402 INFO] Step 26, cuda OOM - batch removed
[2024-07-29 15:04:17,431 INFO] Step 26, cuda OOM - batch removed
[2024-07-29 15:04:33,424 INFO] Step 30/10000; acc: 1.9; ppl:   nan; xent: nan; lr: 0.00194; sents:   22287; bsz: 5390/6565/293; 11133/13560 tok/s;    139 sec;
[2024-07-29 15:04:35,522 INFO] Step 31, cuda OOM - batch removed
[2024-07-29 15:04:35,569 INFO] Step 31, cuda OOM - batch removed
[2024-07-29 15:04:35,588 INFO] Step 31, cuda OOM - batch removed
[2024-07-29 15:04:35,614 INFO] Step 31, cuda OOM - batch removed
[2024-07-29 15:04:59,123 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,162 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,222 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,242 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,334 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,391 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,452 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:04:59,528 INFO] Step 38, cuda OOM - batch removed
[2024-07-29 15:06:06,617 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:06:06,617 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:06:08,111 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-29 15:06:08,111 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:06:08,153 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:06:08,153 INFO] The decoder start token is: </s>
[2024-07-29 15:06:08,199 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:06:08,199 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:06:08,237 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:06:08,237 INFO] The decoder start token is: </s>
[2024-07-29 15:06:08,241 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:06:08,241 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:06:08,241 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:06:08,241 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:06:08,241 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:06:08,241 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:06:08,241 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:06:08,241 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:06:08,241 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:06:08,241 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:06:08,241 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:06:08,241 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:06:08,241 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:06:08,241 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:06:08,242 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:06:08,242 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:06:08,242 INFO] Option: bucket_size , value: 512 overriding model: 262144
[2024-07-29 15:06:08,242 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:06:08,242 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:06:08,242 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:06:08,242 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:06:08,242 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:06:08,242 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:06:08,242 INFO] Option: batch_size , value: 12416 overriding model: 8192
[2024-07-29 15:06:08,242 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:06:08,242 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:06:08,242 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:06:08,242 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:06:08,242 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:06:08,242 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:06:08,242 INFO] Option: dropout , value: [0.9, 0.9, 0.9] overriding model: [0.1]
[2024-07-29 15:06:08,242 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 15:06:08,242 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:06:08,242 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:06:08,242 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:06:08,242 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:06:08,242 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:06:08,242 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:06:08,243 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:06:08,243 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'filtertoolong', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-29 15:06:08,243 INFO] Building model...
[2024-07-29 15:06:10,507 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:06:10,507 INFO] Non quantized layer compute is fp16
[2024-07-29 15:06:10,508 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:06:10,701 INFO] src: 14783 new tokens
[2024-07-29 15:06:11,004 INFO] tgt: 14783 new tokens
[2024-07-29 15:06:11,433 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:06:11,438 INFO] encoder: 100211712
[2024-07-29 15:06:11,438 INFO] decoder: 126051789
[2024-07-29 15:06:11,438 INFO] * number of parameters: 226263501
[2024-07-29 15:06:11,439 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:06:11,439 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:06:11,439 INFO]  * src vocab size = 24013
[2024-07-29 15:06:11,439 INFO]  * tgt vocab size = 24013
[2024-07-29 15:06:11,768 INFO] Starting training on GPU: [0]
[2024-07-29 15:06:11,768 INFO] Start training loop without validation...
[2024-07-29 15:06:11,768 INFO] Scoring with: None
[2024-07-29 15:06:49,297 INFO] Step 10/10000; acc: 0.0; ppl: 375994.4; xent: 12.8; lr: 0.00069; sents:   15273; bsz: 3656/4675/191; 7793/9966 tok/s;     38 sec;
[2024-07-29 15:07:34,436 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:07:34,436 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:07:35,948 WARNING] configured transforms is different from checkpoint: +{'suffix', 'sentencepiece', 'prefix'}
[2024-07-29 15:07:35,948 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:07:35,988 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:07:35,988 INFO] The decoder start token is: </s>
[2024-07-29 15:07:36,017 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:07:36,017 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:07:36,056 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:07:36,056 INFO] The decoder start token is: </s>
[2024-07-29 15:07:36,059 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:07:36,059 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:07:36,059 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:07:36,059 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:07:36,060 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:07:36,060 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:07:36,060 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:07:36,060 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:07:36,060 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:07:36,060 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:07:36,060 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:07:36,060 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:07:36,060 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:07:36,060 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:07:36,060 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:07:36,060 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:07:36,060 INFO] Option: bucket_size , value: 512 overriding model: 262144
[2024-07-29 15:07:36,060 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:07:36,060 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:07:36,060 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:07:36,060 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:07:36,060 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:07:36,060 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:07:36,060 INFO] Option: batch_size , value: 12672 overriding model: 8192
[2024-07-29 15:07:36,061 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:07:36,061 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:07:36,061 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:07:36,061 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:07:36,061 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:07:36,061 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:07:36,061 INFO] Option: dropout , value: [0.9, 0.9, 0.9] overriding model: [0.1]
[2024-07-29 15:07:36,061 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 15:07:36,061 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:07:36,061 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:07:36,061 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:07:36,061 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:07:36,061 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:07:36,061 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:07:36,061 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:07:36,061 INFO] Option: _all_transform , value: {'suffix', 'filtertoolong', 'sentencepiece', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-29 15:07:36,061 INFO] Building model...
[2024-07-29 15:07:38,402 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:07:38,402 INFO] Non quantized layer compute is fp16
[2024-07-29 15:07:38,402 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:07:38,600 INFO] src: 14783 new tokens
[2024-07-29 15:07:38,890 INFO] tgt: 14783 new tokens
[2024-07-29 15:07:39,324 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:07:39,329 INFO] encoder: 100211712
[2024-07-29 15:07:39,329 INFO] decoder: 126051789
[2024-07-29 15:07:39,329 INFO] * number of parameters: 226263501
[2024-07-29 15:07:39,331 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:07:39,331 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:07:39,331 INFO]  * src vocab size = 24013
[2024-07-29 15:07:39,331 INFO]  * tgt vocab size = 24013
[2024-07-29 15:07:39,656 INFO] Starting training on GPU: [0]
[2024-07-29 15:07:39,656 INFO] Start training loop without validation...
[2024-07-29 15:07:39,656 INFO] Scoring with: None
[2024-07-29 15:08:17,036 INFO] Step 10/10000; acc: 0.0; ppl: 377188.5; xent: 12.8; lr: 0.00069; sents:   15283; bsz: 3659/4679/191; 7831/10015 tok/s;     37 sec;
[2024-07-29 15:08:46,552 INFO] Step 20/10000; acc: 0.5; ppl: 111006.7; xent: 11.6; lr: 0.00131; sents:   14218; bsz: 3712/4767/178; 10060/12921 tok/s;     67 sec;
[2024-07-29 15:08:59,708 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:08:59,708 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:09:01,218 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'prefix', 'suffix'}
[2024-07-29 15:09:01,218 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '</s>', ''], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:09:01,259 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:09:01,259 INFO] The decoder start token is: </s>
[2024-07-29 15:09:01,300 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:09:01,300 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '</s>', ''], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:09:01,338 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:09:01,338 INFO] The decoder start token is: </s>
[2024-07-29 15:09:01,342 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:09:01,342 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:09:01,342 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:09:01,342 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:09:01,342 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:09:01,342 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:09:01,342 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:09:01,342 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:09:01,342 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:09:01,342 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:09:01,342 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:09:01,342 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:09:01,342 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:09:01,342 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:09:01,342 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:09:01,343 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:09:01,343 INFO] Option: bucket_size , value: 512 overriding model: 262144
[2024-07-29 15:09:01,343 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:09:01,343 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:09:01,343 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:09:01,343 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:09:01,343 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:09:01,343 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:09:01,343 INFO] Option: batch_size , value: 12928 overriding model: 8192
[2024-07-29 15:09:01,343 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:09:01,343 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:09:01,343 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:09:01,343 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:09:01,343 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:09:01,343 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:09:01,343 INFO] Option: dropout , value: [0.9, 0.9, 0.9] overriding model: [0.1]
[2024-07-29 15:09:01,343 INFO] Option: attention_dropout , value: [0.75, 0.75, 0.75] overriding model: [0.1]
[2024-07-29 15:09:01,343 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:09:01,343 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:09:01,343 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:09:01,343 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:09:01,343 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:09:01,343 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:09:01,343 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:09:01,344 INFO] Option: _all_transform , value: {'filtertoolong', 'sentencepiece', 'prefix', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-29 15:09:01,344 INFO] Building model...
[2024-07-29 15:09:03,712 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:09:03,712 INFO] Non quantized layer compute is fp16
[2024-07-29 15:09:03,712 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:09:03,957 INFO] src: 14783 new tokens
[2024-07-29 15:09:04,348 INFO] tgt: 14783 new tokens
[2024-07-29 15:09:04,782 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.9, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.9, inplace=False)
          (dropout_2): Dropout(p=0.9, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.9, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.75, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:09:04,788 INFO] encoder: 100211712
[2024-07-29 15:09:04,788 INFO] decoder: 126051789
[2024-07-29 15:09:04,788 INFO] * number of parameters: 226263501
[2024-07-29 15:09:04,790 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:09:04,790 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:09:04,790 INFO]  * src vocab size = 24013
[2024-07-29 15:09:04,791 INFO]  * tgt vocab size = 24013
[2024-07-29 15:09:05,128 INFO] Starting training on GPU: [0]
[2024-07-29 15:09:05,128 INFO] Start training loop without validation...
[2024-07-29 15:09:05,128 INFO] Scoring with: None
[2024-07-29 15:09:42,697 INFO] Step 10/10000; acc: 0.1; ppl: 377138.7; xent: 12.8; lr: 0.00069; sents:   15292; bsz: 3662/4684/191; 7797/9973 tok/s;     38 sec;
[2024-07-29 15:10:12,650 INFO] Step 20/10000; acc: 0.4; ppl: 119279.4; xent: 11.7; lr: 0.00131; sents:   14402; bsz: 3799/4876/180; 10146/13024 tok/s;     68 sec;
[2024-07-29 15:10:41,472 INFO] Step 30/10000; acc: 1.7; ppl:   nan; xent: nan; lr: 0.00194; sents:   14097; bsz: 3515/4468/176; 9757/12401 tok/s;     96 sec;
[2024-07-29 15:11:12,289 INFO] Step 40/10000; acc: 1.8; ppl:   nan; xent: nan; lr: 0.00256; sents:   15128; bsz: 3904/4985/189; 10135/12942 tok/s;    127 sec;
[2024-07-29 15:11:30,750 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:11:30,750 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:11:32,121 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-29 15:11:32,122 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:11:32,155 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:11:32,155 INFO] The decoder start token is: </s>
[2024-07-29 15:11:32,181 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:11:32,181 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 15:11:32,214 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:11:32,214 INFO] The decoder start token is: </s>
[2024-07-29 15:11:32,217 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:11:32,217 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:11:32,217 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:11:32,217 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:11:32,217 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:11:32,217 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:11:32,217 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:11:32,218 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:11:32,218 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:11:32,218 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:11:32,218 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:11:32,218 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:11:32,218 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:11:32,218 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:11:32,218 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:11:32,218 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:11:32,218 INFO] Option: bucket_size , value: 64 overriding model: 262144
[2024-07-29 15:11:32,218 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:11:32,218 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:11:32,218 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:11:32,218 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:11:32,218 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:11:32,218 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:11:32,218 INFO] Option: batch_size , value: 13184 overriding model: 8192
[2024-07-29 15:11:32,218 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:11:32,218 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:11:32,218 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:11:32,218 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:11:32,218 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:11:32,219 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:11:32,219 INFO] Option: dropout , value: [0.8, 0.8, 0.8] overriding model: [0.1]
[2024-07-29 15:11:32,219 INFO] Option: attention_dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-29 15:11:32,219 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:11:32,219 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:11:32,219 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:11:32,219 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:11:32,219 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:11:32,219 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:11:32,219 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:11:32,219 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'filtertoolong', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-29 15:11:32,219 INFO] Building model...
[2024-07-29 15:11:34,458 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:11:34,458 INFO] Non quantized layer compute is fp16
[2024-07-29 15:11:34,458 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:11:34,651 INFO] src: 14783 new tokens
[2024-07-29 15:11:34,949 INFO] tgt: 14783 new tokens
[2024-07-29 15:11:35,438 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.8, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.7, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.8, inplace=False)
          (dropout_2): Dropout(p=0.8, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.8, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.8, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.7, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.8, inplace=False)
          (dropout_2): Dropout(p=0.8, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.8, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.7, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:11:35,443 INFO] encoder: 100211712
[2024-07-29 15:11:35,443 INFO] decoder: 126051789
[2024-07-29 15:11:35,443 INFO] * number of parameters: 226263501
[2024-07-29 15:11:35,444 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:11:35,444 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:11:35,445 INFO]  * src vocab size = 24013
[2024-07-29 15:11:35,445 INFO]  * tgt vocab size = 24013
[2024-07-29 15:11:35,762 INFO] Starting training on GPU: [0]
[2024-07-29 15:11:35,762 INFO] Start training loop without validation...
[2024-07-29 15:11:35,762 INFO] Scoring with: None
[2024-07-29 15:11:52,493 INFO] Step 10/10000; acc: 0.2; ppl: 184361.9; xent: 12.1; lr: 0.00069; sents:    5119; bsz:  635/ 845/64; 3038/4039 tok/s;     17 sec;
[2024-07-29 15:12:03,251 INFO] Step 20/10000; acc: 2.7; ppl: 32358.5; xent: 10.4; lr: 0.00131; sents:    5120; bsz:  902/1193/64; 6705/8874 tok/s;     27 sec;
[2024-07-29 15:12:26,742 INFO] Step 30/10000; acc: 1.4; ppl: 8809.8; xent: 9.1; lr: 0.00194; sents:    5120; bsz: 2506/3114/64; 8533/10605 tok/s;     51 sec;
[2024-07-29 15:12:36,084 INFO] Step 40/10000; acc: 3.7; ppl:   nan; xent: nan; lr: 0.00256; sents:    5119; bsz:  707/ 931/64; 6057/7973 tok/s;     60 sec;
[2024-07-29 15:12:43,840 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:12:43,840 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:12:45,182 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-29 15:12:45,182 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:12:45,216 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:12:45,216 INFO] The decoder start token is: </s>
[2024-07-29 15:12:45,258 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:12:45,258 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:12:45,291 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:12:45,291 INFO] The decoder start token is: </s>
[2024-07-29 15:12:45,294 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:12:45,294 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:12:45,294 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:12:45,294 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:12:45,294 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:12:45,294 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:12:45,294 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:12:45,294 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:12:45,294 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:12:45,295 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:12:45,295 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:12:45,295 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:12:45,295 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:12:45,295 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:12:45,295 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:12:45,295 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:12:45,295 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 15:12:45,295 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:12:45,295 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:12:45,295 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:12:45,295 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:12:45,295 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:12:45,295 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:12:45,295 INFO] Option: batch_size , value: 13184 overriding model: 8192
[2024-07-29 15:12:45,295 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:12:45,295 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:12:45,295 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:12:45,296 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:12:45,296 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:12:45,296 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:12:45,296 INFO] Option: dropout , value: [0.8, 0.8, 0.8] overriding model: [0.1]
[2024-07-29 15:12:45,296 INFO] Option: attention_dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-29 15:12:45,296 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:12:45,296 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:12:45,296 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:12:45,296 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:12:45,296 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:12:45,296 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:12:45,296 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:12:45,296 INFO] Option: _all_transform , value: {'filtertoolong', 'prefix', 'suffix', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-29 15:12:45,296 INFO] Building model...
[2024-07-29 15:12:47,503 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:12:47,503 INFO] Non quantized layer compute is fp16
[2024-07-29 15:12:47,504 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:12:47,690 INFO] src: 14783 new tokens
[2024-07-29 15:12:48,001 INFO] tgt: 14783 new tokens
[2024-07-29 15:12:48,388 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.8, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.7, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.8, inplace=False)
          (dropout_2): Dropout(p=0.8, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.8, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.8, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.7, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.8, inplace=False)
          (dropout_2): Dropout(p=0.8, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.8, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.7, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:12:48,393 INFO] encoder: 100211712
[2024-07-29 15:12:48,393 INFO] decoder: 126051789
[2024-07-29 15:12:48,393 INFO] * number of parameters: 226263501
[2024-07-29 15:12:48,395 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:12:48,395 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:12:48,395 INFO]  * src vocab size = 24013
[2024-07-29 15:12:48,395 INFO]  * tgt vocab size = 24013
[2024-07-29 15:12:48,736 INFO] Starting training on GPU: [0]
[2024-07-29 15:12:48,736 INFO] Start training loop without validation...
[2024-07-29 15:12:48,736 INFO] Scoring with: None
[2024-07-29 15:13:52,020 INFO] Step 10/10000; acc: 0.2; ppl: 184488.8; xent: 12.1; lr: 0.00069; sents:   19204; bsz: 4647/5955/240; 5875/7528 tok/s;     63 sec;
[2024-07-29 15:14:25,913 INFO] Step 20/10000; acc: 2.0; ppl: 16658.4; xent: 9.7; lr: 0.00131; sents:   18052; bsz: 4701/5943/226; 11095/14027 tok/s;     97 sec;
[2024-07-29 15:15:00,228 INFO] Step 30/10000; acc: 2.5; ppl:   nan; xent: nan; lr: 0.00194; sents:   19633; bsz: 4682/5972/245; 10916/13924 tok/s;    131 sec;
[2024-07-29 15:15:34,476 INFO] Step 40/10000; acc: 2.7; ppl:   nan; xent: nan; lr: 0.00256; sents:   18656; bsz: 4734/6051/233; 11058/14135 tok/s;    166 sec;
[2024-07-29 15:16:08,710 INFO] Step 50/10000; acc: 3.2; ppl:   nan; xent: nan; lr: 0.00319; sents:   18346; bsz: 4872/6146/229; 11384/14362 tok/s;    200 sec;
[2024-07-29 15:16:23,986 INFO] Parsed 1 corpora from -data.
[2024-07-29 15:16:23,986 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 15:16:25,354 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-29 15:16:25,354 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:16:25,387 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:16:25,387 INFO] The decoder start token is: </s>
[2024-07-29 15:16:25,414 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 15:16:25,414 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 15:16:25,446 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 15:16:25,446 INFO] The decoder start token is: </s>
[2024-07-29 15:16:25,449 INFO] Over-ride model option set to true - use with care
[2024-07-29 15:16:25,449 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 15:16:25,449 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 15:16:25,449 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 15:16:25,449 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 15:16:25,449 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:16:25,449 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 15:16:25,450 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:16:25,450 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 15:16:25,450 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-29 15:16:25,450 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 15:16:25,450 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:16:25,450 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 15:16:25,450 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 15:16:25,450 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 15:16:25,450 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-29 15:16:25,450 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 15:16:25,450 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-29 15:16:25,450 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V17 overriding model: nllb
[2024-07-29 15:16:25,450 INFO] Option: save_checkpoint_steps , value: 250 overriding model: 5000
[2024-07-29 15:16:25,450 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 15:16:25,450 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 15:16:25,450 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 15:16:25,450 INFO] Option: batch_size , value: 13184 overriding model: 8192
[2024-07-29 15:16:25,450 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 15:16:25,450 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:16:25,450 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 15:16:25,450 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 15:16:25,450 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 15:16:25,450 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 15:16:25,451 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-29 15:16:25,451 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-29 15:16:25,451 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 15:16:25,451 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 15:16:25,451 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 15:16:25,451 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 15:16:25,451 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 15:16:25,451 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V17.log overriding model: 
[2024-07-29 15:16:25,451 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 15:16:25,451 INFO] Option: _all_transform , value: {'prefix', 'filtertoolong', 'suffix', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-29 15:16:25,451 INFO] Building model...
[2024-07-29 15:16:27,671 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 15:16:27,671 INFO] Non quantized layer compute is fp16
[2024-07-29 15:16:27,671 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 15:16:27,853 INFO] src: 14783 new tokens
[2024-07-29 15:16:28,157 INFO] tgt: 14783 new tokens
[2024-07-29 15:16:28,574 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 15:16:28,579 INFO] encoder: 100211712
[2024-07-29 15:16:28,579 INFO] decoder: 126051789
[2024-07-29 15:16:28,579 INFO] * number of parameters: 226263501
[2024-07-29 15:16:28,581 INFO] Trainable parameters = {'torch.float32': 226263501, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:16:28,581 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 15:16:28,581 INFO]  * src vocab size = 24013
[2024-07-29 15:16:28,581 INFO]  * tgt vocab size = 24013
[2024-07-29 15:16:28,914 INFO] Starting training on GPU: [0]
[2024-07-29 15:16:28,914 INFO] Start training loop without validation...
[2024-07-29 15:16:28,914 INFO] Scoring with: None
[2024-07-29 15:17:32,438 INFO] Step 10/10000; acc: 1.5; ppl: 36788.0; xent: 10.5; lr: 0.00069; sents:   19204; bsz: 4647/5955/240; 5853/7500 tok/s;     64 sec;
[2024-07-29 15:18:06,969 INFO] Step 20/10000; acc: 2.9; ppl: 3318.8; xent: 8.1; lr: 0.00131; sents:   18052; bsz: 4701/5943/226; 10890/13767 tok/s;     98 sec;
[2024-07-29 15:18:41,526 INFO] Step 30/10000; acc: 3.9; ppl: 2212.4; xent: 7.7; lr: 0.00194; sents:   19633; bsz: 4682/5972/245; 10840/13827 tok/s;    133 sec;
[2024-07-29 15:19:16,435 INFO] Step 40/10000; acc: 3.9; ppl: 1837.2; xent: 7.5; lr: 0.00256; sents:   18656; bsz: 4734/6051/233; 10848/13868 tok/s;    168 sec;
[2024-07-29 15:19:51,537 INFO] Step 50/10000; acc: 4.1; ppl: 1610.8; xent: 7.4; lr: 0.00319; sents:   18346; bsz: 4872/6146/229; 11103/14007 tok/s;    203 sec;
[2024-07-29 15:20:25,695 INFO] Step 60/10000; acc: 4.9; ppl: 1469.9; xent: 7.3; lr: 0.00381; sents:   18544; bsz: 4672/5859/232; 10942/13721 tok/s;    237 sec;
[2024-07-29 15:20:59,787 INFO] Step 70/10000; acc: 6.1; ppl: 1320.0; xent: 7.2; lr: 0.00444; sents:   20059; bsz: 4486/5807/251; 10528/13628 tok/s;    271 sec;
[2024-07-29 15:21:34,548 INFO] Step 80/10000; acc: 6.6; ppl: 1334.7; xent: 7.2; lr: 0.00506; sents:   18425; bsz: 4800/6129/230; 11047/14106 tok/s;    306 sec;
[2024-07-29 15:22:09,069 INFO] Step 90/10000; acc: 8.8; ppl: 1175.3; xent: 7.1; lr: 0.00569; sents:   18996; bsz: 4728/6053/237; 10957/14029 tok/s;    340 sec;
[2024-07-29 15:22:42,582 INFO] Step 100/10000; acc: 11.4; ppl: 991.6; xent: 6.9; lr: 0.00622; sents:   18678; bsz: 4549/5817/233; 10858/13885 tok/s;    374 sec;
[2024-07-29 15:23:17,731 INFO] Step 110/10000; acc: 13.9; ppl: 873.5; xent: 6.8; lr: 0.00593; sents:   19531; bsz: 4830/6149/244; 10992/13996 tok/s;    409 sec;
[2024-07-29 15:23:51,479 INFO] Step 120/10000; acc: 14.6; ppl: 824.1; xent: 6.7; lr: 0.00568; sents:   17976; bsz: 4683/5939/225; 11102/14078 tok/s;    443 sec;
[2024-07-29 15:24:26,186 INFO] Step 130/10000; acc: 15.2; ppl: 767.5; xent: 6.6; lr: 0.00546; sents:   18373; bsz: 4870/6143/230; 11225/14160 tok/s;    477 sec;
[2024-07-29 15:25:00,239 INFO] Step 140/10000; acc: 17.5; ppl: 642.4; xent: 6.5; lr: 0.00526; sents:   19130; bsz: 4628/5926/239; 10873/13921 tok/s;    511 sec;
[2024-07-29 15:25:34,395 INFO] Step 150/10000; acc: 18.7; ppl: 545.4; xent: 6.3; lr: 0.00509; sents:   19850; bsz: 4626/5940/248; 10835/13912 tok/s;    545 sec;
[2024-07-29 15:26:08,119 INFO] Step 160/10000; acc: 18.1; ppl: 520.2; xent: 6.3; lr: 0.00493; sents:   18007; bsz: 4684/5944/225; 11112/14100 tok/s;    579 sec;
[2024-07-29 15:26:42,402 INFO] Step 170/10000; acc: 19.6; ppl: 443.3; xent: 6.1; lr: 0.00478; sents:   19800; bsz: 4622/5908/248; 10786/13787 tok/s;    613 sec;
[2024-07-29 15:27:16,788 INFO] Step 180/10000; acc: 18.8; ppl: 442.2; xent: 6.1; lr: 0.00465; sents:   18609; bsz: 4788/6087/233; 11139/14161 tok/s;    648 sec;
[2024-07-29 15:27:50,744 INFO] Step 190/10000; acc: 18.6; ppl: 422.6; xent: 6.0; lr: 0.00452; sents:   17546; bsz: 4719/5977/219; 11119/14082 tok/s;    682 sec;
[2024-07-29 15:28:25,195 INFO] Step 200/10000; acc: 19.9; ppl: 374.0; xent: 5.9; lr: 0.00441; sents:   19172; bsz: 4721/6009/240; 10964/13955 tok/s;    716 sec;
[2024-07-29 15:28:58,942 INFO] Step 210/10000; acc: 19.5; ppl: 372.2; xent: 5.9; lr: 0.00430; sents:   18437; bsz: 4630/5897/230; 10975/13980 tok/s;    750 sec;
[2024-07-29 15:29:33,748 INFO] Step 220/10000; acc: 20.6; ppl: 338.2; xent: 5.8; lr: 0.00420; sents:   19786; bsz: 4702/6072/247; 10807/13957 tok/s;    785 sec;
[2024-07-29 15:30:08,581 INFO] Step 230/10000; acc: 20.4; ppl: 327.0; xent: 5.8; lr: 0.00411; sents:   18602; bsz: 4911/6198/233; 11279/14234 tok/s;    820 sec;
[2024-07-29 15:30:42,168 INFO] Step 240/10000; acc: 22.2; ppl: 285.9; xent: 5.7; lr: 0.00403; sents:   18788; bsz: 4481/5720/235; 10674/13625 tok/s;    853 sec;
[2024-07-29 15:31:16,742 INFO] Step 250/10000; acc: 22.1; ppl: 275.2; xent: 5.6; lr: 0.00394; sents:   18892; bsz: 4759/5993/236; 11012/13868 tok/s;    888 sec;
[2024-07-29 15:31:16,756 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_250.pt
[2024-07-29 15:31:52,506 INFO] Step 260/10000; acc: 23.8; ppl: 244.5; xent: 5.5; lr: 0.00387; sents:   19513; bsz: 4467/5741/244; 9991/12841 tok/s;    924 sec;
[2024-07-29 15:32:28,045 INFO] Step 270/10000; acc: 22.2; ppl: 253.7; xent: 5.5; lr: 0.00380; sents:   18236; bsz: 5036/6426/228; 11336/14465 tok/s;    959 sec;
[2024-07-29 15:33:01,671 INFO] Step 280/10000; acc: 24.3; ppl: 221.6; xent: 5.4; lr: 0.00373; sents:   18851; bsz: 4541/5785/236; 10804/13763 tok/s;    993 sec;
[2024-07-29 15:33:35,269 INFO] Step 290/10000; acc: 24.5; ppl: 212.4; xent: 5.4; lr: 0.00366; sents:   18232; bsz: 4554/5793/228; 10843/13795 tok/s;   1026 sec;
[2024-07-29 15:34:10,221 INFO] Step 300/10000; acc: 24.5; ppl: 203.0; xent: 5.3; lr: 0.00360; sents:   18239; bsz: 4919/6290/228; 11259/14397 tok/s;   1061 sec;
[2024-07-29 15:34:44,671 INFO] Step 310/10000; acc: 25.4; ppl: 191.8; xent: 5.3; lr: 0.00354; sents:   18995; bsz: 4755/6004/237; 11042/13943 tok/s;   1096 sec;
[2024-07-29 15:35:19,088 INFO] Step 320/10000; acc: 27.0; ppl: 169.4; xent: 5.1; lr: 0.00349; sents:   19397; bsz: 4633/5928/242; 10770/13779 tok/s;   1130 sec;
[2024-07-29 15:35:52,666 INFO] Step 330/10000; acc: 28.1; ppl: 154.8; xent: 5.0; lr: 0.00344; sents:   19330; bsz: 4466/5730/242; 10640/13651 tok/s;   1164 sec;
[2024-07-29 15:36:27,951 INFO] Step 340/10000; acc: 28.1; ppl: 150.1; xent: 5.0; lr: 0.00338; sents:   19368; bsz: 4855/6167/242; 11008/13983 tok/s;   1199 sec;
[2024-07-29 15:37:01,552 INFO] Step 350/10000; acc: 29.0; ppl: 138.1; xent: 4.9; lr: 0.00334; sents:   18504; bsz: 4506/5807/231; 10728/13825 tok/s;   1233 sec;
[2024-07-29 15:37:36,511 INFO] Step 360/10000; acc: 28.9; ppl: 134.7; xent: 4.9; lr: 0.00329; sents:   18543; bsz: 4892/6157/232; 11195/14089 tok/s;   1268 sec;
[2024-07-29 15:38:11,056 INFO] Step 370/10000; acc: 30.0; ppl: 123.4; xent: 4.8; lr: 0.00324; sents:   18792; bsz: 4717/6005/235; 10924/13906 tok/s;   1302 sec;
[2024-07-29 15:38:45,722 INFO] Step 380/10000; acc: 30.3; ppl: 118.0; xent: 4.8; lr: 0.00320; sents:   18804; bsz: 4843/6151/235; 11176/14195 tok/s;   1337 sec;
[2024-07-29 15:39:18,956 INFO] Step 390/10000; acc: 32.2; ppl: 106.8; xent: 4.7; lr: 0.00316; sents:   19470; bsz: 4359/5660/243; 10492/13625 tok/s;   1370 sec;
[2024-07-29 15:39:53,763 INFO] Step 400/10000; acc: 31.7; ppl: 104.1; xent: 4.6; lr: 0.00312; sents:   18519; bsz: 4813/6069/231; 11063/13950 tok/s;   1405 sec;
[2024-07-29 15:40:28,608 INFO] Step 410/10000; acc: 31.7; ppl:  99.3; xent: 4.6; lr: 0.00308; sents:   17964; bsz: 4901/6243/225; 11253/14334 tok/s;   1440 sec;
[2024-07-29 15:41:02,565 INFO] Step 420/10000; acc: 33.4; ppl:  90.5; xent: 4.5; lr: 0.00305; sents:   18527; bsz: 4589/5801/232; 10811/13666 tok/s;   1474 sec;
[2024-07-29 15:41:37,174 INFO] Step 430/10000; acc: 33.8; ppl:  87.1; xent: 4.5; lr: 0.00301; sents:   19347; bsz: 4814/6090/242; 11127/14077 tok/s;   1508 sec;
[2024-07-29 15:42:11,665 INFO] Step 440/10000; acc: 35.0; ppl:  79.8; xent: 4.4; lr: 0.00298; sents:   19142; bsz: 4607/5909/239; 10685/13705 tok/s;   1543 sec;
[2024-07-29 15:42:46,230 INFO] Step 450/10000; acc: 35.4; ppl:  76.4; xent: 4.3; lr: 0.00294; sents:   19068; bsz: 4707/6008/238; 10894/13905 tok/s;   1577 sec;
[2024-07-29 15:43:19,856 INFO] Step 460/10000; acc: 36.5; ppl:  70.4; xent: 4.3; lr: 0.00291; sents:   18617; bsz: 4521/5749/233; 10757/13676 tok/s;   1611 sec;
[2024-07-29 15:43:54,709 INFO] Step 470/10000; acc: 36.6; ppl:  68.0; xent: 4.2; lr: 0.00288; sents:   18571; bsz: 4846/6146/232; 11123/14108 tok/s;   1646 sec;
[2024-07-29 15:44:29,003 INFO] Step 480/10000; acc: 37.9; ppl:  63.4; xent: 4.2; lr: 0.00285; sents:   19279; bsz: 4667/5990/241; 10887/13974 tok/s;   1680 sec;
[2024-07-29 15:45:03,310 INFO] Step 490/10000; acc: 38.7; ppl:  60.2; xent: 4.1; lr: 0.00282; sents:   19082; bsz: 4711/6015/239; 10985/14027 tok/s;   1714 sec;
[2024-07-29 15:45:38,096 INFO] Step 500/10000; acc: 39.3; ppl:  56.3; xent: 4.0; lr: 0.00279; sents:   18390; bsz: 4880/6162/230; 11222/14171 tok/s;   1749 sec;
[2024-07-29 15:45:38,109 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_500.pt
[2024-07-29 15:46:13,608 INFO] Step 510/10000; acc: 40.6; ppl:  53.8; xent: 4.0; lr: 0.00276; sents:   19483; bsz: 4369/5619/244; 9843/12659 tok/s;   1785 sec;
[2024-07-29 15:46:48,342 INFO] Step 520/10000; acc: 40.7; ppl:  50.9; xent: 3.9; lr: 0.00274; sents:   18233; bsz: 4870/6177/228; 11217/14226 tok/s;   1819 sec;
[2024-07-29 15:47:23,021 INFO] Step 530/10000; acc: 41.5; ppl:  48.7; xent: 3.9; lr: 0.00271; sents:   18646; bsz: 4894/6183/233; 11289/14263 tok/s;   1854 sec;
[2024-07-29 15:47:56,693 INFO] Step 540/10000; acc: 42.4; ppl:  46.2; xent: 3.8; lr: 0.00269; sents:   18003; bsz: 4603/5909/225; 10936/14040 tok/s;   1888 sec;
[2024-07-29 15:48:30,670 INFO] Step 550/10000; acc: 43.8; ppl:  44.2; xent: 3.8; lr: 0.00266; sents:   20311; bsz: 4480/5768/254; 10548/13581 tok/s;   1922 sec;
[2024-07-29 15:49:05,234 INFO] Step 560/10000; acc: 43.4; ppl:  43.1; xent: 3.8; lr: 0.00264; sents:   18764; bsz: 4746/6034/235; 10985/13966 tok/s;   1956 sec;
[2024-07-29 15:49:39,918 INFO] Step 570/10000; acc: 44.6; ppl:  40.8; xent: 3.7; lr: 0.00262; sents:   18974; bsz: 4761/6013/237; 10981/13870 tok/s;   1991 sec;
[2024-07-29 15:50:13,514 INFO] Step 580/10000; acc: 45.6; ppl:  38.1; xent: 3.6; lr: 0.00259; sents:   18348; bsz: 4503/5821/229; 10723/13861 tok/s;   2025 sec;
[2024-07-29 15:50:49,595 INFO] Step 590/10000; acc: 45.6; ppl:  37.7; xent: 3.6; lr: 0.00257; sents:   18487; bsz: 5193/6475/231; 11514/14358 tok/s;   2061 sec;
[2024-07-29 15:51:22,880 INFO] Step 600/10000; acc: 47.2; ppl:  35.9; xent: 3.6; lr: 0.00255; sents:   19379; bsz: 4374/5644/242; 10512/13564 tok/s;   2094 sec;
[2024-07-29 15:51:57,509 INFO] Step 610/10000; acc: 47.2; ppl:  34.4; xent: 3.5; lr: 0.00253; sents:   18489; bsz: 4778/6077/231; 11039/14040 tok/s;   2129 sec;
[2024-07-29 15:52:31,459 INFO] Step 620/10000; acc: 48.1; ppl:  33.0; xent: 3.5; lr: 0.00251; sents:   18212; bsz: 4706/5938/228; 11088/13992 tok/s;   2163 sec;
[2024-07-29 15:53:06,622 INFO] Step 630/10000; acc: 48.8; ppl:  32.2; xent: 3.5; lr: 0.00249; sents:   19893; bsz: 4793/6154/249; 10904/14002 tok/s;   2198 sec;
[2024-07-29 15:53:40,270 INFO] Step 640/10000; acc: 50.0; ppl:  30.3; xent: 3.4; lr: 0.00247; sents:   18660; bsz: 4516/5788/233; 10737/13761 tok/s;   2231 sec;
[2024-07-29 15:54:14,318 INFO] Step 650/10000; acc: 50.2; ppl:  29.3; xent: 3.4; lr: 0.00245; sents:   18148; bsz: 4724/5975/227; 11099/14040 tok/s;   2265 sec;
[2024-07-29 15:54:48,802 INFO] Step 660/10000; acc: 50.6; ppl:  29.1; xent: 3.4; lr: 0.00243; sents:   18987; bsz: 4688/5921/237; 10876/13735 tok/s;   2300 sec;
[2024-07-29 15:55:22,880 INFO] Step 670/10000; acc: 51.8; ppl:  27.5; xent: 3.3; lr: 0.00241; sents:   19957; bsz: 4533/5890/249; 10641/13826 tok/s;   2334 sec;
[2024-07-29 15:55:57,801 INFO] Step 680/10000; acc: 52.0; ppl:  26.8; xent: 3.3; lr: 0.00240; sents:   18289; bsz: 4885/6170/229; 11191/14133 tok/s;   2369 sec;
[2024-07-29 15:56:32,276 INFO] Step 690/10000; acc: 52.6; ppl:  26.1; xent: 3.3; lr: 0.00238; sents:   18787; bsz: 4749/6027/235; 11021/13986 tok/s;   2403 sec;
[2024-07-29 15:57:06,402 INFO] Step 700/10000; acc: 53.6; ppl:  25.0; xent: 3.2; lr: 0.00236; sents:   19763; bsz: 4474/5846/247; 10487/13703 tok/s;   2437 sec;
[2024-07-29 15:57:40,369 INFO] Step 710/10000; acc: 53.0; ppl:  25.5; xent: 3.2; lr: 0.00234; sents:   18171; bsz: 4847/6049/227; 11415/14246 tok/s;   2471 sec;
[2024-07-29 15:58:14,849 INFO] Step 720/10000; acc: 54.3; ppl:  23.9; xent: 3.2; lr: 0.00233; sents:   18744; bsz: 4691/5987/234; 10884/13892 tok/s;   2506 sec;
[2024-07-29 15:58:49,388 INFO] Step 730/10000; acc: 55.0; ppl:  23.3; xent: 3.1; lr: 0.00231; sents:   19185; bsz: 4811/6074/240; 11142/14069 tok/s;   2540 sec;
[2024-07-29 15:59:23,053 INFO] Step 740/10000; acc: 55.6; ppl:  22.4; xent: 3.1; lr: 0.00230; sents:   17961; bsz: 4553/5809/225; 10821/13804 tok/s;   2574 sec;
[2024-07-29 15:59:58,048 INFO] Step 750/10000; acc: 56.3; ppl:  21.9; xent: 3.1; lr: 0.00228; sents:   20059; bsz: 4784/6134/251; 10937/14023 tok/s;   2609 sec;
[2024-07-29 15:59:58,062 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_750.pt
[2024-07-29 16:00:34,311 INFO] Step 760/10000; acc: 57.0; ppl:  21.1; xent: 3.0; lr: 0.00227; sents:   18389; bsz: 4640/5907/230; 10236/13032 tok/s;   2645 sec;
[2024-07-29 16:01:08,607 INFO] Step 770/10000; acc: 57.7; ppl:  20.5; xent: 3.0; lr: 0.00225; sents:   19246; bsz: 4670/5993/241; 10895/13981 tok/s;   2680 sec;
[2024-07-29 16:01:43,260 INFO] Step 780/10000; acc: 58.0; ppl:  20.3; xent: 3.0; lr: 0.00224; sents:   18691; bsz: 4794/6050/234; 11068/13967 tok/s;   2714 sec;
[2024-07-29 16:02:17,583 INFO] Step 790/10000; acc: 59.2; ppl:  18.8; xent: 2.9; lr: 0.00222; sents:   17278; bsz: 4860/6151/216; 11327/14337 tok/s;   2749 sec;
[2024-07-29 16:02:51,489 INFO] Step 800/10000; acc: 59.0; ppl:  19.7; xent: 3.0; lr: 0.00221; sents:   20404; bsz: 4409/5694/255; 10404/13435 tok/s;   2783 sec;
[2024-07-29 16:03:26,526 INFO] Step 810/10000; acc: 60.2; ppl:  18.3; xent: 2.9; lr: 0.00219; sents:   18052; bsz: 4929/6222/226; 11254/14207 tok/s;   2818 sec;
[2024-07-29 16:04:00,151 INFO] Step 820/10000; acc: 60.3; ppl:  18.4; xent: 2.9; lr: 0.00218; sents:   19120; bsz: 4452/5687/239; 10591/13531 tok/s;   2851 sec;
[2024-07-29 16:04:35,189 INFO] Step 830/10000; acc: 61.0; ppl:  17.5; xent: 2.9; lr: 0.00217; sents:   17673; bsz: 4976/6267/221; 11362/14309 tok/s;   2886 sec;
[2024-07-29 16:05:09,294 INFO] Step 840/10000; acc: 60.4; ppl:  18.2; xent: 2.9; lr: 0.00216; sents:   20042; bsz: 4504/5832/251; 10566/13680 tok/s;   2920 sec;
[2024-07-29 16:05:43,993 INFO] Step 850/10000; acc: 62.0; ppl:  16.9; xent: 2.8; lr: 0.00214; sents:   18247; bsz: 4864/6196/228; 11214/14285 tok/s;   2955 sec;
[2024-07-29 16:06:18,838 INFO] Step 860/10000; acc: 62.5; ppl:  16.5; xent: 2.8; lr: 0.00213; sents:   18320; bsz: 4851/6110/229; 11137/14027 tok/s;   2990 sec;
[2024-07-29 16:06:52,787 INFO] Step 870/10000; acc: 62.5; ppl:  16.8; xent: 2.8; lr: 0.00212; sents:   20082; bsz: 4431/5777/251; 10442/13614 tok/s;   3024 sec;
[2024-07-29 16:07:26,647 INFO] Step 880/10000; acc: 63.3; ppl:  16.2; xent: 2.8; lr: 0.00211; sents:   18969; bsz: 4625/5824/237; 10927/13760 tok/s;   3058 sec;
[2024-07-29 16:08:00,912 INFO] Step 890/10000; acc: 64.1; ppl:  15.7; xent: 2.8; lr: 0.00209; sents:   19496; bsz: 4585/5888/244; 10705/13747 tok/s;   3092 sec;
[2024-07-29 16:08:35,580 INFO] Step 900/10000; acc: 65.1; ppl:  14.8; xent: 2.7; lr: 0.00208; sents:   16442; bsz: 5053/6267/206; 11660/14462 tok/s;   3127 sec;
[2024-07-29 16:09:09,975 INFO] Step 910/10000; acc: 64.7; ppl:  15.5; xent: 2.7; lr: 0.00207; sents:   21444; bsz: 4447/5849/268; 10343/13605 tok/s;   3161 sec;
[2024-07-29 16:09:44,175 INFO] Step 920/10000; acc: 66.0; ppl:  14.3; xent: 2.7; lr: 0.00206; sents:   17519; bsz: 4769/6000/219; 11155/14035 tok/s;   3195 sec;
[2024-07-29 16:10:18,582 INFO] Step 930/10000; acc: 65.9; ppl:  14.5; xent: 2.7; lr: 0.00205; sents:   19112; bsz: 4741/6026/239; 11024/14010 tok/s;   3230 sec;
[2024-07-29 16:10:53,094 INFO] Step 940/10000; acc: 67.5; ppl:  13.5; xent: 2.6; lr: 0.00204; sents:   18163; bsz: 4757/6136/227; 11027/14224 tok/s;   3264 sec;
[2024-07-29 16:11:26,209 INFO] Step 950/10000; acc: 65.8; ppl:  14.8; xent: 2.7; lr: 0.00203; sents:   20709; bsz: 4311/5513/259; 10415/13317 tok/s;   3297 sec;
[2024-07-29 16:12:01,177 INFO] Step 960/10000; acc: 67.9; ppl:  13.3; xent: 2.6; lr: 0.00202; sents:   17597; bsz: 4949/6235/220; 11323/14265 tok/s;   3332 sec;
[2024-07-29 16:12:35,683 INFO] Step 970/10000; acc: 68.2; ppl:  13.2; xent: 2.6; lr: 0.00201; sents:   18693; bsz: 4798/6129/234; 11125/14210 tok/s;   3367 sec;
[2024-07-29 16:13:09,048 INFO] Step 980/10000; acc: 67.5; ppl:  13.7; xent: 2.6; lr: 0.00200; sents:   19024; bsz: 4464/5711/238; 10703/13694 tok/s;   3400 sec;
[2024-07-29 16:13:43,791 INFO] Step 990/10000; acc: 69.1; ppl:  12.8; xent: 2.6; lr: 0.00199; sents:   18766; bsz: 4841/6105/235; 11147/14057 tok/s;   3435 sec;
[2024-07-29 16:14:18,188 INFO] Step 1000/10000; acc: 69.0; ppl:  12.9; xent: 2.6; lr: 0.00198; sents:   18701; bsz: 4728/6018/234; 10996/13998 tok/s;   3469 sec;
[2024-07-29 16:14:18,204 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_1000.pt
[2024-07-29 16:14:54,510 INFO] Step 1010/10000; acc: 69.7; ppl:  12.6; xent: 2.5; lr: 0.00197; sents:   19485; bsz: 4581/5899/244; 10090/12993 tok/s;   3506 sec;
[2024-07-29 16:15:29,267 INFO] Step 1020/10000; acc: 70.5; ppl:  12.2; xent: 2.5; lr: 0.00196; sents:   18133; bsz: 4954/6252/227; 11402/14391 tok/s;   3540 sec;
[2024-07-29 16:16:02,719 INFO] Step 1030/10000; acc: 71.3; ppl:  11.9; xent: 2.5; lr: 0.00195; sents:   18357; bsz: 4560/5833/229; 10905/13951 tok/s;   3574 sec;
[2024-07-29 16:16:37,015 INFO] Step 1040/10000; acc: 70.1; ppl:  12.4; xent: 2.5; lr: 0.00194; sents:   19544; bsz: 4650/5919/244; 10848/13808 tok/s;   3608 sec;
[2024-07-29 16:17:11,194 INFO] Step 1050/10000; acc: 71.0; ppl:  12.0; xent: 2.5; lr: 0.00193; sents:   19513; bsz: 4662/5949/244; 10911/13925 tok/s;   3642 sec;
[2024-07-29 16:17:44,856 INFO] Step 1060/10000; acc: 72.3; ppl:  11.4; xent: 2.4; lr: 0.00192; sents:   18302; bsz: 4652/5876/229; 11057/13966 tok/s;   3676 sec;
[2024-07-29 16:18:19,254 INFO] Step 1070/10000; acc: 72.7; ppl:  11.3; xent: 2.4; lr: 0.00191; sents:   18624; bsz: 4791/6135/233; 11143/14269 tok/s;   3710 sec;
[2024-07-29 16:18:53,619 INFO] Step 1080/10000; acc: 72.1; ppl:  11.5; xent: 2.4; lr: 0.00190; sents:   18958; bsz: 4702/5992/237; 10946/13949 tok/s;   3745 sec;
[2024-07-29 16:19:27,783 INFO] Step 1090/10000; acc: 73.1; ppl:  11.2; xent: 2.4; lr: 0.00189; sents:   18840; bsz: 4659/5991/236; 10909/14028 tok/s;   3779 sec;
[2024-07-29 16:20:01,524 INFO] Step 1100/10000; acc: 73.4; ppl:  11.0; xent: 2.4; lr: 0.00188; sents:   18864; bsz: 4678/5862/236; 11092/13898 tok/s;   3813 sec;
[2024-07-29 16:20:36,330 INFO] Step 1110/10000; acc: 73.8; ppl:  10.8; xent: 2.4; lr: 0.00188; sents:   19908; bsz: 4702/6108/249; 10808/14039 tok/s;   3847 sec;
[2024-07-29 16:21:10,435 INFO] Step 1120/10000; acc: 74.3; ppl:  10.6; xent: 2.4; lr: 0.00187; sents:   17618; bsz: 4786/5990/220; 11226/14051 tok/s;   3882 sec;
[2024-07-29 16:21:44,738 INFO] Step 1130/10000; acc: 74.1; ppl:  10.7; xent: 2.4; lr: 0.00186; sents:   19034; bsz: 4800/6109/238; 11195/14247 tok/s;   3916 sec;
[2024-07-29 16:22:18,233 INFO] Step 1140/10000; acc: 74.7; ppl:  10.5; xent: 2.4; lr: 0.00185; sents:   18517; bsz: 4485/5720/231; 10712/13662 tok/s;   3949 sec;
[2024-07-29 16:22:52,601 INFO] Step 1150/10000; acc: 75.1; ppl:  10.4; xent: 2.3; lr: 0.00184; sents:   18831; bsz: 4768/6103/235; 11098/14206 tok/s;   3984 sec;
[2024-07-29 16:23:26,914 INFO] Step 1160/10000; acc: 75.2; ppl:  10.3; xent: 2.3; lr: 0.00183; sents:   19335; bsz: 4683/5977/242; 10919/13935 tok/s;   4018 sec;
[2024-07-29 16:24:01,094 INFO] Step 1170/10000; acc: 75.2; ppl:  10.3; xent: 2.3; lr: 0.00183; sents:   19355; bsz: 4617/5897/242; 10806/13802 tok/s;   4052 sec;
[2024-07-29 16:24:34,958 INFO] Step 1180/10000; acc: 76.0; ppl:  10.0; xent: 2.3; lr: 0.00182; sents:   18435; bsz: 4695/5948/230; 11091/14051 tok/s;   4086 sec;
[2024-07-29 16:25:09,694 INFO] Step 1190/10000; acc: 77.6; ppl:   9.4; xent: 2.2; lr: 0.00181; sents:   17229; bsz: 4968/6320/215; 11442/14555 tok/s;   4121 sec;
[2024-07-29 16:25:43,754 INFO] Step 1200/10000; acc: 75.8; ppl:  10.2; xent: 2.3; lr: 0.00180; sents:   20134; bsz: 4542/5797/252; 10668/13617 tok/s;   4155 sec;
[2024-07-29 16:26:17,893 INFO] Step 1210/10000; acc: 77.1; ppl:   9.6; xent: 2.3; lr: 0.00180; sents:   19262; bsz: 4624/5969/241; 10836/13987 tok/s;   4189 sec;
[2024-07-29 16:26:52,459 INFO] Step 1220/10000; acc: 77.1; ppl:   9.7; xent: 2.3; lr: 0.00179; sents:   19090; bsz: 4795/6061/239; 11098/14027 tok/s;   4224 sec;
[2024-07-29 16:27:26,343 INFO] Step 1230/10000; acc: 77.7; ppl:   9.5; xent: 2.2; lr: 0.00178; sents:   17818; bsz: 4750/5981/223; 11215/14121 tok/s;   4257 sec;
[2024-07-29 16:28:00,524 INFO] Step 1240/10000; acc: 77.7; ppl:   9.5; xent: 2.2; lr: 0.00177; sents:   19299; bsz: 4598/5894/241; 10762/13795 tok/s;   4292 sec;
[2024-07-29 16:28:35,343 INFO] Step 1250/10000; acc: 79.5; ppl:   8.9; xent: 2.2; lr: 0.00177; sents:   17628; bsz: 4986/6288/220; 11456/14448 tok/s;   4326 sec;
[2024-07-29 16:28:35,356 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_1250.pt
[2024-07-29 16:29:11,464 INFO] Step 1260/10000; acc: 77.9; ppl:   9.4; xent: 2.2; lr: 0.00176; sents:   19853; bsz: 4576/5896/248; 10134/13059 tok/s;   4363 sec;
[2024-07-29 16:29:44,529 INFO] Step 1270/10000; acc: 77.8; ppl:   9.4; xent: 2.2; lr: 0.00175; sents:   19781; bsz: 4283/5546/247; 10364/13418 tok/s;   4396 sec;
[2024-07-29 16:30:19,505 INFO] Step 1280/10000; acc: 80.0; ppl:   8.7; xent: 2.2; lr: 0.00175; sents:   18039; bsz: 4968/6229/225; 11363/14249 tok/s;   4431 sec;
[2024-07-29 16:30:54,118 INFO] Step 1290/10000; acc: 79.8; ppl:   8.8; xent: 2.2; lr: 0.00174; sents:   18285; bsz: 4891/6204/229; 11303/14340 tok/s;   4465 sec;
[2024-07-29 16:31:27,956 INFO] Step 1300/10000; acc: 78.8; ppl:   9.1; xent: 2.2; lr: 0.00173; sents:   20510; bsz: 4388/5647/256; 10374/13351 tok/s;   4499 sec;
[2024-07-29 16:32:01,798 INFO] Step 1310/10000; acc: 81.0; ppl:   8.4; xent: 2.1; lr: 0.00173; sents:   17470; bsz: 4710/6077/218; 11135/14365 tok/s;   4533 sec;
[2024-07-29 16:32:36,447 INFO] Step 1320/10000; acc: 79.3; ppl:   8.9; xent: 2.2; lr: 0.00172; sents:   19065; bsz: 4814/6028/238; 11116/13919 tok/s;   4568 sec;
[2024-07-29 16:33:10,600 INFO] Step 1330/10000; acc: 81.3; ppl:   8.4; xent: 2.1; lr: 0.00171; sents:   17248; bsz: 4860/6107/216; 11385/14304 tok/s;   4602 sec;
[2024-07-29 16:33:44,525 INFO] Step 1340/10000; acc: 79.5; ppl:   8.9; xent: 2.2; lr: 0.00171; sents:   20074; bsz: 4479/5788/251; 10562/13649 tok/s;   4636 sec;
[2024-07-29 16:34:19,202 INFO] Step 1350/10000; acc: 81.2; ppl:   8.4; xent: 2.1; lr: 0.00170; sents:   18202; bsz: 4861/6158/228; 11215/14207 tok/s;   4670 sec;
[2024-07-29 16:34:53,366 INFO] Step 1360/10000; acc: 80.4; ppl:   8.6; xent: 2.2; lr: 0.00169; sents:   19748; bsz: 4572/5862/247; 10707/13727 tok/s;   4704 sec;
[2024-07-29 16:35:28,296 INFO] Step 1370/10000; acc: 82.6; ppl:   8.0; xent: 2.1; lr: 0.00169; sents:   17757; bsz: 5010/6311/222; 11474/14455 tok/s;   4739 sec;
[2024-07-29 16:36:01,227 INFO] Step 1380/10000; acc: 80.5; ppl:   8.6; xent: 2.2; lr: 0.00168; sents:   19607; bsz: 4274/5536/245; 10382/13449 tok/s;   4772 sec;
[2024-07-29 16:36:35,615 INFO] Step 1390/10000; acc: 81.5; ppl:   8.3; xent: 2.1; lr: 0.00168; sents:   19335; bsz: 4788/6081/242; 11139/14146 tok/s;   4807 sec;
[2024-07-29 16:37:10,133 INFO] Step 1400/10000; acc: 82.2; ppl:   8.1; xent: 2.1; lr: 0.00167; sents:   18853; bsz: 4748/6032/236; 11005/13981 tok/s;   4841 sec;
[2024-07-29 16:37:43,882 INFO] Step 1410/10000; acc: 82.2; ppl:   8.1; xent: 2.1; lr: 0.00166; sents:   18181; bsz: 4659/5911/227; 11044/14012 tok/s;   4875 sec;
[2024-07-29 16:38:18,964 INFO] Step 1420/10000; acc: 82.5; ppl:   8.0; xent: 2.1; lr: 0.00166; sents:   19982; bsz: 4790/6103/250; 10922/13917 tok/s;   4910 sec;
[2024-07-29 16:38:52,826 INFO] Step 1430/10000; acc: 83.1; ppl:   7.9; xent: 2.1; lr: 0.00165; sents:   17454; bsz: 4720/6017/218; 11152/14216 tok/s;   4944 sec;
[2024-07-29 16:39:27,037 INFO] Step 1440/10000; acc: 82.4; ppl:   8.0; xent: 2.1; lr: 0.00165; sents:   19659; bsz: 4622/5914/246; 10808/13828 tok/s;   4978 sec;
[2024-07-29 16:40:01,471 INFO] Step 1450/10000; acc: 82.8; ppl:   7.9; xent: 2.1; lr: 0.00164; sents:   19083; bsz: 4774/6058/239; 11091/14076 tok/s;   5013 sec;
[2024-07-29 16:40:36,315 INFO] Step 1460/10000; acc: 84.5; ppl:   7.5; xent: 2.0; lr: 0.00164; sents:   17712; bsz: 4924/6228/221; 11305/14300 tok/s;   5047 sec;
[2024-07-29 16:41:09,397 INFO] Step 1470/10000; acc: 82.4; ppl:   8.0; xent: 2.1; lr: 0.00163; sents:   19961; bsz: 4249/5499/250; 10275/13297 tok/s;   5080 sec;
[2024-07-29 16:41:44,005 INFO] Step 1480/10000; acc: 84.0; ppl:   7.6; xent: 2.0; lr: 0.00162; sents:   18720; bsz: 4839/6153/234; 11187/14224 tok/s;   5115 sec;
[2024-07-29 16:42:17,441 INFO] Step 1490/10000; acc: 83.0; ppl:   7.9; xent: 2.1; lr: 0.00162; sents:   19488; bsz: 4397/5643/244; 10521/13502 tok/s;   5149 sec;
[2024-07-29 16:42:52,688 INFO] Step 1500/10000; acc: 85.7; ppl:   7.2; xent: 2.0; lr: 0.00161; sents:   16491; bsz: 5159/6442/206; 11709/14621 tok/s;   5184 sec;
[2024-07-29 16:42:52,701 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_1500.pt
[2024-07-29 16:43:28,935 INFO] Step 1510/10000; acc: 82.5; ppl:   8.0; xent: 2.1; lr: 0.00161; sents:   20788; bsz: 4420/5758/260; 9755/12708 tok/s;   5220 sec;
[2024-07-29 16:44:03,424 INFO] Step 1520/10000; acc: 84.7; ppl:   7.5; xent: 2.0; lr: 0.00160; sents:   18768; bsz: 4751/6023/235; 11020/13971 tok/s;   5255 sec;
[2024-07-29 16:44:37,833 INFO] Step 1530/10000; acc: 84.3; ppl:   7.5; xent: 2.0; lr: 0.00160; sents:   19176; bsz: 4735/6018/240; 11010/13993 tok/s;   5289 sec;
[2024-07-29 16:45:11,705 INFO] Step 1540/10000; acc: 85.8; ppl:   7.2; xent: 2.0; lr: 0.00159; sents:   17708; bsz: 4651/5999/221; 10986/14168 tok/s;   5323 sec;
[2024-07-29 16:45:46,151 INFO] Step 1550/10000; acc: 84.4; ppl:   7.5; xent: 2.0; lr: 0.00159; sents:   19066; bsz: 4688/5909/238; 10887/13725 tok/s;   5357 sec;
[2024-07-29 16:46:20,691 INFO] Step 1560/10000; acc: 85.1; ppl:   7.3; xent: 2.0; lr: 0.00158; sents:   19139; bsz: 4797/6089/239; 11110/14104 tok/s;   5392 sec;
[2024-07-29 16:46:55,673 INFO] Step 1570/10000; acc: 86.0; ppl:   7.1; xent: 2.0; lr: 0.00158; sents:   18082; bsz: 4904/6163/226; 11216/14094 tok/s;   5427 sec;
[2024-07-29 16:47:29,663 INFO] Step 1580/10000; acc: 84.7; ppl:   7.4; xent: 2.0; lr: 0.00157; sents:   19660; bsz: 4606/5964/246; 10841/14036 tok/s;   5461 sec;
[2024-07-29 16:48:03,352 INFO] Step 1590/10000; acc: 85.1; ppl:   7.3; xent: 2.0; lr: 0.00157; sents:   19147; bsz: 4520/5744/239; 10732/13639 tok/s;   5494 sec;
[2024-07-29 16:48:38,057 INFO] Step 1600/10000; acc: 86.3; ppl:   7.1; xent: 2.0; lr: 0.00156; sents:   18554; bsz: 4751/6042/232; 10953/13929 tok/s;   5529 sec;
[2024-07-29 16:49:11,925 INFO] Step 1610/10000; acc: 86.4; ppl:   7.0; xent: 1.9; lr: 0.00156; sents:   18071; bsz: 4688/5951/226; 11073/14058 tok/s;   5563 sec;
[2024-07-29 16:49:47,131 INFO] Step 1620/10000; acc: 85.8; ppl:   7.2; xent: 2.0; lr: 0.00155; sents:   19711; bsz: 4845/6195/246; 11009/14078 tok/s;   5598 sec;
[2024-07-29 16:50:20,560 INFO] Step 1630/10000; acc: 84.5; ppl:   7.4; xent: 2.0; lr: 0.00155; sents:   19837; bsz: 4289/5486/248; 10263/13128 tok/s;   5632 sec;
[2024-07-29 16:50:55,621 INFO] Step 1640/10000; acc: 87.5; ppl:   6.8; xent: 1.9; lr: 0.00154; sents:   17603; bsz: 4993/6340/220; 11393/14466 tok/s;   5667 sec;
[2024-07-29 16:51:30,331 INFO] Step 1650/10000; acc: 86.2; ppl:   7.0; xent: 2.0; lr: 0.00154; sents:   19098; bsz: 4812/6063/239; 11092/13975 tok/s;   5701 sec;
[2024-07-29 16:52:04,738 INFO] Step 1660/10000; acc: 86.0; ppl:   7.1; xent: 2.0; lr: 0.00153; sents:   19481; bsz: 4583/5891/244; 10656/13698 tok/s;   5736 sec;
[2024-07-29 16:52:38,817 INFO] Step 1670/10000; acc: 87.3; ppl:   6.8; xent: 1.9; lr: 0.00153; sents:   17439; bsz: 4733/6035/218; 11112/14168 tok/s;   5770 sec;
[2024-07-29 16:53:13,194 INFO] Step 1680/10000; acc: 86.6; ppl:   6.9; xent: 1.9; lr: 0.00152; sents:   19417; bsz: 4658/5946/243; 10841/13838 tok/s;   5804 sec;
[2024-07-29 16:53:47,923 INFO] Step 1690/10000; acc: 87.6; ppl:   6.7; xent: 1.9; lr: 0.00152; sents:   18343; bsz: 4824/6133/229; 11113/14128 tok/s;   5839 sec;
[2024-07-29 16:54:22,400 INFO] Step 1700/10000; acc: 87.4; ppl:   6.8; xent: 1.9; lr: 0.00152; sents:   18913; bsz: 4754/6029/236; 11030/13990 tok/s;   5873 sec;
[2024-07-29 16:54:56,327 INFO] Step 1710/10000; acc: 87.9; ppl:   6.7; xent: 1.9; lr: 0.00151; sents:   17733; bsz: 4772/6019/222; 11251/14193 tok/s;   5907 sec;
[2024-07-29 16:55:30,571 INFO] Step 1720/10000; acc: 86.5; ppl:   6.9; xent: 1.9; lr: 0.00151; sents:   20083; bsz: 4500/5804/251; 10514/13559 tok/s;   5942 sec;
[2024-07-29 16:56:04,767 INFO] Step 1730/10000; acc: 87.0; ppl:   6.8; xent: 1.9; lr: 0.00150; sents:   19777; bsz: 4627/5888/247; 10826/13776 tok/s;   5976 sec;
[2024-07-29 16:56:39,327 INFO] Step 1740/10000; acc: 87.9; ppl:   6.7; xent: 1.9; lr: 0.00150; sents:   18746; bsz: 4703/6027/234; 10886/13952 tok/s;   6010 sec;
[2024-07-29 16:57:13,258 INFO] Step 1750/10000; acc: 87.9; ppl:   6.7; xent: 1.9; lr: 0.00149; sents:   18232; bsz: 4706/5956/228; 11096/14043 tok/s;   6044 sec;
[2024-07-29 16:57:13,269 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_1750.pt
[2024-07-29 16:57:50,150 INFO] Step 1760/10000; acc: 88.1; ppl:   6.6; xent: 1.9; lr: 0.00149; sents:   19078; bsz: 4686/6016/238; 10161/13046 tok/s;   6081 sec;
[2024-07-29 16:58:25,194 INFO] Step 1770/10000; acc: 88.5; ppl:   6.5; xent: 1.9; lr: 0.00149; sents:   18008; bsz: 4897/6174/225; 11180/14095 tok/s;   6116 sec;
[2024-07-29 16:58:58,536 INFO] Step 1780/10000; acc: 87.2; ppl:   6.8; xent: 1.9; lr: 0.00148; sents:   19707; bsz: 4363/5582/246; 10467/13394 tok/s;   6150 sec;
[2024-07-29 16:59:32,774 INFO] Step 1790/10000; acc: 88.1; ppl:   6.6; xent: 1.9; lr: 0.00148; sents:   19436; bsz: 4663/6016/243; 10896/14056 tok/s;   6184 sec;
[2024-07-29 17:00:07,784 INFO] Step 1800/10000; acc: 88.9; ppl:   6.5; xent: 1.9; lr: 0.00147; sents:   18044; bsz: 4888/6175/226; 11169/14111 tok/s;   6219 sec;
[2024-07-29 17:00:42,561 INFO] Step 1810/10000; acc: 88.8; ppl:   6.5; xent: 1.9; lr: 0.00147; sents:   18499; bsz: 4805/6050/231; 11052/13917 tok/s;   6254 sec;
[2024-07-29 17:01:16,248 INFO] Step 1820/10000; acc: 88.6; ppl:   6.5; xent: 1.9; lr: 0.00146; sents:   18657; bsz: 4544/5884/233; 10791/13973 tok/s;   6287 sec;
[2024-07-29 17:01:50,825 INFO] Step 1830/10000; acc: 88.8; ppl:   6.4; xent: 1.9; lr: 0.00146; sents:   19245; bsz: 4702/5980/241; 10879/13835 tok/s;   6322 sec;
[2024-07-29 17:02:25,226 INFO] Step 1840/10000; acc: 88.7; ppl:   6.5; xent: 1.9; lr: 0.00146; sents:   19085; bsz: 4699/5975/239; 10928/13895 tok/s;   6356 sec;
[2024-07-29 17:02:59,291 INFO] Step 1850/10000; acc: 89.5; ppl:   6.3; xent: 1.8; lr: 0.00145; sents:   17595; bsz: 4778/6003/220; 11220/14099 tok/s;   6390 sec;
[2024-07-29 17:03:34,263 INFO] Step 1860/10000; acc: 88.9; ppl:   6.4; xent: 1.9; lr: 0.00145; sents:   19665; bsz: 4808/6172/246; 10999/14118 tok/s;   6425 sec;
[2024-07-29 17:04:07,507 INFO] Step 1870/10000; acc: 88.3; ppl:   6.5; xent: 1.9; lr: 0.00144; sents:   19540; bsz: 4373/5670/244; 10523/13644 tok/s;   6459 sec;
[2024-07-29 17:04:42,229 INFO] Step 1880/10000; acc: 89.8; ppl:   6.3; xent: 1.8; lr: 0.00144; sents:   18421; bsz: 4902/6162/230; 11296/14197 tok/s;   6493 sec;
[2024-07-29 17:05:15,772 INFO] Step 1890/10000; acc: 89.2; ppl:   6.4; xent: 1.9; lr: 0.00144; sents:   18685; bsz: 4517/5789/234; 10773/13807 tok/s;   6527 sec;
[2024-07-29 17:05:50,491 INFO] Step 1900/10000; acc: 89.7; ppl:   6.3; xent: 1.8; lr: 0.00143; sents:   18368; bsz: 4913/6156/230; 11321/14184 tok/s;   6562 sec;
[2024-07-29 17:06:24,483 INFO] Step 1910/10000; acc: 89.1; ppl:   6.4; xent: 1.9; lr: 0.00143; sents:   19872; bsz: 4587/5924/248; 10795/13942 tok/s;   6596 sec;
[2024-07-29 17:06:59,018 INFO] Step 1920/10000; acc: 89.9; ppl:   6.2; xent: 1.8; lr: 0.00143; sents:   18468; bsz: 4755/6057/231; 11015/14031 tok/s;   6630 sec;
[2024-07-29 17:07:32,988 INFO] Step 1930/10000; acc: 89.1; ppl:   6.4; xent: 1.8; lr: 0.00142; sents:   19725; bsz: 4538/5825/247; 10688/13719 tok/s;   6664 sec;
[2024-07-29 17:08:07,071 INFO] Step 1940/10000; acc: 91.1; ppl:   6.0; xent: 1.8; lr: 0.00142; sents:   16964; bsz: 4884/6173/212; 11464/14490 tok/s;   6698 sec;
[2024-07-29 17:08:42,283 INFO] Step 1950/10000; acc: 90.2; ppl:   6.2; xent: 1.8; lr: 0.00141; sents:   19469; bsz: 4909/6212/243; 11152/14114 tok/s;   6733 sec;
[2024-07-29 17:09:15,784 INFO] Step 1960/10000; acc: 89.7; ppl:   6.2; xent: 1.8; lr: 0.00141; sents:   18999; bsz: 4494/5754/237; 10732/13740 tok/s;   6767 sec;
[2024-07-29 17:09:48,941 INFO] Step 1970/10000; acc: 89.9; ppl:   6.2; xent: 1.8; lr: 0.00141; sents:   19302; bsz: 4381/5673/241; 10570/13688 tok/s;   6800 sec;
[2024-07-29 17:10:23,844 INFO] Step 1980/10000; acc: 91.2; ppl:   6.0; xent: 1.8; lr: 0.00140; sents:   17590; bsz: 5004/6275/220; 11469/14383 tok/s;   6835 sec;
[2024-07-29 17:10:58,014 INFO] Step 1990/10000; acc: 89.5; ppl:   6.3; xent: 1.8; lr: 0.00140; sents:   20053; bsz: 4570/5849/251; 10700/13694 tok/s;   6869 sec;
[2024-07-29 17:11:32,621 INFO] Step 2000/10000; acc: 90.7; ppl:   6.1; xent: 1.8; lr: 0.00140; sents:   18859; bsz: 4777/6070/236; 11043/14032 tok/s;   6904 sec;
[2024-07-29 17:11:32,638 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_2000.pt
[2024-07-29 17:12:09,501 INFO] Step 2010/10000; acc: 91.0; ppl:   6.0; xent: 1.8; lr: 0.00139; sents:   18349; bsz: 4771/6103/229; 10350/13239 tok/s;   6941 sec;
[2024-07-29 17:12:43,248 INFO] Step 2020/10000; acc: 90.5; ppl:   6.1; xent: 1.8; lr: 0.00139; sents:   18508; bsz: 4657/5860/231; 11040/13891 tok/s;   6974 sec;
[2024-07-29 17:13:18,766 INFO] Step 2030/10000; acc: 91.2; ppl:   6.0; xent: 1.8; lr: 0.00139; sents:   18987; bsz: 4955/6328/237; 11161/14252 tok/s;   7010 sec;
[2024-07-29 17:13:52,139 INFO] Step 2040/10000; acc: 90.4; ppl:   6.1; xent: 1.8; lr: 0.00138; sents:   19295; bsz: 4442/5698/241; 10649/13658 tok/s;   7043 sec;
[2024-07-29 17:14:26,734 INFO] Step 2050/10000; acc: 91.0; ppl:   6.0; xent: 1.8; lr: 0.00138; sents:   18806; bsz: 4779/6069/235; 11051/14034 tok/s;   7078 sec;
[2024-07-29 17:15:00,485 INFO] Step 2060/10000; acc: 90.8; ppl:   6.0; xent: 1.8; lr: 0.00138; sents:   18929; bsz: 4621/5848/237; 10954/13863 tok/s;   7112 sec;
[2024-07-29 17:15:35,118 INFO] Step 2070/10000; acc: 90.7; ppl:   6.0; xent: 1.8; lr: 0.00137; sents:   20229; bsz: 4587/5979/253; 10595/13810 tok/s;   7146 sec;
[2024-07-29 17:16:09,174 INFO] Step 2080/10000; acc: 91.8; ppl:   5.8; xent: 1.8; lr: 0.00137; sents:   17493; bsz: 4783/6025/219; 11234/14153 tok/s;   7180 sec;
[2024-07-29 17:16:43,403 INFO] Step 2090/10000; acc: 91.2; ppl:   5.9; xent: 1.8; lr: 0.00137; sents:   19146; bsz: 4620/5966/239; 10798/13943 tok/s;   7214 sec;
[2024-07-29 17:17:17,270 INFO] Step 2100/10000; acc: 91.6; ppl:   5.9; xent: 1.8; lr: 0.00136; sents:   18246; bsz: 4747/5946/228; 11213/14045 tok/s;   7248 sec;
[2024-07-29 17:17:51,822 INFO] Step 2110/10000; acc: 91.5; ppl:   5.9; xent: 1.8; lr: 0.00136; sents:   18911; bsz: 4755/6061/236; 11010/14034 tok/s;   7283 sec;
[2024-07-29 17:18:26,036 INFO] Step 2120/10000; acc: 91.4; ppl:   5.9; xent: 1.8; lr: 0.00136; sents:   19291; bsz: 4661/5940/241; 10899/13890 tok/s;   7317 sec;
[2024-07-29 17:19:00,543 INFO] Step 2130/10000; acc: 91.8; ppl:   5.8; xent: 1.8; lr: 0.00135; sents:   18702; bsz: 4768/6051/234; 11054/14029 tok/s;   7352 sec;
[2024-07-29 17:19:35,002 INFO] Step 2140/10000; acc: 91.7; ppl:   5.8; xent: 1.8; lr: 0.00135; sents:   18966; bsz: 4813/6116/237; 11175/14198 tok/s;   7386 sec;
[2024-07-29 17:20:08,211 INFO] Step 2150/10000; acc: 91.0; ppl:   5.9; xent: 1.8; lr: 0.00135; sents:   19793; bsz: 4215/5513/247; 10154/13280 tok/s;   7419 sec;
[2024-07-29 17:20:43,492 INFO] Step 2160/10000; acc: 92.5; ppl:   5.7; xent: 1.7; lr: 0.00134; sents:   17588; bsz: 5035/6307/220; 11416/14302 tok/s;   7455 sec;
[2024-07-29 17:21:18,099 INFO] Step 2170/10000; acc: 92.4; ppl:   nan; xent: nan; lr: 0.00134; sents:   18447; bsz: 4844/6148/231; 11197/14212 tok/s;   7489 sec;
[2024-07-29 17:21:52,102 INFO] Step 2180/10000; acc: 91.7; ppl:   5.8; xent: 1.8; lr: 0.00134; sents:   19428; bsz: 4631/5937/243; 10895/13968 tok/s;   7523 sec;
[2024-07-29 17:22:25,988 INFO] Step 2190/10000; acc: 91.9; ppl:   5.8; xent: 1.8; lr: 0.00134; sents:   18403; bsz: 4670/5920/230; 11026/13977 tok/s;   7557 sec;
[2024-07-29 17:23:00,299 INFO] Step 2200/10000; acc: 92.1; ppl:   5.8; xent: 1.8; lr: 0.00133; sents:   18813; bsz: 4717/6001/235; 10998/13993 tok/s;   7591 sec;
[2024-07-29 17:23:34,594 INFO] Step 2210/10000; acc: 92.5; ppl:   5.7; xent: 1.7; lr: 0.00133; sents:   18310; bsz: 4774/6093/229; 11137/14213 tok/s;   7626 sec;
[2024-07-29 17:24:08,226 INFO] Step 2220/10000; acc: 92.2; ppl:   5.7; xent: 1.7; lr: 0.00133; sents:   18655; bsz: 4687/5876/233; 11150/13976 tok/s;   7659 sec;
[2024-07-29 17:24:42,774 INFO] Step 2230/10000; acc: 91.7; ppl:   5.8; xent: 1.8; lr: 0.00132; sents:   20598; bsz: 4590/5947/257; 10628/13772 tok/s;   7694 sec;
[2024-07-29 17:25:16,434 INFO] Step 2240/10000; acc: 92.4; ppl:   5.7; xent: 1.7; lr: 0.00132; sents:   18201; bsz: 4674/5947/228; 11109/14133 tok/s;   7728 sec;
[2024-07-29 17:25:50,202 INFO] Step 2250/10000; acc: 91.9; ppl:   5.8; xent: 1.8; lr: 0.00132; sents:   19810; bsz: 4483/5821/248; 10621/13791 tok/s;   7761 sec;
[2024-07-29 17:25:50,215 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V17_step_2250.pt
[2024-07-29 17:26:27,828 INFO] Step 2260/10000; acc: 93.2; ppl:   5.6; xent: 1.7; lr: 0.00131; sents:   17274; bsz: 5253/6455/216; 11168/13725 tok/s;   7799 sec;
[2024-07-29 17:27:00,440 INFO] Step 2270/10000; acc: 91.9; ppl:   5.8; xent: 1.8; lr: 0.00131; sents:   19740; bsz: 4174/5541/247; 10240/13593 tok/s;   7832 sec;
[2024-07-29 17:27:35,142 INFO] Step 2280/10000; acc: 92.9; ppl:   5.6; xent: 1.7; lr: 0.00131; sents:   18421; bsz: 4886/6135/230; 11265/14143 tok/s;   7866 sec;
[2024-07-29 17:28:08,952 INFO] Step 2290/10000; acc: 93.1; ppl:   5.6; xent: 1.7; lr: 0.00131; sents:   18036; bsz: 4666/5962/225; 11041/14107 tok/s;   7900 sec;
[2024-07-29 17:28:43,572 INFO] Step 2300/10000; acc: 93.1; ppl:   5.6; xent: 1.7; lr: 0.00130; sents:   18453; bsz: 4956/6173/231; 11453/14265 tok/s;   7935 sec;
[2024-07-29 17:29:17,753 INFO] Step 2310/10000; acc: 92.8; ppl:   5.6; xent: 1.7; lr: 0.00130; sents:   19259; bsz: 4593/5926/241; 10751/13869 tok/s;   7969 sec;
[2024-07-29 17:29:51,671 INFO] Step 2320/10000; acc: 92.4; ppl:   5.7; xent: 1.7; lr: 0.00130; sents:   19850; bsz: 4545/5838/248; 10720/13771 tok/s;   8003 sec;
[2024-07-29 17:30:25,152 INFO] Step 2330/10000; acc: 93.1; ppl:   5.6; xent: 1.7; lr: 0.00129; sents:   18080; bsz: 4600/5858/226; 10992/13998 tok/s;   8036 sec;
[2024-07-29 17:31:00,812 INFO] Step 2340/10000; acc: 93.5; ppl:   5.5; xent: 1.7; lr: 0.00129; sents:   18746; bsz: 5112/6457/234; 11468/14486 tok/s;   8072 sec;
[2024-07-29 17:31:33,885 INFO] Step 2350/10000; acc: 92.7; ppl:   nan; xent: nan; lr: 0.00129; sents:   19120; bsz: 4354/5626/239; 10532/13610 tok/s;   8105 sec;
[2024-07-29 17:32:08,305 INFO] Step 2360/10000; acc: 93.2; ppl:   nan; xent: nan; lr: 0.00129; sents:   19015; bsz: 4831/6099/238; 11229/14177 tok/s;   8139 sec;
[2024-07-29 17:32:42,660 INFO] Step 2370/10000; acc: 93.4; ppl:   5.5; xent: 1.7; lr: 0.00128; sents:   18633; bsz: 4764/6061/233; 11094/14113 tok/s;   8174 sec;
[2024-07-29 17:33:16,201 INFO] Step 2380/10000; acc: 93.4; ppl:   5.5; xent: 1.7; lr: 0.00128; sents:   18376; bsz: 4603/5899/230; 10978/14070 tok/s;   8207 sec;
[2024-07-29 17:33:50,453 INFO] Step 2390/10000; acc: 92.9; ppl:   5.6; xent: 1.7; lr: 0.00128; sents:   19686; bsz: 4573/5806/246; 10682/13561 tok/s;   8242 sec;
[2024-07-29 17:34:24,919 INFO] Step 2400/10000; acc: 93.6; ppl:   nan; xent: nan; lr: 0.00128; sents:   18846; bsz: 4785/6115/236; 11106/14193 tok/s;   8276 sec;
[2024-07-29 17:34:58,253 INFO] Step 2410/10000; acc: 93.2; ppl:   5.5; xent: 1.7; lr: 0.00127; sents:   18726; bsz: 4429/5722/234; 10629/13733 tok/s;   8309 sec;
[2024-07-29 17:35:33,921 INFO] Step 2420/10000; acc: 93.8; ppl:   5.5; xent: 1.7; lr: 0.00127; sents:   18988; bsz: 5103/6359/237; 11446/14262 tok/s;   8345 sec;
[2024-07-29 17:36:08,205 INFO] Step 2430/10000; acc: 93.4; ppl:   5.5; xent: 1.7; lr: 0.00127; sents:   19711; bsz: 4594/5955/246; 10720/13896 tok/s;   8379 sec;
[2024-07-29 17:36:41,948 INFO] Step 2440/10000; acc: 93.6; ppl:   5.5; xent: 1.7; lr: 0.00127; sents:   18223; bsz: 4638/5862/228; 10997/13899 tok/s;   8413 sec;
[2024-07-29 17:37:16,457 INFO] Step 2450/10000; acc: 93.4; ppl:   nan; xent: nan; lr: 0.00126; sents:   18522; bsz: 4815/6118/232; 11162/14183 tok/s;   8448 sec;
[2024-07-29 17:37:49,855 INFO] Step 2460/10000; acc: 93.7; ppl:   nan; xent: nan; lr: 0.00126; sents:   18413; bsz: 4536/5844/230; 10866/13999 tok/s;   8481 sec;
[2024-07-29 17:38:24,231 INFO] Step 2470/10000; acc: 93.9; ppl:   nan; xent: nan; lr: 0.00126; sents:   18510; bsz: 4858/6129/231; 11305/14265 tok/s;   8515 sec;
[2024-07-29 17:38:58,012 INFO] Step 2480/10000; acc: 93.3; ppl:   nan; xent: nan; lr: 0.00125; sents:   19685; bsz: 4589/5861/246; 10869/13880 tok/s;   8549 sec;
[2024-07-29 17:39:31,882 INFO] Step 2490/10000; acc: 93.7; ppl:   5.6; xent: 1.7; lr: 0.00125; sents:   19153; bsz: 4669/5988/239; 11029/14144 tok/s;   8583 sec;
