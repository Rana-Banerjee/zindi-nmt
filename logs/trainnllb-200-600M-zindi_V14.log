[2024-07-29 11:53:38,354 INFO] Parsed 1 corpora from -data.
[2024-07-29 11:53:38,354 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 11:53:39,760 WARNING] configured transforms is different from checkpoint: +{'suffix', 'sentencepiece', 'prefix'}
[2024-07-29 11:53:39,760 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', 'dyu_Latn', ''], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 11:53:39,793 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:53:39,793 INFO] The decoder start token is: </s>
[2024-07-29 11:53:39,823 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 11:53:39,823 INFO] Get special vocabs from Transforms: {'src': ['', '</s>', 'dyu_Latn', ''], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 11:53:39,855 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:53:39,855 INFO] The decoder start token is: </s>
[2024-07-29 11:53:39,858 INFO] Over-ride model option set to true - use with care
[2024-07-29 11:53:39,858 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 11:53:39,858 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 11:53:39,858 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 11:53:39,858 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 11:53:39,858 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:53:39,859 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:53:39,859 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:53:39,859 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:53:39,859 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 11:53:39,859 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:53:39,859 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:53:39,859 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 11:53:39,859 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 11:53:39,859 INFO] Option: transformer_ff , value: 2048 overriding model: 4096
[2024-07-29 11:53:39,859 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 11:53:39,859 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V14 overriding model: nllb
[2024-07-29 11:53:39,859 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 11:53:39,859 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 11:53:39,859 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 11:53:39,859 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 11:53:39,859 INFO] Option: batch_size , value: 9984 overriding model: 8192
[2024-07-29 11:53:39,860 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 11:53:39,860 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:53:39,860 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 11:53:39,860 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 11:53:39,860 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 11:53:39,860 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 11:53:39,860 INFO] Option: dropout , value: [0.3, 0.3, 0.3] overriding model: [0.1]
[2024-07-29 11:53:39,860 INFO] Option: attention_dropout , value: [0.2, 0.2, 0.2] overriding model: [0.1]
[2024-07-29 11:53:39,860 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:53:39,860 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 11:53:39,860 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 11:53:39,860 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 11:53:39,860 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 11:53:39,860 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V14.log overriding model: 
[2024-07-29 11:53:39,860 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 11:53:39,860 INFO] Option: _all_transform , value: {'filtertoolong', 'suffix', 'prefix', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-29 11:53:39,860 INFO] Building model...
[2024-07-29 11:53:42,467 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 11:53:42,467 INFO] Non quantized layer compute is fp16
[2024-07-29 11:53:42,467 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 11:53:42,652 INFO] src: 14783 new tokens
[2024-07-29 11:53:42,940 INFO] tgt: 14783 new tokens
[2024-07-29 11:53:43,457 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 11:53:43,462 INFO] encoder: 125389824
[2024-07-29 11:53:43,462 INFO] decoder: 151229901
[2024-07-29 11:53:43,463 INFO] * number of parameters: 276619725
[2024-07-29 11:53:43,464 INFO] Trainable parameters = {'torch.float32': 276619725, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:53:43,464 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:53:43,464 INFO]  * src vocab size = 24013
[2024-07-29 11:53:43,464 INFO]  * tgt vocab size = 24013
[2024-07-29 11:53:43,825 INFO] Starting training on GPU: [0]
[2024-07-29 11:53:43,825 INFO] Start training loop without validation...
[2024-07-29 11:53:43,825 INFO] Scoring with: None
[2024-07-29 11:54:43,405 INFO] Step 10/10000; acc: 7.2; ppl: 2859.3; xent: 8.0; lr: 0.00069; sents:   14962; bsz: 3714/4733/187; 4987/6355 tok/s;     60 sec;
[2024-07-29 11:55:21,620 INFO] Parsed 1 corpora from -data.
[2024-07-29 11:55:21,620 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 11:55:23,183 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'prefix', 'suffix'}
[2024-07-29 11:55:23,183 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 11:55:23,216 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:55:23,216 INFO] The decoder start token is: </s>
[2024-07-29 11:55:23,249 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 11:55:23,249 INFO] Get special vocabs from Transforms: {'src': ['', 'dyu_Latn', '', '</s>'], 'tgt': ['', 'fra_Latn', '']}.
[2024-07-29 11:55:23,292 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:55:23,292 INFO] The decoder start token is: </s>
[2024-07-29 11:55:23,296 INFO] Over-ride model option set to true - use with care
[2024-07-29 11:55:23,296 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 11:55:23,296 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 11:55:23,296 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 11:55:23,296 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 11:55:23,296 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:55:23,296 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:55:23,296 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:55:23,296 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:55:23,296 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 11:55:23,296 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:55:23,296 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:55:23,296 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 11:55:23,297 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 11:55:23,297 INFO] Option: transformer_ff , value: 2048 overriding model: 4096
[2024-07-29 11:55:23,297 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 11:55:23,297 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V14 overriding model: nllb
[2024-07-29 11:55:23,297 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 11:55:23,297 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 11:55:23,297 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 11:55:23,297 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 11:55:23,297 INFO] Option: batch_size , value: 10240 overriding model: 8192
[2024-07-29 11:55:23,297 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 11:55:23,297 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:55:23,297 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 11:55:23,297 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 11:55:23,297 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 11:55:23,297 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 11:55:23,297 INFO] Option: dropout , value: [0.3, 0.3, 0.3] overriding model: [0.1]
[2024-07-29 11:55:23,297 INFO] Option: attention_dropout , value: [0.2, 0.2, 0.2] overriding model: [0.1]
[2024-07-29 11:55:23,297 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:55:23,297 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 11:55:23,297 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 11:55:23,297 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 11:55:23,297 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 11:55:23,298 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V14.log overriding model: 
[2024-07-29 11:55:23,298 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 11:55:23,298 INFO] Option: _all_transform , value: {'sentencepiece', 'filtertoolong', 'prefix', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-29 11:55:23,298 INFO] Building model...
[2024-07-29 11:55:26,053 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 11:55:26,053 INFO] Non quantized layer compute is fp16
[2024-07-29 11:55:26,053 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 11:55:26,281 INFO] src: 14783 new tokens
[2024-07-29 11:55:26,649 INFO] tgt: 14783 new tokens
[2024-07-29 11:55:27,179 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 11:55:27,185 INFO] encoder: 125389824
[2024-07-29 11:55:27,186 INFO] decoder: 151229901
[2024-07-29 11:55:27,186 INFO] * number of parameters: 276619725
[2024-07-29 11:55:27,187 INFO] Trainable parameters = {'torch.float32': 276619725, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:55:27,187 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:55:27,187 INFO]  * src vocab size = 24013
[2024-07-29 11:55:27,187 INFO]  * tgt vocab size = 24013
[2024-07-29 11:55:27,545 INFO] Starting training on GPU: [0]
[2024-07-29 11:55:27,545 INFO] Start training loop without validation...
[2024-07-29 11:55:27,545 INFO] Scoring with: None
[2024-07-29 11:56:27,578 INFO] Step 10/10000; acc: 7.3; ppl: 2886.3; xent: 8.0; lr: 0.00069; sents:   15303; bsz: 3754/4781/191; 5002/6371 tok/s;     60 sec;
[2024-07-29 11:56:58,945 INFO] Step 20/10000; acc: 11.9; ppl: 1203.5; xent: 7.1; lr: 0.00131; sents:   14968; bsz: 3813/4850/187; 9726/12370 tok/s;     91 sec;
[2024-07-29 11:57:30,342 INFO] Step 30/10000; acc: 16.1; ppl: 699.0; xent: 6.5; lr: 0.00194; sents:   15431; bsz: 3712/4737/193; 9459/12070 tok/s;    123 sec;
[2024-07-29 11:58:01,894 INFO] Step 40/10000; acc: 18.7; ppl: 439.9; xent: 6.1; lr: 0.00256; sents:   15213; bsz: 3755/4774/190; 9520/12104 tok/s;    154 sec;
[2024-07-29 11:58:33,631 INFO] Step 50/10000; acc: 24.3; ppl: 266.9; xent: 5.6; lr: 0.00319; sents:   14965; bsz: 3826/4845/187; 9645/12213 tok/s;    186 sec;
[2024-07-29 11:58:57,301 INFO] Parsed 1 corpora from -data.
[2024-07-29 11:58:57,301 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-29 11:58:58,723 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-29 11:58:58,723 INFO] Get special vocabs from Transforms: {'src': ['</s>', '', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 11:58:58,757 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:58:58,757 INFO] The decoder start token is: </s>
[2024-07-29 11:58:58,799 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-29 11:58:58,799 INFO] Get special vocabs from Transforms: {'src': ['</s>', '', '', 'dyu_Latn'], 'tgt': ['', '', 'fra_Latn']}.
[2024-07-29 11:58:58,833 INFO] The first 10 tokens of the vocabs are:['', '<s>', '<blank>', '</s>', '<unk>', 'dyu_Latn', 'fra_Latn', '.', '▁de', ',']
[2024-07-29 11:58:58,833 INFO] The decoder start token is: </s>
[2024-07-29 11:58:58,836 INFO] Over-ride model option set to true - use with care
[2024-07-29 11:58:58,836 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-29 11:58:58,836 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/all_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/all_fr.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-29 11:58:58,836 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-29 11:58:58,836 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-29 11:58:58,836 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:58:58,836 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary3.txt overriding model: 
[2024-07-29 11:58:58,836 INFO] Option: src_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:58:58,837 INFO] Option: tgt_vocab_size , value: 24013 overriding model: 256206
[2024-07-29 11:58:58,837 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-29 11:58:58,837 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:58:58,837 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm3.model overriding model: 
[2024-07-29 11:58:58,837 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-29 11:58:58,837 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-29 11:58:58,837 INFO] Option: transformer_ff , value: 2048 overriding model: 4096
[2024-07-29 11:58:58,837 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-29 11:58:58,837 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V14 overriding model: nllb
[2024-07-29 11:58:58,837 INFO] Option: save_checkpoint_steps , value: 100 overriding model: 5000
[2024-07-29 11:58:58,837 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-29 11:58:58,837 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-29 11:58:58,837 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-29 11:58:58,837 INFO] Option: batch_size , value: 10752 overriding model: 8192
[2024-07-29 11:58:58,837 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-29 11:58:58,837 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:58:58,837 INFO] Option: valid_steps , value: 10000 overriding model: 5000
[2024-07-29 11:58:58,837 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-29 11:58:58,837 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-29 11:58:58,837 INFO] Option: optim , value: adam overriding model: 
[2024-07-29 11:58:58,837 INFO] Option: dropout , value: [0.3, 0.3, 0.3] overriding model: [0.1]
[2024-07-29 11:58:58,837 INFO] Option: attention_dropout , value: [0.2, 0.2, 0.2] overriding model: [0.1]
[2024-07-29 11:58:58,838 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-29 11:58:58,838 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-29 11:58:58,838 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-29 11:58:58,838 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-29 11:58:58,838 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-29 11:58:58,838 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V14.log overriding model: 
[2024-07-29 11:58:58,838 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-29 11:58:58,838 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'prefix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-29 11:58:58,838 INFO] Building model...
[2024-07-29 11:59:01,447 INFO] Switching model to float32 for amp/apex_amp
[2024-07-29 11:59:01,447 INFO] Non quantized layer compute is fp16
[2024-07-29 11:59:01,447 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-29 11:59:01,628 INFO] src: 14783 new tokens
[2024-07-29 11:59:01,943 INFO] tgt: 14783 new tokens
[2024-07-29 11:59:02,440 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(24013, 1024, padding_idx=2)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.3, inplace=False)
          (dropout_2): Dropout(p=0.3, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.3, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.2, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=24013, bias=True)
)
[2024-07-29 11:59:02,445 INFO] encoder: 125389824
[2024-07-29 11:59:02,445 INFO] decoder: 151229901
[2024-07-29 11:59:02,445 INFO] * number of parameters: 276619725
[2024-07-29 11:59:02,446 INFO] Trainable parameters = {'torch.float32': 276619725, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:59:02,446 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-29 11:59:02,446 INFO]  * src vocab size = 24013
[2024-07-29 11:59:02,446 INFO]  * tgt vocab size = 24013
[2024-07-29 11:59:02,771 INFO] Starting training on GPU: [0]
[2024-07-29 11:59:02,771 INFO] Start training loop without validation...
[2024-07-29 11:59:02,771 INFO] Scoring with: None
[2024-07-29 12:00:04,357 INFO] Step 10/10000; acc: 7.5; ppl: 2864.4; xent: 8.0; lr: 0.00069; sents:   16387; bsz: 3876/4944/205; 5034/6423 tok/s;     62 sec;
[2024-07-29 12:00:37,489 INFO] Step 20/10000; acc: 11.6; ppl: 1211.8; xent: 7.1; lr: 0.00131; sents:   14973; bsz: 4101/5199/187; 9903/12554 tok/s;     95 sec;
[2024-07-29 12:01:09,931 INFO] Step 30/10000; acc: 16.8; ppl: 676.2; xent: 6.5; lr: 0.00194; sents:   16979; bsz: 3723/4805/212; 9180/11848 tok/s;    127 sec;
[2024-07-29 12:01:42,697 INFO] Step 40/10000; acc: 18.7; ppl: 444.8; xent: 6.1; lr: 0.00256; sents:   16127; bsz: 3920/4963/202; 9570/12116 tok/s;    160 sec;
[2024-07-29 12:02:16,121 INFO] Step 50/10000; acc: 23.2; ppl: 287.8; xent: 5.7; lr: 0.00319; sents:   15131; bsz: 4219/5250/189; 10099/12566 tok/s;    193 sec;
[2024-07-29 12:02:48,191 INFO] Step 60/10000; acc: 29.2; ppl: 177.4; xent: 5.2; lr: 0.00381; sents:   16827; bsz: 3715/4847/210; 9267/12092 tok/s;    225 sec;
[2024-07-29 12:03:20,994 INFO] Step 70/10000; acc: 29.3; ppl: 155.0; xent: 5.0; lr: 0.00444; sents:   15065; bsz: 3940/5088/188; 9609/12408 tok/s;    258 sec;
[2024-07-29 12:03:52,785 INFO] Step 80/10000; acc: 34.1; ppl: 112.8; xent: 4.7; lr: 0.00506; sents:   16814; bsz: 3746/4702/210; 9427/11834 tok/s;    290 sec;
[2024-07-29 12:04:26,403 INFO] Step 90/10000; acc: 35.4; ppl:  94.4; xent: 4.5; lr: 0.00569; sents:   16171; bsz: 4062/5209/202; 9667/12396 tok/s;    324 sec;
[2024-07-29 12:04:59,874 INFO] Step 100/10000; acc: 37.9; ppl:  75.3; xent: 4.3; lr: 0.00622; sents:   14968; bsz: 4267/5323/187; 10198/12723 tok/s;    357 sec;
[2024-07-29 12:04:59,887 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V14_step_100.pt
[2024-07-29 12:05:34,142 INFO] Step 110/10000; acc: 43.6; ppl:  54.1; xent: 4.0; lr: 0.00593; sents:   16961; bsz: 3573/4604/212; 8342/10748 tok/s;    391 sec;
[2024-07-29 12:06:08,015 INFO] Step 120/10000; acc: 43.6; ppl:  48.2; xent: 3.9; lr: 0.00568; sents:   15324; bsz: 4222/5371/192; 9971/12685 tok/s;    425 sec;
[2024-07-29 12:06:40,085 INFO] Step 130/10000; acc: 47.4; ppl:  38.7; xent: 3.7; lr: 0.00546; sents:   14938; bsz: 3775/4842/187; 9418/12079 tok/s;    457 sec;
[2024-07-29 12:07:12,290 INFO] Step 140/10000; acc: 52.3; ppl:  30.0; xent: 3.4; lr: 0.00526; sents:   17989; bsz: 3684/4674/225; 9152/11610 tok/s;    490 sec;
[2024-07-29 12:07:45,811 INFO] Step 150/10000; acc: 54.1; ppl:  25.7; xent: 3.2; lr: 0.00509; sents:   13953; bsz: 4203/5373/174; 10030/12824 tok/s;    523 sec;
[2024-07-29 12:08:19,310 INFO] Step 160/10000; acc: 59.2; ppl:  20.2; xent: 3.0; lr: 0.00493; sents:   15785; bsz: 4141/5155/197; 9890/12312 tok/s;    557 sec;
[2024-07-29 12:08:51,502 INFO] Step 170/10000; acc: 64.5; ppl:  16.0; xent: 2.8; lr: 0.00478; sents:   17013; bsz: 3688/4777/213; 9165/11871 tok/s;    589 sec;
[2024-07-29 12:09:24,343 INFO] Step 180/10000; acc: 66.9; ppl:  14.2; xent: 2.7; lr: 0.00465; sents:   16119; bsz: 3952/5024/201; 9627/12240 tok/s;    622 sec;
[2024-07-29 12:09:57,077 INFO] Step 190/10000; acc: 68.8; ppl:  13.2; xent: 2.6; lr: 0.00452; sents:   15936; bsz: 3926/5006/199; 9596/12235 tok/s;    654 sec;
[2024-07-29 12:10:29,272 INFO] Step 200/10000; acc: 72.0; ppl:  11.5; xent: 2.4; lr: 0.00441; sents:   15311; bsz: 3882/4908/191; 9645/12195 tok/s;    687 sec;
[2024-07-29 12:10:29,289 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V14_step_200.pt
[2024-07-29 12:11:04,964 INFO] Step 210/10000; acc: 76.0; ppl:   9.9; xent: 2.3; lr: 0.00430; sents:   15655; bsz: 4021/5095/196; 9013/11419 tok/s;    722 sec;
[2024-07-29 12:11:37,552 INFO] Step 220/10000; acc: 78.3; ppl:   9.2; xent: 2.2; lr: 0.00420; sents:   15847; bsz: 3962/5071/198; 9727/12449 tok/s;    755 sec;
[2024-07-29 12:12:10,270 INFO] Step 230/10000; acc: 81.7; ppl:   8.2; xent: 2.1; lr: 0.00411; sents:   17309; bsz: 3829/4959/216; 9362/12126 tok/s;    787 sec;
[2024-07-29 12:12:42,920 INFO] Step 240/10000; acc: 84.1; ppl:   7.5; xent: 2.0; lr: 0.00403; sents:   15786; bsz: 4017/5077/197; 9843/12440 tok/s;    820 sec;
[2024-07-29 12:13:15,018 INFO] Step 250/10000; acc: 86.5; ppl:   7.0; xent: 1.9; lr: 0.00394; sents:   15180; bsz: 3973/4985/190; 9903/12425 tok/s;    852 sec;
[2024-07-29 12:13:48,010 INFO] Step 260/10000; acc: 88.1; ppl:   6.6; xent: 1.9; lr: 0.00387; sents:   16570; bsz: 3907/5044/207; 9473/12231 tok/s;    885 sec;
[2024-07-29 12:14:20,147 INFO] Step 270/10000; acc: 89.7; ppl:   6.3; xent: 1.8; lr: 0.00380; sents:   16814; bsz: 3836/4908/210; 9548/12218 tok/s;    917 sec;
[2024-07-29 12:14:52,506 INFO] Step 280/10000; acc: 91.0; ppl:   6.0; xent: 1.8; lr: 0.00373; sents:   15617; bsz: 3945/5070/195; 9753/12534 tok/s;    950 sec;
[2024-07-29 12:15:25,541 INFO] Step 290/10000; acc: 91.6; ppl:   5.9; xent: 1.8; lr: 0.00366; sents:   15718; bsz: 4127/5120/196; 9995/12400 tok/s;    983 sec;
[2024-07-29 12:15:58,140 INFO] Step 300/10000; acc: 92.3; ppl:   5.8; xent: 1.8; lr: 0.00360; sents:   15703; bsz: 3957/5026/196; 9711/12334 tok/s;   1015 sec;
[2024-07-29 12:15:58,151 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V14_step_300.pt
[2024-07-29 12:16:32,823 INFO] Step 310/10000; acc: 94.0; ppl:   5.5; xent: 1.7; lr: 0.00354; sents:   14740; bsz: 3963/5068/184; 9141/11689 tok/s;   1050 sec;
[2024-07-29 12:17:05,306 INFO] Step 320/10000; acc: 93.5; ppl:   5.5; xent: 1.7; lr: 0.00349; sents:   18115; bsz: 3790/4863/226; 9333/11978 tok/s;   1083 sec;
[2024-07-29 12:17:37,177 INFO] Step 330/10000; acc: 94.5; ppl:   5.4; xent: 1.7; lr: 0.00344; sents:   15061; bsz: 3783/4823/188; 9497/12106 tok/s;   1114 sec;
[2024-07-29 12:18:09,748 INFO] Step 340/10000; acc: 95.3; ppl:   5.2; xent: 1.7; lr: 0.00338; sents:   16406; bsz: 3944/5018/205; 9687/12325 tok/s;   1147 sec;
[2024-07-29 12:18:42,981 INFO] Step 350/10000; acc: 96.0; ppl:   5.1; xent: 1.6; lr: 0.00334; sents:   14452; bsz: 4228/5316/181; 10177/12797 tok/s;   1180 sec;
[2024-07-29 12:19:14,885 INFO] Step 360/10000; acc: 95.6; ppl:   5.1; xent: 1.6; lr: 0.00329; sents:   17527; bsz: 3664/4730/219; 9187/11860 tok/s;   1212 sec;
[2024-07-29 12:19:47,346 INFO] Step 370/10000; acc: 96.2; ppl:   5.0; xent: 1.6; lr: 0.00324; sents:   16042; bsz: 3884/4986/201; 9571/12288 tok/s;   1245 sec;
[2024-07-29 12:20:20,080 INFO] Step 380/10000; acc: 96.5; ppl:   5.0; xent: 1.6; lr: 0.00320; sents:   15607; bsz: 4022/5090/195; 9830/12441 tok/s;   1277 sec;
[2024-07-29 12:20:52,396 INFO] Step 390/10000; acc: 95.7; ppl:   5.1; xent: 1.6; lr: 0.00316; sents:   16315; bsz: 3831/4919/204; 9484/12176 tok/s;   1310 sec;
[2024-07-29 12:21:24,637 INFO] Step 400/10000; acc: 96.5; ppl:   5.0; xent: 1.6; lr: 0.00312; sents:   15187; bsz: 3975/4975/190; 9864/12343 tok/s;   1342 sec;
[2024-07-29 12:21:24,657 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V14_step_400.pt
[2024-07-29 12:22:00,261 INFO] Step 410/10000; acc: 97.1; ppl:   4.8; xent: 1.6; lr: 0.00308; sents:   15901; bsz: 3907/5008/199; 8774/11246 tok/s;   1377 sec;
[2024-07-29 12:22:33,061 INFO] Step 420/10000; acc: 97.6; ppl:   4.8; xent: 1.6; lr: 0.00305; sents:   15868; bsz: 4054/5106/198; 9888/12453 tok/s;   1410 sec;
[2024-07-29 12:23:06,112 INFO] Step 430/10000; acc: 97.9; ppl:   4.7; xent: 1.5; lr: 0.00301; sents:   16947; bsz: 3918/5091/212; 9483/12323 tok/s;   1443 sec;
[2024-07-29 12:23:38,748 INFO] Step 440/10000; acc: 98.1; ppl:   4.6; xent: 1.5; lr: 0.00298; sents:   15751; bsz: 3890/4926/197; 9536/12075 tok/s;   1476 sec;
[2024-07-29 12:24:11,018 INFO] Step 450/10000; acc: 98.1; ppl:   4.6; xent: 1.5; lr: 0.00294; sents:   17603; bsz: 3787/4855/220; 9389/12037 tok/s;   1508 sec;
[2024-07-29 12:24:44,397 INFO] Step 460/10000; acc: 98.4; ppl:   4.6; xent: 1.5; lr: 0.00291; sents:   14563; bsz: 4160/5232/182; 9971/12540 tok/s;   1542 sec;
[2024-07-29 12:25:16,226 INFO] Step 470/10000; acc: 98.2; ppl:   4.6; xent: 1.5; lr: 0.00288; sents:   15759; bsz: 3736/4782/197; 9390/12021 tok/s;   1573 sec;
[2024-07-29 12:25:48,960 INFO] Step 480/10000; acc: 98.4; ppl:   4.6; xent: 1.5; lr: 0.00285; sents:   17853; bsz: 3765/4899/223; 9202/11973 tok/s;   1606 sec;
[2024-07-29 12:26:21,644 INFO] Step 490/10000; acc: 98.5; ppl:   4.5; xent: 1.5; lr: 0.00282; sents:   13424; bsz: 4164/5229/168; 10193/12800 tok/s;   1639 sec;
[2024-07-29 12:26:55,178 INFO] Step 500/10000; acc: 98.6; ppl:   4.5; xent: 1.5; lr: 0.00279; sents:   16736; bsz: 4138/5219/209; 9871/12450 tok/s;   1672 sec;
[2024-07-29 12:26:55,192 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V14_step_500.pt
[2024-07-29 12:27:29,305 INFO] Step 510/10000; acc: 98.6; ppl:   4.5; xent: 1.5; lr: 0.00276; sents:   16727; bsz: 3537/4574/209; 8291/10723 tok/s;   1707 sec;
