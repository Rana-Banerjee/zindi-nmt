[2024-07-30 09:25:58,513 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:25:58,513 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:26:00,528 WARNING] configured transforms is different from checkpoint: +{'suffix', 'sentencepiece', 'prefix'}
[2024-07-30 09:26:00,528 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:26:01,171 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:26:01,173 INFO] The decoder start token is: </s>
[2024-07-30 09:26:01,212 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:26:01,214 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:26:01,952 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:26:01,954 INFO] The decoder start token is: </s>
[2024-07-30 09:26:02,028 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:26:02,028 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:26:02,028 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:26:02,029 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:26:02,029 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:26:02,029 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:26:02,029 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:26:02,029 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:26:02,030 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:26:02,030 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:26:02,030 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-30 09:26:02,030 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:26:02,030 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:26:02,030 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:26:02,030 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:26:02,030 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:26:02,030 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:26:02,030 INFO] Option: batch_size , value: 13184 overriding model: 8192
[2024-07-30 09:26:02,030 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:26:02,030 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:26:02,030 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:26:02,030 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:26:02,030 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:26:02,031 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:26:02,031 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:26:02,031 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:26:02,031 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:26:02,031 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:26:02,031 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:26:02,031 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:26:02,031 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:26:02,031 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:26:02,031 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:26:02,031 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:26:02,031 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'prefix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-30 09:26:02,031 INFO] Building model...
[2024-07-30 09:26:09,794 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:26:09,795 INFO] Non quantized layer compute is fp16
[2024-07-30 09:26:09,795 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:26:12,388 INFO] src: 0 new tokens
[2024-07-30 09:26:17,906 INFO] tgt: 0 new tokens
[2024-07-30 09:26:19,076 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:26:19,097 INFO] encoder: 337977344
[2024-07-30 09:26:19,097 INFO] decoder: 126283982
[2024-07-30 09:26:19,097 INFO] * number of parameters: 464261326
[2024-07-30 09:26:19,104 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:26:19,104 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:26:19,104 INFO]  * src vocab size = 256206
[2024-07-30 09:26:19,104 INFO]  * tgt vocab size = 256206
[2024-07-30 09:26:20,189 INFO] Starting training on GPU: [0]
[2024-07-30 09:26:20,190 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:26:20,190 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:26:50,538 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:51,102 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:51,299 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:51,414 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:51,991 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:52,556 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:52,700 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:26:52,796 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:27:43,863 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:27:43,863 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:27:46,309 WARNING] configured transforms is different from checkpoint: +{'suffix', 'prefix', 'sentencepiece'}
[2024-07-30 09:27:46,309 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:27:47,177 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:27:47,179 INFO] The decoder start token is: </s>
[2024-07-30 09:27:47,251 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:27:47,257 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:27:48,197 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:27:48,200 INFO] The decoder start token is: </s>
[2024-07-30 09:27:48,321 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:27:48,322 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:27:48,322 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:27:48,322 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:27:48,322 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:27:48,322 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:27:48,322 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:27:48,322 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:27:48,322 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:27:48,322 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:27:48,322 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:27:48,323 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:27:48,323 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:27:48,323 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:27:48,323 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:27:48,323 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:27:48,323 INFO] Option: bucket_size , value: 64 overriding model: 262144
[2024-07-30 09:27:48,323 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:27:48,323 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:27:48,323 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:27:48,323 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:27:48,323 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:27:48,323 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:27:48,323 INFO] Option: batch_size , value: 10480 overriding model: 8192
[2024-07-30 09:27:48,324 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:27:48,324 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:27:48,324 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:27:48,324 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:27:48,324 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:27:48,324 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:27:48,324 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:27:48,324 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:27:48,324 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:27:48,324 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:27:48,324 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:27:48,324 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:27:48,324 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:27:48,325 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:27:48,325 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:27:48,325 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:27:48,325 INFO] Option: _all_transform , value: {'prefix', 'suffix', 'filtertoolong', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-30 09:27:48,325 INFO] Building model...
[2024-07-30 09:27:57,461 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:27:57,462 INFO] Non quantized layer compute is fp16
[2024-07-30 09:27:57,462 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:28:00,534 INFO] src: 0 new tokens
[2024-07-30 09:28:06,612 INFO] tgt: 0 new tokens
[2024-07-30 09:28:08,297 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:28:08,318 INFO] encoder: 337977344
[2024-07-30 09:28:08,318 INFO] decoder: 126283982
[2024-07-30 09:28:08,318 INFO] * number of parameters: 464261326
[2024-07-30 09:28:08,325 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:28:08,325 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:28:08,325 INFO]  * src vocab size = 256206
[2024-07-30 09:28:08,325 INFO]  * tgt vocab size = 256206
[2024-07-30 09:28:09,428 INFO] Starting training on GPU: [0]
[2024-07-30 09:28:09,428 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:28:09,428 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:28:54,774 INFO] Step 10/10000; acc: 2.3; ppl: 196246.7; xent: 12.2; lr: 0.00069; sents:    5119; bsz:  673/ 668/64; 1188/1179 tok/s;     45 sec;
[2024-07-30 09:29:19,262 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:29:20,805 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:29:21,146 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:29:21,465 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:29:21,836 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:29:21,953 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:29:22,510 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:29:22,970 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:29:23,210 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:29:23,635 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:29:24,160 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:29:24,443 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:30:01,293 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:30:01,293 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:30:03,432 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-30 09:30:03,433 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:30:04,165 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:30:04,167 INFO] The decoder start token is: </s>
[2024-07-30 09:30:04,211 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:30:04,214 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:30:05,138 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:30:05,141 INFO] The decoder start token is: </s>
[2024-07-30 09:30:05,258 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:30:05,259 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:30:05,259 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:30:05,259 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:30:05,259 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:30:05,259 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:30:05,259 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:30:05,259 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:30:05,259 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:30:05,259 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:30:05,259 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:30:05,260 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:30:05,260 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:30:05,260 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:30:05,260 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:30:05,260 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:30:05,260 INFO] Option: bucket_size , value: 64 overriding model: 262144
[2024-07-30 09:30:05,260 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:30:05,260 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:30:05,260 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:30:05,260 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:30:05,260 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:30:05,260 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:30:05,260 INFO] Option: batch_size , value: 10240 overriding model: 8192
[2024-07-30 09:30:05,261 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:30:05,261 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:30:05,261 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:30:05,261 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:30:05,261 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:30:05,261 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:30:05,261 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:30:05,261 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:30:05,261 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:30:05,261 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:30:05,261 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:30:05,261 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:30:05,261 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:30:05,262 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:30:05,262 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:30:05,262 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:30:05,262 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'prefix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-30 09:30:05,262 INFO] Building model...
[2024-07-30 09:30:13,343 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:30:13,343 INFO] Non quantized layer compute is fp16
[2024-07-30 09:30:13,343 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:30:16,431 INFO] src: 0 new tokens
[2024-07-30 09:30:23,152 INFO] tgt: 0 new tokens
[2024-07-30 09:30:24,316 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:30:24,325 INFO] encoder: 337977344
[2024-07-30 09:30:24,325 INFO] decoder: 126283982
[2024-07-30 09:30:24,325 INFO] * number of parameters: 464261326
[2024-07-30 09:30:24,327 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:30:24,327 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:30:24,328 INFO]  * src vocab size = 256206
[2024-07-30 09:30:24,328 INFO]  * tgt vocab size = 256206
[2024-07-30 09:30:25,134 INFO] Starting training on GPU: [0]
[2024-07-30 09:30:25,135 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:30:25,135 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:31:09,581 INFO] Step 10/10000; acc: 2.3; ppl: 196246.7; xent: 12.2; lr: 0.00069; sents:    5119; bsz:  673/ 668/64; 1212/1202 tok/s;     44 sec;
[2024-07-30 09:31:37,153 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:31:37,153 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:31:38,979 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'prefix', 'suffix'}
[2024-07-30 09:31:38,979 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:31:39,639 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:31:39,641 INFO] The decoder start token is: </s>
[2024-07-30 09:31:39,682 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:31:39,684 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:31:40,406 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:31:40,408 INFO] The decoder start token is: </s>
[2024-07-30 09:31:40,475 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:31:40,476 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:31:40,476 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:31:40,476 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:31:40,476 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:31:40,476 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:31:40,476 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:31:40,476 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:31:40,476 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:31:40,476 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:31:40,476 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:31:40,477 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:31:40,477 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:31:40,477 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:31:40,477 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:31:40,477 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:31:40,477 INFO] Option: bucket_size , value: 64 overriding model: 262144
[2024-07-30 09:31:40,477 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:31:40,477 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:31:40,477 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:31:40,477 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:31:40,477 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:31:40,477 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:31:40,478 INFO] Option: batch_size , value: 10752 overriding model: 8192
[2024-07-30 09:31:40,478 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:31:40,478 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:31:40,478 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:31:40,478 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:31:40,478 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:31:40,478 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:31:40,478 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:31:40,478 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:31:40,478 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:31:40,478 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:31:40,478 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:31:40,478 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:31:40,478 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:31:40,478 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:31:40,479 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:31:40,479 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:31:40,479 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'prefix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-30 09:31:40,479 INFO] Building model...
[2024-07-30 09:31:48,925 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:31:48,925 INFO] Non quantized layer compute is fp16
[2024-07-30 09:31:48,925 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:31:51,781 INFO] src: 0 new tokens
[2024-07-30 09:31:57,661 INFO] tgt: 0 new tokens
[2024-07-30 09:31:58,788 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:31:58,806 INFO] encoder: 337977344
[2024-07-30 09:31:58,807 INFO] decoder: 126283982
[2024-07-30 09:31:58,807 INFO] * number of parameters: 464261326
[2024-07-30 09:31:58,812 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:31:58,812 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:31:58,812 INFO]  * src vocab size = 256206
[2024-07-30 09:31:58,813 INFO]  * tgt vocab size = 256206
[2024-07-30 09:31:59,648 INFO] Starting training on GPU: [0]
[2024-07-30 09:31:59,648 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:31:59,648 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:32:45,655 INFO] Step 10/10000; acc: 2.3; ppl: 196246.7; xent: 12.2; lr: 0.00069; sents:    5119; bsz:  673/ 668/64; 1171/1162 tok/s;     46 sec;
[2024-07-30 09:33:11,122 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:33:12,646 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:33:12,976 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:33:13,329 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:33:13,714 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:33:13,885 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:33:14,539 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:33:14,984 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:33:15,289 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:33:32,845 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:33:32,845 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:33:34,926 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-30 09:33:34,926 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:33:35,616 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:33:35,619 INFO] The decoder start token is: </s>
[2024-07-30 09:33:35,661 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:33:35,664 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:33:36,426 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:33:36,429 INFO] The decoder start token is: </s>
[2024-07-30 09:33:36,501 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:33:36,502 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:33:36,502 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:33:36,502 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:33:36,502 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:33:36,502 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:33:36,502 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:33:36,502 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:33:36,502 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:33:36,502 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:33:36,502 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:33:36,503 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:33:36,503 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:33:36,503 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:33:36,503 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:33:36,503 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:33:36,503 INFO] Option: bucket_size , value: 64 overriding model: 262144
[2024-07-30 09:33:36,503 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:33:36,503 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:33:36,503 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:33:36,503 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:33:36,503 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:33:36,503 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:33:36,503 INFO] Option: batch_size , value: 10240 overriding model: 8192
[2024-07-30 09:33:36,504 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:33:36,504 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:33:36,504 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:33:36,504 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:33:36,504 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:33:36,504 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:33:36,504 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:33:36,504 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:33:36,504 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:33:36,504 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:33:36,504 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:33:36,504 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:33:36,504 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:33:36,504 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:33:36,505 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:33:36,505 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:33:36,505 INFO] Option: _all_transform , value: {'filtertoolong', 'suffix', 'prefix', 'sentencepiece'} overriding model: {'filtertoolong'}
[2024-07-30 09:33:36,505 INFO] Building model...
[2024-07-30 09:33:44,485 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:33:44,486 INFO] Non quantized layer compute is fp16
[2024-07-30 09:33:44,486 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:33:47,510 INFO] src: 0 new tokens
[2024-07-30 09:33:53,400 INFO] tgt: 0 new tokens
[2024-07-30 09:33:54,702 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:33:54,711 INFO] encoder: 337977344
[2024-07-30 09:33:54,711 INFO] decoder: 126283982
[2024-07-30 09:33:54,711 INFO] * number of parameters: 464261326
[2024-07-30 09:33:54,713 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:33:54,714 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:33:54,714 INFO]  * src vocab size = 256206
[2024-07-30 09:33:54,714 INFO]  * tgt vocab size = 256206
[2024-07-30 09:33:55,503 INFO] Starting training on GPU: [0]
[2024-07-30 09:33:55,504 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:33:55,504 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:34:42,927 INFO] Step 10/10000; acc: 2.3; ppl: 196246.7; xent: 12.2; lr: 0.00069; sents:    5119; bsz:  673/ 668/64; 1136/1127 tok/s;     47 sec;
[2024-07-30 09:35:08,204 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:35:09,692 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:35:10,045 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:35:10,365 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:35:10,762 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:35:10,949 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:35:32,208 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:35:32,208 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:35:34,212 WARNING] configured transforms is different from checkpoint: +{'suffix', 'sentencepiece', 'prefix'}
[2024-07-30 09:35:34,213 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:35:34,879 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:35:34,881 INFO] The decoder start token is: </s>
[2024-07-30 09:35:34,921 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:35:34,924 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:35:35,676 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:35:35,678 INFO] The decoder start token is: </s>
[2024-07-30 09:35:35,757 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:35:35,757 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:35:35,757 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:35:35,757 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:35:35,757 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:35:35,757 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:35:35,757 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:35:35,757 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:35:35,758 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:35:35,758 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:35:35,758 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:35:35,758 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:35:35,758 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:35:35,758 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:35:35,758 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:35:35,758 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:35:35,758 INFO] Option: bucket_size , value: 64 overriding model: 262144
[2024-07-30 09:35:35,758 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:35:35,758 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:35:35,758 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:35:35,758 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:35:35,759 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:35:35,759 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:35:35,759 INFO] Option: batch_size , value: 9216 overriding model: 8192
[2024-07-30 09:35:35,759 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:35:35,759 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:35:35,759 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:35:35,759 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:35:35,759 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:35:35,759 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:35:35,759 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:35:35,759 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:35:35,759 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:35:35,759 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:35:35,759 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:35:35,759 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:35:35,760 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:35:35,760 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:35:35,760 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:35:35,760 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:35:35,760 INFO] Option: _all_transform , value: {'filtertoolong', 'suffix', 'sentencepiece', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-30 09:35:35,760 INFO] Building model...
[2024-07-30 09:35:44,019 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:35:44,019 INFO] Non quantized layer compute is fp16
[2024-07-30 09:35:44,020 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:35:46,654 INFO] src: 0 new tokens
[2024-07-30 09:35:52,341 INFO] tgt: 0 new tokens
[2024-07-30 09:35:53,402 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:35:53,410 INFO] encoder: 337977344
[2024-07-30 09:35:53,411 INFO] decoder: 126283982
[2024-07-30 09:35:53,411 INFO] * number of parameters: 464261326
[2024-07-30 09:35:53,413 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:35:53,413 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:35:53,413 INFO]  * src vocab size = 256206
[2024-07-30 09:35:53,413 INFO]  * tgt vocab size = 256206
[2024-07-30 09:35:54,146 INFO] Starting training on GPU: [0]
[2024-07-30 09:35:54,146 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:35:54,147 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:36:41,598 INFO] Step 10/10000; acc: 2.3; ppl: 196246.7; xent: 12.2; lr: 0.00069; sents:    5119; bsz:  673/ 668/64; 1135/1126 tok/s;     47 sec;
[2024-07-30 09:37:06,228 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:37:07,775 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:37:08,106 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:37:08,469 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:37:08,857 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:37:09,028 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:37:09,648 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:10,069 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:10,298 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:10,707 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:11,212 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:11,502 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:11,931 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:12,167 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:37:45,143 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:37:45,143 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:37:47,257 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-30 09:37:47,257 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:37:48,034 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:37:48,037 INFO] The decoder start token is: </s>
[2024-07-30 09:37:48,089 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:37:48,092 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:37:48,910 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:37:48,913 INFO] The decoder start token is: </s>
[2024-07-30 09:37:48,987 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:37:48,987 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:37:48,987 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:37:48,987 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:37:48,987 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:37:48,987 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:37:48,987 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:37:48,987 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:37:48,987 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:37:48,987 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:37:48,988 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:37:48,988 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:37:48,988 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:37:48,988 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:37:48,988 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:37:48,988 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:37:48,988 INFO] Option: bucket_size , value: 32 overriding model: 262144
[2024-07-30 09:37:48,988 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:37:48,988 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:37:48,988 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:37:48,988 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:37:48,988 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:37:48,989 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:37:48,989 INFO] Option: batch_size , value: 7168 overriding model: 8192
[2024-07-30 09:37:48,989 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:37:48,989 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:37:48,989 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:37:48,989 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:37:48,989 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:37:48,989 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:37:48,989 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:37:48,989 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:37:48,989 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:37:48,989 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:37:48,990 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:37:48,990 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:37:48,990 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:37:48,990 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:37:48,990 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:37:48,990 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:37:48,990 INFO] Option: _all_transform , value: {'sentencepiece', 'prefix', 'filtertoolong', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-30 09:37:48,990 INFO] Building model...
[2024-07-30 09:37:57,493 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:37:57,493 INFO] Non quantized layer compute is fp16
[2024-07-30 09:37:57,494 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:38:00,492 INFO] src: 0 new tokens
[2024-07-30 09:38:06,514 INFO] tgt: 0 new tokens
[2024-07-30 09:38:07,784 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:38:07,806 INFO] encoder: 337977344
[2024-07-30 09:38:07,806 INFO] decoder: 126283982
[2024-07-30 09:38:07,806 INFO] * number of parameters: 464261326
[2024-07-30 09:38:07,812 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:38:07,813 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:38:07,813 INFO]  * src vocab size = 256206
[2024-07-30 09:38:07,813 INFO]  * tgt vocab size = 256206
[2024-07-30 09:38:08,944 INFO] Starting training on GPU: [0]
[2024-07-30 09:38:08,944 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:38:08,944 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:38:55,506 INFO] Step 10/10000; acc: 3.2; ppl: 134537.3; xent: 11.8; lr: 0.00069; sents:    2559; bsz:  318/ 225/32; 547/387 tok/s;     47 sec;
[2024-07-30 09:39:24,363 INFO] Step 20/10000; acc: 5.5; ppl: 18230.6; xent: 9.8; lr: 0.00131; sents:    2560; bsz:  355/ 443/32; 984/1228 tok/s;     75 sec;
[2024-07-30 09:39:34,104 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:39:34,104 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:39:36,080 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-30 09:39:36,080 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:39:36,738 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:39:36,740 INFO] The decoder start token is: </s>
[2024-07-30 09:39:36,779 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:39:36,781 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:39:37,537 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:39:37,539 INFO] The decoder start token is: </s>
[2024-07-30 09:39:37,611 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:39:37,611 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:39:37,612 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:39:37,612 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:39:37,612 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:39:37,612 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:39:37,612 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:39:37,612 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:39:37,613 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:39:37,613 INFO] Option: bucket_size , value: 32 overriding model: 262144
[2024-07-30 09:39:37,613 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:39:37,613 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:39:37,613 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:39:37,613 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:39:37,613 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:39:37,613 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:39:37,613 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:39:37,613 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:39:37,613 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:39:37,613 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:39:37,613 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:39:37,613 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:39:37,613 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:39:37,614 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:39:37,614 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:39:37,614 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:39:37,614 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:39:37,614 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:39:37,614 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:39:37,614 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:39:37,614 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:39:37,614 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:39:37,614 INFO] Option: _all_transform , value: {'sentencepiece', 'prefix', 'suffix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-30 09:39:37,614 INFO] Building model...
[2024-07-30 09:39:45,691 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:39:45,692 INFO] Non quantized layer compute is fp16
[2024-07-30 09:39:45,692 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:39:48,326 INFO] src: 0 new tokens
[2024-07-30 09:39:54,367 INFO] tgt: 0 new tokens
[2024-07-30 09:39:55,629 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:39:55,657 INFO] encoder: 337977344
[2024-07-30 09:39:55,657 INFO] decoder: 126283982
[2024-07-30 09:39:55,658 INFO] * number of parameters: 464261326
[2024-07-30 09:39:55,662 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:39:55,662 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:39:55,662 INFO]  * src vocab size = 256206
[2024-07-30 09:39:55,662 INFO]  * tgt vocab size = 256206
[2024-07-30 09:39:56,690 INFO] Starting training on GPU: [0]
[2024-07-30 09:39:56,690 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:39:56,690 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:40:41,215 INFO] Step 10/10000; acc: 3.2; ppl: 134537.3; xent: 11.8; lr: 0.00069; sents:    2559; bsz:  318/ 225/32; 572/404 tok/s;     45 sec;
[2024-07-30 09:41:09,841 INFO] Step 20/10000; acc: 5.5; ppl: 18230.6; xent: 9.8; lr: 0.00131; sents:    2560; bsz:  355/ 443/32; 992/1238 tok/s;     73 sec;
[2024-07-30 09:41:38,588 INFO] Step 30/10000; acc: 6.5; ppl: 2914.6; xent: 8.0; lr: 0.00194; sents:    2559; bsz:  338/ 424/32; 941/1179 tok/s;    102 sec;
[2024-07-30 09:42:06,000 INFO] Step 40/10000; acc: 5.2; ppl: 3412.5; xent: 8.1; lr: 0.00256; sents:    2560; bsz:  652/ 692/32; 1904/2018 tok/s;    129 sec;
[2024-07-30 09:42:10,688 INFO] Step 42, cuda OOM - batch removed
[2024-07-30 09:42:11,035 INFO] Step 42, cuda OOM - batch removed
[2024-07-30 09:42:11,174 INFO] Step 42, cuda OOM - batch removed
[2024-07-30 09:42:11,302 INFO] Step 42, cuda OOM - batch removed
[2024-07-30 09:42:11,382 INFO] Step 42, cuda OOM - batch removed
[2024-07-30 09:42:11,494 INFO] Step 42, cuda OOM - batch removed
[2024-07-30 09:42:12,756 INFO] Step 43, cuda OOM - batch removed
[2024-07-30 09:42:13,166 INFO] Step 43, cuda OOM - batch removed
[2024-07-30 09:42:13,293 INFO] Step 43, cuda OOM - batch removed
[2024-07-30 09:42:13,477 INFO] Step 43, cuda OOM - batch removed
[2024-07-30 09:42:13,594 INFO] Step 43, cuda OOM - batch removed
[2024-07-30 09:42:13,703 INFO] Step 43, cuda OOM - batch removed
[2024-07-30 09:42:49,952 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:42:49,952 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:42:51,947 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'prefix', 'suffix'}
[2024-07-30 09:42:51,947 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:42:52,621 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:42:52,623 INFO] The decoder start token is: </s>
[2024-07-30 09:42:52,664 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:42:52,667 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:42:53,432 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:42:53,435 INFO] The decoder start token is: </s>
[2024-07-30 09:42:53,508 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:42:53,508 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:42:53,508 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:42:53,508 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:42:53,509 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:42:53,509 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:42:53,509 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:42:53,509 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:42:53,509 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:42:53,510 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:42:53,510 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-30 09:42:53,510 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:42:53,510 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:42:53,510 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:42:53,510 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:42:53,510 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:42:53,510 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:42:53,510 INFO] Option: batch_size , value: 6144 overriding model: 8192
[2024-07-30 09:42:53,510 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:42:53,510 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:42:53,510 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:42:53,510 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:42:53,510 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:42:53,511 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:42:53,511 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:42:53,511 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:42:53,511 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:42:53,511 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:42:53,511 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:42:53,511 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:42:53,511 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:42:53,511 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:42:53,511 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:42:53,511 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:42:53,511 INFO] Option: _all_transform , value: {'sentencepiece', 'filtertoolong', 'prefix', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-30 09:42:53,511 INFO] Building model...
[2024-07-30 09:43:01,779 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:43:01,779 INFO] Non quantized layer compute is fp16
[2024-07-30 09:43:01,779 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:43:04,572 INFO] src: 0 new tokens
[2024-07-30 09:43:10,581 INFO] tgt: 0 new tokens
[2024-07-30 09:43:11,706 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:43:11,716 INFO] encoder: 337977344
[2024-07-30 09:43:11,716 INFO] decoder: 126283982
[2024-07-30 09:43:11,716 INFO] * number of parameters: 464261326
[2024-07-30 09:43:11,719 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:43:11,719 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:43:11,719 INFO]  * src vocab size = 256206
[2024-07-30 09:43:11,719 INFO]  * tgt vocab size = 256206
[2024-07-30 09:43:12,624 INFO] Starting training on GPU: [0]
[2024-07-30 09:43:12,624 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:43:12,624 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:43:44,972 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,379 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,468 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,577 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,743 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,825 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,908 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:43:45,990 INFO] Step 1, cuda OOM - batch removed
[2024-07-30 09:44:36,624 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:44:36,624 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:44:38,617 WARNING] configured transforms is different from checkpoint: +{'prefix', 'suffix', 'sentencepiece'}
[2024-07-30 09:44:38,617 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:44:39,284 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:44:39,286 INFO] The decoder start token is: </s>
[2024-07-30 09:44:39,326 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:44:39,329 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 09:44:40,168 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:44:40,171 INFO] The decoder start token is: </s>
[2024-07-30 09:44:40,259 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:44:40,259 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:44:40,260 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:44:40,260 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:44:40,260 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:44:40,260 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:44:40,260 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:44:40,261 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:44:40,261 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:44:40,261 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:44:40,261 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-30 09:44:40,261 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:44:40,261 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:44:40,261 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:44:40,261 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:44:40,261 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:44:40,261 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:44:40,261 INFO] Option: batch_size , value: 4096 overriding model: 8192
[2024-07-30 09:44:40,261 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:44:40,261 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:44:40,261 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:44:40,262 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:44:40,262 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:44:40,262 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:44:40,262 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:44:40,262 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:44:40,262 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:44:40,262 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:44:40,262 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:44:40,262 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:44:40,262 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:44:40,262 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:44:40,262 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:44:40,262 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:44:40,262 INFO] Option: _all_transform , value: {'sentencepiece', 'prefix', 'suffix', 'filtertoolong'} overriding model: {'filtertoolong'}
[2024-07-30 09:44:40,263 INFO] Building model...
[2024-07-30 09:44:48,227 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:44:48,228 INFO] Non quantized layer compute is fp16
[2024-07-30 09:44:48,229 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:44:51,002 INFO] src: 0 new tokens
[2024-07-30 09:44:56,630 INFO] tgt: 0 new tokens
[2024-07-30 09:44:58,003 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:44:58,017 INFO] encoder: 337977344
[2024-07-30 09:44:58,017 INFO] decoder: 126283982
[2024-07-30 09:44:58,017 INFO] * number of parameters: 464261326
[2024-07-30 09:44:58,021 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:44:58,021 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:44:58,022 INFO]  * src vocab size = 256206
[2024-07-30 09:44:58,022 INFO]  * tgt vocab size = 256206
[2024-07-30 09:44:58,868 INFO] Starting training on GPU: [0]
[2024-07-30 09:44:58,869 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:44:58,869 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 09:45:40,099 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:40,424 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:40,531 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:40,631 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:40,763 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:40,857 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:40,957 INFO] Step 3, cuda OOM - batch removed
[2024-07-30 09:45:41,851 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:42,202 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:42,369 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:42,454 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:42,528 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:42,606 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:42,699 INFO] Step 4, cuda OOM - batch removed
[2024-07-30 09:45:43,768 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:44,139 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:44,240 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:44,356 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:44,541 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:44,648 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:44,823 INFO] Step 5, cuda OOM - batch removed
[2024-07-30 09:45:45,794 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:46,153 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:46,260 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:46,367 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:46,444 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:46,554 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:46,639 INFO] Step 6, cuda OOM - batch removed
[2024-07-30 09:45:47,603 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:47,924 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:48,036 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:48,119 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:48,188 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:48,259 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:48,328 INFO] Step 7, cuda OOM - batch removed
[2024-07-30 09:45:49,489 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:49,789 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:49,869 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:49,962 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:50,060 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:50,154 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:50,243 INFO] Step 8, cuda OOM - batch removed
[2024-07-30 09:45:51,214 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:51,559 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:51,671 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:51,770 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:52,025 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:52,156 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:52,434 INFO] Step 9, cuda OOM - batch removed
[2024-07-30 09:45:53,610 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:53,979 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:54,141 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:54,384 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:54,610 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:54,864 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:55,127 INFO] Step 10, cuda OOM - batch removed
[2024-07-30 09:45:55,244 INFO] Step 10/10000; acc: 0.6; ppl: 392250.5; xent: 12.9; lr: 0.00069; sents:    4393; bsz: 8878/2891/183; 3779/1231 tok/s;     56 sec;
[2024-07-30 09:45:56,303 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:56,664 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:56,794 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:56,879 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:57,050 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:57,132 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:57,248 INFO] Step 11, cuda OOM - batch removed
[2024-07-30 09:45:58,593 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:45:58,960 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:45:59,041 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:45:59,160 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:45:59,267 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:45:59,391 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:45:59,509 INFO] Step 12, cuda OOM - batch removed
[2024-07-30 09:46:00,673 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:01,050 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:01,184 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:01,285 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:01,362 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:01,431 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:01,510 INFO] Step 13, cuda OOM - batch removed
[2024-07-30 09:46:02,561 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:02,928 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:03,081 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:03,168 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:03,240 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:03,329 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:03,411 INFO] Step 14, cuda OOM - batch removed
[2024-07-30 09:46:04,789 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:05,146 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:05,254 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:05,327 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:05,450 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:05,536 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:05,623 INFO] Step 15, cuda OOM - batch removed
[2024-07-30 09:46:06,532 INFO] Step 16, cuda OOM - batch removed
[2024-07-30 09:46:07,427 INFO] Step 16, cuda OOM - batch removed
[2024-07-30 09:46:07,781 INFO] Step 16, cuda OOM - batch removed
[2024-07-30 09:46:07,871 INFO] Step 16, cuda OOM - batch removed
[2024-07-30 09:46:08,138 INFO] Step 16, cuda OOM - batch removed
[2024-07-30 09:46:08,271 INFO] Step 16, cuda OOM - batch removed
[2024-07-30 09:46:09,278 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:09,666 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:10,130 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:10,412 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:10,836 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:11,171 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:11,275 INFO] Step 17, cuda OOM - batch removed
[2024-07-30 09:46:12,598 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:12,933 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:13,052 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:13,151 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:13,245 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:13,355 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:13,435 INFO] Step 18, cuda OOM - batch removed
[2024-07-30 09:46:14,593 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:14,945 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:15,072 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:15,177 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:15,315 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:15,427 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:15,542 INFO] Step 19, cuda OOM - batch removed
[2024-07-30 09:46:16,633 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:16,983 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:17,061 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:17,177 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:17,255 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:17,405 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:17,478 INFO] Step 20, cuda OOM - batch removed
[2024-07-30 09:46:17,570 INFO] Step 20/10000; acc: 3.6; ppl: 7760.2; xent: 9.0; lr: 0.00131; sents:    1722; bsz: 20940/2842/157; 10317/1400 tok/s;     79 sec;
[2024-07-30 09:46:18,581 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:18,931 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:19,103 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:19,189 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:19,324 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:19,404 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:19,533 INFO] Step 21, cuda OOM - batch removed
[2024-07-30 09:46:20,495 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:20,827 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:20,987 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:21,120 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:21,208 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:21,329 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:21,426 INFO] Step 22, cuda OOM - batch removed
[2024-07-30 09:46:22,257 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:22,662 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:22,917 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:23,152 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:23,252 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:23,378 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:23,477 INFO] Step 23, cuda OOM - batch removed
[2024-07-30 09:46:24,721 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:25,104 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:25,352 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:25,527 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:25,759 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:25,866 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:26,160 INFO] Step 24, cuda OOM - batch removed
[2024-07-30 09:46:27,442 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:27,792 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:27,876 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:27,988 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:28,089 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:28,243 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:28,330 INFO] Step 25, cuda OOM - batch removed
[2024-07-30 09:46:29,596 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:29,940 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:30,044 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:30,171 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:30,258 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:30,365 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:30,456 INFO] Step 26, cuda OOM - batch removed
[2024-07-30 09:46:31,617 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:31,995 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:32,083 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:32,164 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:32,261 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:32,347 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:32,437 INFO] Step 27, cuda OOM - batch removed
[2024-07-30 09:46:33,726 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:34,130 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:34,362 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:34,703 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:35,083 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:35,420 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:35,551 INFO] Step 28, cuda OOM - batch removed
[2024-07-30 09:46:36,672 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:37,003 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:37,097 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:37,171 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:37,235 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:37,315 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:37,383 INFO] Step 29, cuda OOM - batch removed
[2024-07-30 09:46:38,021 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:38,385 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:38,500 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:38,591 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:38,700 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:38,841 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:38,955 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:46:39,084 INFO] Step 30, cuda OOM - batch removed
[2024-07-30 09:58:48,886 INFO] Parsed 2 corpora from -data.
[2024-07-30 09:58:48,886 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 09:58:50,689 WARNING] configured transforms is different from checkpoint: +{'suffix', 'prefix', 'sentencepiece'}
[2024-07-30 09:58:50,690 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:58:51,350 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:58:51,351 INFO] The decoder start token is: </s>
[2024-07-30 09:58:51,397 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 09:58:51,399 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.
[2024-07-30 09:58:52,122 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 09:58:52,124 INFO] The decoder start token is: </s>
[2024-07-30 09:58:52,189 INFO] Over-ride model option set to true - use with care
[2024-07-30 09:58:52,190 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 09:58:52,190 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 09:58:52,190 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 09:58:52,190 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 09:58:52,190 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:58:52,190 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 09:58:52,191 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 09:58:52,191 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 09:58:52,191 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 09:58:52,191 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-30 09:58:52,191 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 09:58:52,191 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 09:58:52,191 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 09:58:52,191 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 09:58:52,191 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 09:58:52,191 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 09:58:52,191 INFO] Option: batch_size , value: 2048 overriding model: 8192
[2024-07-30 09:58:52,192 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 09:58:52,192 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:58:52,192 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 09:58:52,192 INFO] Option: valid_batch_size , value: 32 overriding model: 4096
[2024-07-30 09:58:52,192 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 09:58:52,192 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 09:58:52,192 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 09:58:52,192 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 09:58:52,192 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 09:58:52,192 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 09:58:52,192 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 09:58:52,192 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 09:58:52,192 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 09:58:52,192 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 09:58:52,193 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 09:58:52,193 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 09:58:52,193 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'filtertoolong', 'prefix'} overriding model: {'filtertoolong'}
[2024-07-30 09:58:52,193 INFO] Building model...
[2024-07-30 09:59:00,637 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 09:59:00,637 INFO] Non quantized layer compute is fp16
[2024-07-30 09:59:00,637 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 09:59:03,465 INFO] src: 0 new tokens
[2024-07-30 09:59:09,137 INFO] tgt: 0 new tokens
[2024-07-30 09:59:10,271 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 09:59:10,283 INFO] encoder: 337977344
[2024-07-30 09:59:10,283 INFO] decoder: 126283982
[2024-07-30 09:59:10,283 INFO] * number of parameters: 464261326
[2024-07-30 09:59:10,286 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:59:10,287 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 09:59:10,287 INFO]  * src vocab size = 256206
[2024-07-30 09:59:10,287 INFO]  * tgt vocab size = 256206
[2024-07-30 09:59:11,276 INFO] Starting training on GPU: [0]
[2024-07-30 09:59:11,276 INFO] Start training loop and validate every 100 steps...
[2024-07-30 09:59:11,276 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 10:00:10,124 INFO] Step 10/10000; acc: 1.4; ppl: 153464.5; xent: 11.9; lr: 0.00069; sents:    7961; bsz: 1345/1659/100; 1828/2255 tok/s;     59 sec;
[2024-07-30 10:00:35,312 INFO] Step 20/10000; acc: 4.3; ppl: 3921.7; xent: 8.3; lr: 0.00131; sents:    7564; bsz: 1562/1631/95; 4961/5180 tok/s;     84 sec;
[2024-07-30 10:01:02,713 INFO] Step 30/10000; acc: 4.9; ppl: 1985.6; xent: 7.6; lr: 0.00194; sents:    7683; bsz: 1573/1666/96; 4593/4863 tok/s;    111 sec;
[2024-07-30 10:01:30,807 INFO] Step 40/10000; acc: 6.3; ppl: 1402.2; xent: 7.2; lr: 0.00256; sents:    8941; bsz: 1496/1584/112; 4261/4512 tok/s;    140 sec;
[2024-07-30 10:01:49,315 INFO] Parsed 2 corpora from -data.
[2024-07-30 10:01:49,315 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt
[2024-07-30 10:01:51,190 WARNING] configured transforms is different from checkpoint: +{'sentencepiece', 'suffix', 'prefix'}
[2024-07-30 10:01:51,190 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 10:01:51,864 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 10:01:51,866 INFO] The decoder start token is: </s>
[2024-07-30 10:01:51,904 INFO] Updating checkpoint vocabulary with new vocabulary
[2024-07-30 10:01:51,906 INFO] Get special vocabs from Transforms: {'src': ['dyu_Latn', '</s>'], 'tgt': ['fra_Latn', '']}.
[2024-07-30 10:01:52,634 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']
[2024-07-30 10:01:52,636 INFO] The decoder start token is: </s>
[2024-07-30 10:01:52,700 INFO] Over-ride model option set to true - use with care
[2024-07-30 10:01:52,700 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: 
[2024-07-30 10:01:52,700 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}
[2024-07-30 10:01:52,700 INFO] Option: skip_empty_level , value: warning overriding model: silent
[2024-07-30 10:01:52,701 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: 
[2024-07-30 10:01:52,703 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 10:01:52,703 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: 
[2024-07-30 10:01:52,704 INFO] Option: src_seq_length , value: 130 overriding model: 150
[2024-07-30 10:01:52,704 INFO] Option: tgt_seq_length , value: 192 overriding model: 150
[2024-07-30 10:01:52,704 INFO] Option: src_prefix , value: dyu_Latn overriding model: 
[2024-07-30 10:01:52,704 INFO] Option: tgt_prefix , value: fra_Latn overriding model: 
[2024-07-30 10:01:52,704 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 10:01:52,704 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: 
[2024-07-30 10:01:52,704 INFO] Option: update_vocab , value: True overriding model: False
[2024-07-30 10:01:52,704 INFO] Option: heads , value: 8 overriding model: 16
[2024-07-30 10:01:52,704 INFO] Option: transformer_ff , value: 1024 overriding model: 4096
[2024-07-30 10:01:52,704 INFO] Option: bucket_size , value: 65536 overriding model: 262144
[2024-07-30 10:01:52,704 INFO] Option: prefetch_factor , value: 200 overriding model: 400
[2024-07-30 10:01:52,704 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb
[2024-07-30 10:01:52,705 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000
[2024-07-30 10:01:52,705 INFO] Option: keep_checkpoint , value: 3 overriding model: 50
[2024-07-30 10:01:52,705 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: 
[2024-07-30 10:01:52,705 INFO] Option: reset_optim , value: all overriding model: none
[2024-07-30 10:01:52,705 INFO] Option: batch_size , value: 3072 overriding model: 8192
[2024-07-30 10:01:52,705 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]
[2024-07-30 10:01:52,705 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 10:01:52,705 INFO] Option: valid_steps , value: 100 overriding model: 5000
[2024-07-30 10:01:52,705 INFO] Option: valid_batch_size , value: 1024 overriding model: 4096
[2024-07-30 10:01:52,705 INFO] Option: train_steps , value: 10000 overriding model: 100000
[2024-07-30 10:01:52,705 INFO] Option: early_stopping , value: 15 overriding model: 0
[2024-07-30 10:01:52,705 INFO] Option: optim , value: adam overriding model: 
[2024-07-30 10:01:52,705 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]
[2024-07-30 10:01:52,705 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]
[2024-07-30 10:01:52,706 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]
[2024-07-30 10:01:52,706 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0
[2024-07-30 10:01:52,706 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05
[2024-07-30 10:01:52,706 INFO] Option: decay_method , value: noam overriding model: none
[2024-07-30 10:01:52,706 INFO] Option: warmup_steps , value: 100 overriding model: 4000
[2024-07-30 10:01:52,706 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: 
[2024-07-30 10:01:52,706 INFO] Option: report_every , value: 10 overriding model: 100
[2024-07-30 10:01:52,706 INFO] Option: _all_transform , value: {'prefix', 'sentencepiece', 'filtertoolong', 'suffix'} overriding model: {'filtertoolong'}
[2024-07-30 10:01:52,706 INFO] Building model...
[2024-07-30 10:02:00,909 INFO] Switching model to float32 for amp/apex_amp
[2024-07-30 10:02:00,909 INFO] Non quantized layer compute is fp16
[2024-07-30 10:02:00,909 INFO] Updating vocabulary embeddings with checkpoint embeddings
[2024-07-30 10:02:03,495 INFO] src: 0 new tokens
[2024-07-30 10:02:09,076 INFO] tgt: 0 new tokens
[2024-07-30 10:02:10,175 INFO] NMTModel(
  (encoder): TransformerEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (transformer): ModuleList(
      (0-11): 12 x TransformerEncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(256206, 1024, padding_idx=1)
        )
        (pe): PositionalEncoding()
      )
      (dropout): Dropout(p=0.7, inplace=False)
    )
    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    (transformer_layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=1024, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=1024, bias=True)
          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.7, inplace=False)
          (dropout_2): Dropout(p=0.7, inplace=False)
        )
        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.7, inplace=False)
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)
          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)
          (softmax): Softmax(dim=-1)
          (dropout): Dropout(p=0.5, inplace=False)
          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      )
    )
  )
  (generator): Linear(in_features=1024, out_features=256206, bias=True)
)
[2024-07-30 10:02:10,195 INFO] encoder: 337977344
[2024-07-30 10:02:10,195 INFO] decoder: 126283982
[2024-07-30 10:02:10,195 INFO] * number of parameters: 464261326
[2024-07-30 10:02:10,201 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 10:02:10,201 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-07-30 10:02:10,201 INFO]  * src vocab size = 256206
[2024-07-30 10:02:10,202 INFO]  * tgt vocab size = 256206
[2024-07-30 10:02:11,005 INFO] Starting training on GPU: [0]
[2024-07-30 10:02:11,005 INFO] Start training loop and validate every 100 steps...
[2024-07-30 10:02:11,005 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']
[2024-07-30 10:03:14,469 INFO] Step 10/10000; acc: 1.4; ppl: 135487.0; xent: 11.8; lr: 0.00069; sents:   12536; bsz: 2240/2391/157; 2824/3014 tok/s;     63 sec;
[2024-07-30 10:03:48,334 INFO] Step 20/10000; acc: 4.4; ppl: 3437.2; xent: 8.1; lr: 0.00131; sents:   12355; bsz: 2324/2439/154; 5491/5762 tok/s;     97 sec;
[2024-07-30 10:04:20,261 INFO] Step 30/10000; acc: 5.8; ppl: 1678.1; xent: 7.4; lr: 0.00194; sents:   12581; bsz: 2154/2363/157; 5398/5921 tok/s;    129 sec;
[2024-07-30 10:04:52,815 INFO] Step 40/10000; acc: 5.7; ppl: 1493.1; xent: 7.3; lr: 0.00256; sents:   12325; bsz: 2389/2492/154; 5871/6124 tok/s;    162 sec;
[2024-07-30 10:05:26,893 INFO] Step 50/10000; acc: 5.5; ppl: 1608.4; xent: 7.4; lr: 0.00319; sents:    9757; bsz: 2197/2305/122; 5157/5411 tok/s;    196 sec;
[2024-07-30 10:05:59,958 INFO] Step 60/10000; acc: 7.7; ppl: 1262.7; xent: 7.1; lr: 0.00381; sents:   13403; bsz: 2357/2466/168; 5703/5966 tok/s;    229 sec;
[2024-07-30 10:06:32,013 INFO] Step 70/10000; acc: 9.0; ppl: 1258.7; xent: 7.1; lr: 0.00444; sents:   12543; bsz: 2351/2508/157; 5867/6258 tok/s;    261 sec;
[2024-07-30 10:07:02,954 INFO] Step 80/10000; acc: 12.6; ppl: 1090.9; xent: 7.0; lr: 0.00506; sents:   12509; bsz: 2202/2501/156; 5693/6468 tok/s;    292 sec;
[2024-07-30 10:07:34,017 INFO] Step 90/10000; acc: 14.4; ppl: 1257.5; xent: 7.1; lr: 0.00569; sents:   10706; bsz: 2349/2517/134; 6050/6482 tok/s;    323 sec;
[2024-07-30 10:08:04,438 INFO] Step 100/10000; acc: 16.1; ppl: 935.3; xent: 6.8; lr: 0.00622; sents:   10493; bsz: 2307/2406/131; 6068/6327 tok/s;    353 sec;
[2024-07-30 10:08:38,070 INFO] valid stats calculation
                           took: 33.622756242752075 s.
[2024-07-30 10:08:38,080 INFO] Train perplexity: 2256.94
[2024-07-30 10:08:38,080 INFO] Train accuracy: 8.31382
[2024-07-30 10:08:38,081 INFO] Sentences processed: 119208
[2024-07-30 10:08:38,081 INFO] Average bsz: 2287/2439/149
[2024-07-30 10:08:38,081 INFO] Validation perplexity: 955.872
[2024-07-30 10:08:38,081 INFO] Validation accuracy: 17.9656
[2024-07-30 10:08:38,081 INFO] Model is improving ppl: inf --> 955.872.
[2024-07-30 10:08:38,081 INFO] Model is improving acc: -inf --> 17.9656.
[2024-07-30 10:09:08,332 INFO] Step 110/10000; acc: 20.7; ppl: 730.8; xent: 6.6; lr: 0.00593; sents:   12097; bsz: 2475/2487/151; 3099/3114 tok/s;    417 sec;
[2024-07-30 10:09:39,704 INFO] Step 120/10000; acc: 21.7; ppl: 724.5; xent: 6.6; lr: 0.00568; sents:    9742; bsz: 2112/2296/122; 5385/5854 tok/s;    449 sec;
[2024-07-30 10:10:11,444 INFO] Step 130/10000; acc: 21.4; ppl: 685.5; xent: 6.5; lr: 0.00546; sents:    8757; bsz: 2290/2401/109; 5771/6051 tok/s;    480 sec;
[2024-07-30 10:10:41,813 INFO] Step 140/10000; acc: 23.1; ppl: 563.5; xent: 6.3; lr: 0.00526; sents:    9649; bsz: 2294/2412/121; 6042/6353 tok/s;    511 sec;
[2024-07-30 10:11:14,440 INFO] Step 150/10000; acc: 24.9; ppl: 461.5; xent: 6.1; lr: 0.00509; sents:   10497; bsz: 2238/2393/131; 5487/5867 tok/s;    543 sec;
[2024-07-30 10:11:47,082 INFO] Step 160/10000; acc: 26.7; ppl: 386.0; xent: 6.0; lr: 0.00493; sents:   11067; bsz: 2259/2370/138; 5536/5808 tok/s;    576 sec;
[2024-07-30 10:12:20,437 INFO] Step 170/10000; acc: 28.0; ppl: 342.5; xent: 5.8; lr: 0.00478; sents:   11684; bsz: 2281/2447/146; 5470/5869 tok/s;    609 sec;
[2024-07-30 10:12:53,761 INFO] Step 180/10000; acc: 27.6; ppl: 331.5; xent: 5.8; lr: 0.00465; sents:   11435; bsz: 2274/2450/143; 5458/5881 tok/s;    643 sec;
[2024-07-30 10:13:26,946 INFO] Step 190/10000; acc: 28.3; ppl: 305.2; xent: 5.7; lr: 0.00452; sents:   11200; bsz: 2261/2461/140; 5451/5933 tok/s;    676 sec;
[2024-07-30 10:14:00,149 INFO] Step 200/10000; acc: 30.3; ppl: 259.8; xent: 5.6; lr: 0.00441; sents:   12565; bsz: 2308/2570/157; 5562/6192 tok/s;    709 sec;
[2024-07-30 10:14:34,659 INFO] valid stats calculation
                           took: 34.493996143341064 s.
[2024-07-30 10:14:34,668 INFO] Train perplexity: 1004.62
[2024-07-30 10:14:34,668 INFO] Train accuracy: 16.7988
[2024-07-30 10:14:34,668 INFO] Sentences processed: 227901
[2024-07-30 10:14:34,668 INFO] Average bsz: 2283/2434/142
[2024-07-30 10:14:34,668 INFO] Validation perplexity: 576.05
[2024-07-30 10:14:34,668 INFO] Validation accuracy: 30.5879
[2024-07-30 10:14:34,668 INFO] Model is improving ppl: 955.872 --> 576.05.
[2024-07-30 10:14:34,669 INFO] Model is improving acc: 17.9656 --> 30.5879.
[2024-07-30 10:15:07,124 INFO] Step 210/10000; acc: 30.7; ppl: 249.3; xent: 5.5; lr: 0.00430; sents:   11127; bsz: 2197/2271/139; 2624/2713 tok/s;    776 sec;
[2024-07-30 10:15:40,509 INFO] Step 220/10000; acc: 27.4; ppl: 285.9; xent: 5.7; lr: 0.00420; sents:    9265; bsz: 2248/2401/116; 5387/5753 tok/s;    810 sec;
[2024-07-30 10:16:13,854 INFO] Step 230/10000; acc: 29.7; ppl: 246.7; xent: 5.5; lr: 0.00411; sents:   10299; bsz: 2256/2333/129; 5413/5598 tok/s;    843 sec;
[2024-07-30 10:16:45,453 INFO] Step 240/10000; acc: 32.4; ppl: 206.5; xent: 5.3; lr: 0.00403; sents:   12048; bsz: 2307/2407/151; 5840/6095 tok/s;    874 sec;
[2024-07-30 10:17:15,833 INFO] Step 250/10000; acc: 34.1; ppl: 185.2; xent: 5.2; lr: 0.00394; sents:   12953; bsz: 2208/2462/162; 5814/6484 tok/s;    905 sec;
[2024-07-30 10:17:49,378 INFO] Step 260/10000; acc: 33.0; ppl: 191.1; xent: 5.3; lr: 0.00387; sents:   11631; bsz: 2197/2372/145; 5239/5657 tok/s;    938 sec;
[2024-07-30 10:18:22,165 INFO] Step 270/10000; acc: 33.0; ppl: 186.7; xent: 5.2; lr: 0.00380; sents:   11722; bsz: 2413/2443/147; 5887/5960 tok/s;    971 sec;
[2024-07-30 10:18:54,180 INFO] Step 280/10000; acc: 35.2; ppl: 163.9; xent: 5.1; lr: 0.00373; sents:   12741; bsz: 2263/2390/159; 5656/5972 tok/s;   1003 sec;
[2024-07-30 10:19:26,196 INFO] Step 290/10000; acc: 32.9; ppl: 180.7; xent: 5.2; lr: 0.00366; sents:   11308; bsz: 2321/2428/141; 5799/6067 tok/s;   1035 sec;
[2024-07-30 10:19:57,879 INFO] Step 300/10000; acc: 34.8; ppl: 162.1; xent: 5.1; lr: 0.00360; sents:   12481; bsz: 2297/2528/156; 5799/6384 tok/s;   1067 sec;
[2024-07-30 10:20:32,064 INFO] valid stats calculation
                           took: 34.16839027404785 s.
[2024-07-30 10:20:32,073 INFO] Train perplexity: 590.886
[2024-07-30 10:20:32,073 INFO] Train accuracy: 21.9367
[2024-07-30 10:20:32,073 INFO] Sentences processed: 343476
[2024-07-30 10:20:32,073 INFO] Average bsz: 2279/2424/143
[2024-07-30 10:20:32,074 INFO] Validation perplexity: 345.139
[2024-07-30 10:20:32,074 INFO] Validation accuracy: 33.9802
[2024-07-30 10:20:32,074 INFO] Model is improving ppl: 576.05 --> 345.139.
[2024-07-30 10:20:32,074 INFO] Model is improving acc: 30.5879 --> 33.9802.
[2024-07-30 10:21:04,608 INFO] Step 310/10000; acc: 34.7; ppl: 158.3; xent: 5.1; lr: 0.00354; sents:   11416; bsz: 2270/2409/143; 2721/2888 tok/s;   1134 sec;
[2024-07-30 10:21:36,214 INFO] Step 320/10000; acc: 34.0; ppl: 159.6; xent: 5.1; lr: 0.00349; sents:   10986; bsz: 2331/2486/137; 5901/6294 tok/s;   1165 sec;
[2024-07-30 10:22:08,687 INFO] Step 330/10000; acc: 32.7; ppl: 171.3; xent: 5.1; lr: 0.00344; sents:    9918; bsz: 2250/2353/124; 5543/5796 tok/s;   1198 sec;
[2024-07-30 10:22:43,314 INFO] Step 340/10000; acc: 34.5; ppl: 155.0; xent: 5.0; lr: 0.00338; sents:   10853; bsz: 2269/2393/136; 5243/5528 tok/s;   1232 sec;
[2024-07-30 10:23:13,779 INFO] Step 350/10000; acc: 36.8; ppl: 137.0; xent: 4.9; lr: 0.00334; sents:   11970; bsz: 2197/2311/150; 5769/6068 tok/s;   1263 sec;
[2024-07-30 10:23:46,008 INFO] Step 360/10000; acc: 36.6; ppl: 132.6; xent: 4.9; lr: 0.00329; sents:   11802; bsz: 2286/2453/148; 5673/6089 tok/s;   1295 sec;
[2024-07-30 10:24:17,725 INFO] Step 370/10000; acc: 34.7; ppl: 147.0; xent: 5.0; lr: 0.00324; sents:   10544; bsz: 2307/2387/132; 5820/6021 tok/s;   1327 sec;
[2024-07-30 10:24:49,564 INFO] Step 380/10000; acc: 36.7; ppl: 133.4; xent: 4.9; lr: 0.00320; sents:   11983; bsz: 2280/2475/150; 5728/6218 tok/s;   1359 sec;
[2024-07-30 10:25:23,137 INFO] Step 390/10000; acc: 37.9; ppl: 120.8; xent: 4.8; lr: 0.00316; sents:   12468; bsz: 2382/2513/156; 5677/5989 tok/s;   1392 sec;
[2024-07-30 10:25:53,829 INFO] Step 400/10000; acc: 37.1; ppl: 126.5; xent: 4.8; lr: 0.00312; sents:   11439; bsz: 2283/2348/143; 5950/6120 tok/s;   1423 sec;
[2024-07-30 10:26:28,574 INFO] valid stats calculation
                           took: 34.73101186752319 s.
[2024-07-30 10:26:28,589 INFO] Train perplexity: 415.083
[2024-07-30 10:26:28,589 INFO] Train accuracy: 25.3337
[2024-07-30 10:26:28,589 INFO] Sentences processed: 456855
[2024-07-30 10:26:28,589 INFO] Average bsz: 2281/2421/143
[2024-07-30 10:26:28,590 INFO] Validation perplexity: 238.502
[2024-07-30 10:26:28,590 INFO] Validation accuracy: 36.271
[2024-07-30 10:26:28,590 INFO] Model is improving ppl: 345.139 --> 238.502.
[2024-07-30 10:26:28,590 INFO] Model is improving acc: 33.9802 --> 36.271.
[2024-07-30 10:26:59,926 INFO] Step 410/10000; acc: 37.3; ppl: 125.0; xent: 4.8; lr: 0.00308; sents:   12063; bsz: 2239/2514/151; 2711/3042 tok/s;   1489 sec;
[2024-07-30 10:27:32,100 INFO] Step 420/10000; acc: 37.4; ppl: 123.1; xent: 4.8; lr: 0.00305; sents:   11346; bsz: 2279/2357/142; 5668/5862 tok/s;   1521 sec;
[2024-07-30 10:28:06,848 INFO] Step 430/10000; acc: 38.1; ppl: 116.3; xent: 4.8; lr: 0.00301; sents:   11611; bsz: 2383/2448/145; 5487/5635 tok/s;   1556 sec;
[2024-07-30 10:28:40,930 INFO] Step 440/10000; acc: 38.8; ppl: 112.9; xent: 4.7; lr: 0.00298; sents:   11863; bsz: 2256/2429/148; 5296/5701 tok/s;   1590 sec;
[2024-07-30 10:29:16,018 INFO] Step 450/10000; acc: 37.4; ppl: 123.2; xent: 4.8; lr: 0.00294; sents:   10335; bsz: 2115/2316/129; 4822/5279 tok/s;   1625 sec;
[2024-07-30 10:29:49,922 INFO] Step 460/10000; acc: 36.6; ppl: 123.5; xent: 4.8; lr: 0.00291; sents:    9520; bsz: 2195/2271/119; 5180/5359 tok/s;   1659 sec;
[2024-07-30 10:30:24,231 INFO] Step 470/10000; acc: 38.0; ppl: 114.7; xent: 4.7; lr: 0.00288; sents:   10895; bsz: 2244/2482/136; 5232/5788 tok/s;   1693 sec;
[2024-07-30 10:30:57,808 INFO] Step 480/10000; acc: 38.3; ppl: 115.1; xent: 4.7; lr: 0.00285; sents:   10495; bsz: 2144/2356/131; 5108/5614 tok/s;   1727 sec;
[2024-07-30 10:31:31,188 INFO] Step 490/10000; acc: 39.7; ppl: 103.8; xent: 4.6; lr: 0.00282; sents:   12092; bsz: 2370/2458/151; 5679/5891 tok/s;   1760 sec;
[2024-07-30 10:32:04,956 INFO] Step 500/10000; acc: 38.0; ppl: 115.2; xent: 4.7; lr: 0.00279; sents:   10354; bsz: 2204/2400/129; 5222/5686 tok/s;   1794 sec;
[2024-07-30 10:32:40,515 INFO] valid stats calculation
                           took: 35.542184591293335 s.
[2024-07-30 10:32:40,529 INFO] Train perplexity: 322.716
[2024-07-30 10:32:40,529 INFO] Train accuracy: 27.8484
[2024-07-30 10:32:40,529 INFO] Sentences processed: 567429
[2024-07-30 10:32:40,529 INFO] Average bsz: 2273/2417/142
[2024-07-30 10:32:40,529 INFO] Validation perplexity: 194.334
[2024-07-30 10:32:40,530 INFO] Validation accuracy: 37.5013
[2024-07-30 10:32:40,530 INFO] Model is improving ppl: 238.502 --> 194.334.
[2024-07-30 10:32:40,530 INFO] Model is improving acc: 36.271 --> 37.5013.
[2024-07-30 10:32:40,741 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V18_step_500.pt
[2024-07-30 10:33:23,550 INFO] Step 510/10000; acc: 42.4; ppl:  88.9; xent: 4.5; lr: 0.00276; sents:   13072; bsz: 2227/2385/163; 2267/2428 tok/s;   1873 sec;
[2024-07-30 10:33:53,059 INFO] Step 520/10000; acc: 40.3; ppl:  96.1; xent: 4.6; lr: 0.00274; sents:   11136; bsz: 2202/2401/139; 5968/6509 tok/s;   1902 sec;
[2024-07-30 10:34:25,160 INFO] Step 530/10000; acc: 38.7; ppl: 103.5; xent: 4.6; lr: 0.00271; sents:   10164; bsz: 2228/2426/127; 5553/6045 tok/s;   1934 sec;
[2024-07-30 10:34:57,803 INFO] Step 540/10000; acc: 40.2; ppl:  96.0; xent: 4.6; lr: 0.00269; sents:   10861; bsz: 2175/2347/136; 5331/5752 tok/s;   1967 sec;
[2024-07-30 10:35:29,221 INFO] Step 550/10000; acc: 40.3; ppl:  95.4; xent: 4.6; lr: 0.00266; sents:   11116; bsz: 2264/2484/139; 5765/6324 tok/s;   1998 sec;
[2024-07-30 10:35:59,962 INFO] Step 560/10000; acc: 42.4; ppl:  84.9; xent: 4.4; lr: 0.00264; sents:   12306; bsz: 2309/2438/154; 6008/6345 tok/s;   2029 sec;
[2024-07-30 10:36:30,830 INFO] Step 570/10000; acc: 42.3; ppl:  84.0; xent: 4.4; lr: 0.00262; sents:   12160; bsz: 2371/2473/152; 6146/6410 tok/s;   2060 sec;
[2024-07-30 10:37:00,119 INFO] Step 580/10000; acc: 42.7; ppl:  81.7; xent: 4.4; lr: 0.00259; sents:   12214; bsz: 2326/2404/153; 6354/6567 tok/s;   2089 sec;
[2024-07-30 10:37:32,422 INFO] Step 590/10000; acc: 42.6; ppl:  82.9; xent: 4.4; lr: 0.00257; sents:   12056; bsz: 2225/2459/151; 5509/6090 tok/s;   2121 sec;
[2024-07-30 10:38:07,000 INFO] Step 600/10000; acc: 42.2; ppl:  84.7; xent: 4.4; lr: 0.00255; sents:   11600; bsz: 2302/2499/145; 5327/5782 tok/s;   2156 sec;
[2024-07-30 10:38:41,920 INFO] valid stats calculation
                           took: 34.90391516685486 s.
[2024-07-30 10:38:41,939 INFO] Train perplexity: 260.336
[2024-07-30 10:38:41,939 INFO] Train accuracy: 30.1221
[2024-07-30 10:38:41,939 INFO] Sentences processed: 684114
[2024-07-30 10:38:41,939 INFO] Average bsz: 2271/2420/143
[2024-07-30 10:38:41,939 INFO] Validation perplexity: 197.275
[2024-07-30 10:38:41,939 INFO] Validation accuracy: 37.5219
[2024-07-30 10:38:41,939 INFO] Stalled patience: 14/15
[2024-07-30 10:39:14,974 INFO] Step 610/10000; acc: 42.4; ppl:  83.6; xent: 4.4; lr: 0.00253; sents:   11718; bsz: 2299/2476/146; 2706/2914 tok/s;   2224 sec;
[2024-07-30 10:39:47,715 INFO] Step 620/10000; acc: 41.8; ppl:  87.1; xent: 4.5; lr: 0.00251; sents:   11008; bsz: 2132/2389/138; 5209/5836 tok/s;   2257 sec;
[2024-07-30 10:40:19,903 INFO] Step 630/10000; acc: 43.5; ppl:  78.0; xent: 4.4; lr: 0.00249; sents:   12748; bsz: 2402/2558/159; 5970/6359 tok/s;   2289 sec;
[2024-07-30 10:40:53,159 INFO] Step 640/10000; acc: 41.4; ppl:  87.9; xent: 4.5; lr: 0.00247; sents:   10585; bsz: 2318/2368/132; 5577/5697 tok/s;   2322 sec;
[2024-07-30 10:41:26,175 INFO] Step 650/10000; acc: 41.4; ppl:  86.4; xent: 4.5; lr: 0.00245; sents:   10335; bsz: 2259/2369/129; 5474/5741 tok/s;   2355 sec;
[2024-07-30 10:41:59,799 INFO] Step 660/10000; acc: 41.3; ppl:  86.7; xent: 4.5; lr: 0.00243; sents:   10903; bsz: 2373/2518/136; 5646/5991 tok/s;   2389 sec;
[2024-07-30 10:42:30,782 INFO] Step 670/10000; acc: 42.4; ppl:  82.7; xent: 4.4; lr: 0.00241; sents:   11356; bsz: 2322/2384/142; 5996/6157 tok/s;   2420 sec;
[2024-07-30 10:43:02,663 INFO] Step 680/10000; acc: 43.1; ppl:  78.3; xent: 4.4; lr: 0.00240; sents:   11916; bsz: 2460/2508/149; 6172/6293 tok/s;   2452 sec;
[2024-07-30 10:43:34,242 INFO] Step 690/10000; acc: 44.0; ppl:  75.5; xent: 4.3; lr: 0.00238; sents:   12215; bsz: 2390/2491/153; 6055/6311 tok/s;   2483 sec;
[2024-07-30 10:44:06,455 INFO] Step 700/10000; acc: 44.5; ppl:  73.1; xent: 4.3; lr: 0.00236; sents:   12384; bsz: 2281/2493/155; 5665/6190 tok/s;   2515 sec;
[2024-07-30 10:44:41,820 INFO] valid stats calculation
                           took: 35.34853935241699 s.
[2024-07-30 10:44:41,835 INFO] Train perplexity: 220.146
[2024-07-30 10:44:41,835 INFO] Train accuracy: 31.9267
[2024-07-30 10:44:41,835 INFO] Sentences processed: 799282
[2024-07-30 10:44:41,835 INFO] Average bsz: 2279/2425/143
[2024-07-30 10:44:41,835 INFO] Validation perplexity: 194.336
[2024-07-30 10:44:41,835 INFO] Validation accuracy: 38.0521
[2024-07-30 10:44:41,836 INFO] Stalled patience: 13/15
[2024-07-30 10:45:13,650 INFO] Step 710/10000; acc: 43.9; ppl:  76.6; xent: 4.3; lr: 0.00234; sents:   11950; bsz: 2158/2410/149; 2569/2869 tok/s;   2583 sec;
[2024-07-30 10:45:44,542 INFO] Step 720/10000; acc: 43.6; ppl:  76.9; xent: 4.3; lr: 0.00233; sents:   11618; bsz: 2306/2410/145; 5971/6241 tok/s;   2614 sec;
[2024-07-30 10:46:16,231 INFO] Step 730/10000; acc: 44.1; ppl:  74.6; xent: 4.3; lr: 0.00231; sents:   12440; bsz: 2340/2465/156; 5907/6222 tok/s;   2645 sec;
[2024-07-30 10:46:47,977 INFO] Step 740/10000; acc: 43.7; ppl:  77.4; xent: 4.3; lr: 0.00230; sents:   11818; bsz: 2217/2487/148; 5587/6266 tok/s;   2677 sec;
[2024-07-30 10:47:19,717 INFO] Step 750/10000; acc: 44.1; ppl:  74.7; xent: 4.3; lr: 0.00228; sents:   11576; bsz: 2148/2395/145; 5414/6037 tok/s;   2709 sec;
[2024-07-30 10:47:50,993 INFO] Step 760/10000; acc: 41.2; ppl:  87.3; xent: 4.5; lr: 0.00227; sents:    9638; bsz: 2120/2352/120; 5423/6017 tok/s;   2740 sec;
[2024-07-30 10:48:22,533 INFO] Step 770/10000; acc: 43.0; ppl:  78.1; xent: 4.4; lr: 0.00225; sents:   10894; bsz: 2279/2416/136; 5780/6129 tok/s;   2772 sec;
[2024-07-30 10:48:54,939 INFO] Step 780/10000; acc: 43.9; ppl:  74.8; xent: 4.3; lr: 0.00224; sents:   11794; bsz: 2285/2465/147; 5640/6085 tok/s;   2804 sec;
[2024-07-30 10:49:27,969 INFO] Step 790/10000; acc: 43.0; ppl:  77.2; xent: 4.3; lr: 0.00222; sents:   10657; bsz: 2282/2355/133; 5526/5704 tok/s;   2837 sec;
[2024-07-30 10:50:00,520 INFO] Step 800/10000; acc: 42.9; ppl:  78.8; xent: 4.4; lr: 0.00221; sents:   10740; bsz: 2332/2383/134; 5730/5857 tok/s;   2870 sec;
[2024-07-30 10:50:35,756 INFO] valid stats calculation
                           took: 35.226675033569336 s.
[2024-07-30 10:50:35,768 INFO] Train perplexity: 193.323
[2024-07-30 10:50:35,769 INFO] Train accuracy: 33.3485
[2024-07-30 10:50:35,769 INFO] Sentences processed: 912407
[2024-07-30 10:50:35,769 INFO] Average bsz: 2275/2423/143
[2024-07-30 10:50:35,769 INFO] Validation perplexity: 163.485
[2024-07-30 10:50:35,769 INFO] Validation accuracy: 39.8245
[2024-07-30 10:50:35,769 INFO] Model is improving ppl: 194.334 --> 163.485.
[2024-07-30 10:50:35,769 INFO] Model is improving acc: 37.5013 --> 39.8245.
[2024-07-30 10:51:08,979 INFO] Step 810/10000; acc: 43.9; ppl:  74.8; xent: 4.3; lr: 0.00219; sents:   11642; bsz: 2299/2368/146; 2687/2768 tok/s;   2938 sec;
[2024-07-30 10:51:44,543 INFO] Step 820/10000; acc: 43.6; ppl:  75.2; xent: 4.3; lr: 0.00218; sents:   11418; bsz: 2301/2411/143; 5176/5423 tok/s;   2974 sec;
[2024-07-30 10:52:19,080 INFO] Step 830/10000; acc: 41.9; ppl:  82.1; xent: 4.4; lr: 0.00217; sents:   10175; bsz: 2282/2446/127; 5287/5666 tok/s;   3008 sec;
[2024-07-30 10:52:52,974 INFO] Step 840/10000; acc: 43.2; ppl:  76.7; xent: 4.3; lr: 0.00216; sents:   10742; bsz: 2353/2397/134; 5553/5658 tok/s;   3042 sec;
[2024-07-30 10:53:25,605 INFO] Step 850/10000; acc: 42.8; ppl:  78.6; xent: 4.4; lr: 0.00214; sents:   10096; bsz: 2180/2276/126; 5345/5580 tok/s;   3075 sec;
[2024-07-30 10:53:58,058 INFO] Step 860/10000; acc: 45.6; ppl:  67.4; xent: 4.2; lr: 0.00213; sents:   12275; bsz: 2272/2453/153; 5600/6046 tok/s;   3107 sec;
[2024-07-30 10:54:31,939 INFO] Step 870/10000; acc: 45.3; ppl:  68.1; xent: 4.2; lr: 0.00212; sents:   11756; bsz: 2276/2385/147; 5375/5632 tok/s;   3141 sec;
[2024-07-30 10:55:06,834 INFO] Step 880/10000; acc: 43.4; ppl:  75.4; xent: 4.3; lr: 0.00211; sents:   10722; bsz: 2204/2405/134; 5054/5515 tok/s;   3176 sec;
[2024-07-30 10:55:40,086 INFO] Step 890/10000; acc: 43.3; ppl:  74.7; xent: 4.3; lr: 0.00209; sents:   10398; bsz: 2351/2428/130; 5657/5841 tok/s;   3209 sec;
[2024-07-30 10:56:13,856 INFO] Step 900/10000; acc: 46.0; ppl:  65.6; xent: 4.2; lr: 0.00208; sents:   12578; bsz: 2273/2484/157; 5385/5884 tok/s;   3243 sec;
[2024-07-30 10:56:48,622 INFO] valid stats calculation
                           took: 34.750027656555176 s.
[2024-07-30 10:56:48,636 INFO] Train perplexity: 173.787
[2024-07-30 10:56:48,636 INFO] Train accuracy: 34.5136
[2024-07-30 10:56:48,636 INFO] Sentences processed: 1.02421e+06
[2024-07-30 10:56:48,636 INFO] Average bsz: 2275/2421/142
[2024-07-30 10:56:48,636 INFO] Validation perplexity: 147.964
[2024-07-30 10:56:48,636 INFO] Validation accuracy: 41.2488
[2024-07-30 10:56:48,636 INFO] Model is improving ppl: 163.485 --> 147.964.
[2024-07-30 10:56:48,637 INFO] Model is improving acc: 39.8245 --> 41.2488.
[2024-07-30 10:57:20,946 INFO] Step 910/10000; acc: 45.0; ppl:  69.5; xent: 4.2; lr: 0.00207; sents:   11994; bsz: 2407/2484/150; 2870/2962 tok/s;   3310 sec;
[2024-07-30 10:57:53,673 INFO] Step 920/10000; acc: 45.0; ppl:  69.3; xent: 4.2; lr: 0.00206; sents:   11859; bsz: 2264/2485/148; 5534/6075 tok/s;   3343 sec;
[2024-07-30 10:58:26,241 INFO] Step 930/10000; acc: 44.1; ppl:  73.3; xent: 4.3; lr: 0.00205; sents:   11115; bsz: 2261/2386/139; 5554/5860 tok/s;   3375 sec;
[2024-07-30 10:59:00,521 INFO] Step 940/10000; acc: 43.1; ppl:  74.2; xent: 4.3; lr: 0.00204; sents:    9802; bsz: 2323/2332/123; 5421/5441 tok/s;   3410 sec;
[2024-07-30 10:59:35,013 INFO] Step 950/10000; acc: 44.9; ppl:  67.4; xent: 4.2; lr: 0.00203; sents:   11146; bsz: 2337/2458/139; 5420/5702 tok/s;   3444 sec;
[2024-07-30 11:00:08,749 INFO] Step 960/10000; acc: 46.0; ppl:  65.5; xent: 4.2; lr: 0.00202; sents:   12287; bsz: 2253/2420/154; 5342/5739 tok/s;   3478 sec;
[2024-07-30 11:00:41,815 INFO] Step 970/10000; acc: 44.3; ppl:  72.0; xent: 4.3; lr: 0.00201; sents:   10833; bsz: 2173/2381/135; 5257/5760 tok/s;   3511 sec;
[2024-07-30 11:01:14,398 INFO] Step 980/10000; acc: 46.8; ppl:  63.0; xent: 4.1; lr: 0.00200; sents:   12680; bsz: 2300/2499/158; 5646/6136 tok/s;   3543 sec;
[2024-07-30 11:01:46,466 INFO] Step 990/10000; acc: 46.0; ppl:  65.6; xent: 4.2; lr: 0.00199; sents:   12110; bsz: 2224/2415/151; 5548/6024 tok/s;   3575 sec;
[2024-07-30 11:02:20,533 INFO] Step 1000/10000; acc: 44.6; ppl:  70.6; xent: 4.3; lr: 0.00198; sents:   10762; bsz: 2155/2294/135; 5061/5387 tok/s;   3610 sec;
[2024-07-30 11:02:55,012 INFO] valid stats calculation
                           took: 34.469671964645386 s.
[2024-07-30 11:02:55,024 INFO] Train perplexity: 158.462
[2024-07-30 11:02:55,024 INFO] Train accuracy: 35.5604
[2024-07-30 11:02:55,024 INFO] Sentences processed: 1.1388e+06
[2024-07-30 11:02:55,024 INFO] Average bsz: 2275/2421/142
[2024-07-30 11:02:55,024 INFO] Validation perplexity: 138.221
[2024-07-30 11:02:55,024 INFO] Validation accuracy: 41.6915
[2024-07-30 11:02:55,024 INFO] Model is improving ppl: 147.964 --> 138.221.
[2024-07-30 11:02:55,024 INFO] Model is improving acc: 41.2488 --> 41.6915.
[2024-07-30 11:02:55,229 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V18_step_1000.pt
[2024-07-30 11:03:36,453 INFO] Step 1010/10000; acc: 45.5; ppl:  66.0; xent: 4.2; lr: 0.00197; sents:   11312; bsz: 2199/2410/141; 2317/2540 tok/s;   3685 sec;
[2024-07-30 11:04:08,000 INFO] Step 1020/10000; acc: 45.9; ppl:  62.9; xent: 4.1; lr: 0.00196; sents:   11504; bsz: 2288/2471/144; 5802/6267 tok/s;   3717 sec;
[2024-07-30 11:04:42,174 INFO] Step 1030/10000; acc: 45.2; ppl:  65.4; xent: 4.2; lr: 0.00195; sents:   10863; bsz: 2234/2373/136; 5229/5555 tok/s;   3751 sec;
[2024-07-30 11:05:16,988 INFO] Step 1040/10000; acc: 44.7; ppl:  66.4; xent: 4.2; lr: 0.00194; sents:   10373; bsz: 2325/2484/130; 5342/5707 tok/s;   3786 sec;
[2024-07-30 11:05:50,847 INFO] Step 1050/10000; acc: 46.4; ppl:  61.5; xent: 4.1; lr: 0.00193; sents:   11515; bsz: 2265/2439/144; 5351/5763 tok/s;   3820 sec;
[2024-07-30 11:06:25,141 INFO] Step 1060/10000; acc: 45.5; ppl:  65.0; xent: 4.2; lr: 0.00192; sents:   10965; bsz: 2253/2410/137; 5256/5622 tok/s;   3854 sec;
[2024-07-30 11:06:57,739 INFO] Step 1070/10000; acc: 46.3; ppl:  62.4; xent: 4.1; lr: 0.00191; sents:   11224; bsz: 2132/2317/140; 5232/5686 tok/s;   3887 sec;
[2024-07-30 11:07:30,421 INFO] Step 1080/10000; acc: 45.9; ppl:  64.9; xent: 4.2; lr: 0.00190; sents:   11332; bsz: 2144/2347/142; 5249/5746 tok/s;   3919 sec;
[2024-07-30 11:08:02,303 INFO] Step 1090/10000; acc: 47.0; ppl:  60.7; xent: 4.1; lr: 0.00189; sents:   12297; bsz: 2249/2462/154; 5643/6179 tok/s;   3951 sec;
[2024-07-30 11:08:34,381 INFO] Step 1100/10000; acc: 46.8; ppl:  61.8; xent: 4.1; lr: 0.00188; sents:   11881; bsz: 2296/2353/149; 5725/5868 tok/s;   3983 sec;
[2024-07-30 11:09:08,805 INFO] valid stats calculation
                           took: 34.414464473724365 s.
[2024-07-30 11:09:08,818 INFO] Train perplexity: 145.924
[2024-07-30 11:09:08,818 INFO] Train accuracy: 36.4969
[2024-07-30 11:09:08,819 INFO] Sentences processed: 1.25206e+06
[2024-07-30 11:09:08,819 INFO] Average bsz: 2271/2420/142
[2024-07-30 11:09:08,819 INFO] Validation perplexity: 139.476
[2024-07-30 11:09:08,819 INFO] Validation accuracy: 41.6092
[2024-07-30 11:09:08,819 INFO] Decreasing patience: 14/15
[2024-07-30 11:09:40,741 INFO] Step 1110/10000; acc: 48.9; ppl:  54.5; xent: 4.0; lr: 0.00188; sents:   13054; bsz: 2374/2502/163; 2862/3017 tok/s;   4050 sec;
[2024-07-30 11:10:12,774 INFO] Step 1120/10000; acc: 45.9; ppl:  63.0; xent: 4.1; lr: 0.00187; sents:   10866; bsz: 2250/2454/136; 5619/6130 tok/s;   4082 sec;
[2024-07-30 11:10:46,023 INFO] Step 1130/10000; acc: 46.2; ppl:  61.8; xent: 4.1; lr: 0.00186; sents:   10910; bsz: 2263/2449/136; 5445/5893 tok/s;   4115 sec;
[2024-07-30 11:11:17,929 INFO] Step 1140/10000; acc: 48.1; ppl:  56.3; xent: 4.0; lr: 0.00185; sents:   12314; bsz: 2266/2370/154; 5681/5942 tok/s;   4147 sec;
[2024-07-30 11:11:49,141 INFO] Step 1150/10000; acc: 45.2; ppl:  65.1; xent: 4.2; lr: 0.00184; sents:   10295; bsz: 2289/2428/129; 5867/6223 tok/s;   4178 sec;
[2024-07-30 11:12:20,889 INFO] Step 1160/10000; acc: 47.3; ppl:  58.5; xent: 4.1; lr: 0.00183; sents:   11965; bsz: 2331/2487/150; 5874/6266 tok/s;   4210 sec;
[2024-07-30 11:12:52,680 INFO] Step 1170/10000; acc: 45.9; ppl:  62.5; xent: 4.1; lr: 0.00183; sents:   10638; bsz: 2345/2462/133; 5902/6196 tok/s;   4242 sec;
[2024-07-30 11:13:25,843 INFO] Step 1180/10000; acc: 47.0; ppl:  59.5; xent: 4.1; lr: 0.00182; sents:   11345; bsz: 2341/2404/142; 5648/5799 tok/s;   4275 sec;
[2024-07-30 11:14:00,901 INFO] Step 1190/10000; acc: 47.1; ppl:  58.9; xent: 4.1; lr: 0.00181; sents:   11688; bsz: 2284/2388/146; 5211/5450 tok/s;   4310 sec;
[2024-07-30 11:14:34,644 INFO] Step 1200/10000; acc: 49.6; ppl:  52.6; xent: 4.0; lr: 0.00180; sents:   13744; bsz: 2332/2438/172; 5528/5779 tok/s;   4344 sec;
[2024-07-30 11:15:07,978 INFO] valid stats calculation
                           took: 33.322115421295166 s.
[2024-07-30 11:15:07,992 INFO] Train perplexity: 135.273
[2024-07-30 11:15:07,992 INFO] Train accuracy: 37.3893
[2024-07-30 11:15:07,992 INFO] Sentences processed: 1.36888e+06
[2024-07-30 11:15:07,993 INFO] Average bsz: 2274/2421/143
[2024-07-30 11:15:07,993 INFO] Validation perplexity: 143.362
[2024-07-30 11:15:07,993 INFO] Validation accuracy: 41.2849
[2024-07-30 11:15:07,993 INFO] Decreasing patience: 13/15
[2024-07-30 11:15:40,796 INFO] Step 1210/10000; acc: 48.3; ppl:  56.3; xent: 4.0; lr: 0.00180; sents:   12473; bsz: 2289/2424/156; 2768/2932 tok/s;   4410 sec;
[2024-07-30 11:16:14,421 INFO] Step 1220/10000; acc: 47.1; ppl:  58.6; xent: 4.1; lr: 0.00179; sents:   11960; bsz: 2325/2516/150; 5533/5986 tok/s;   4443 sec;
[2024-07-30 11:16:49,014 INFO] Step 1230/10000; acc: 46.4; ppl:  60.9; xent: 4.1; lr: 0.00178; sents:   11032; bsz: 2279/2432/138; 5270/5623 tok/s;   4478 sec;
[2024-07-30 11:17:22,636 INFO] Step 1240/10000; acc: 47.5; ppl:  58.6; xent: 4.1; lr: 0.00177; sents:   11690; bsz: 2205/2386/146; 5247/5677 tok/s;   4512 sec;
[2024-07-30 11:17:56,726 INFO] Step 1250/10000; acc: 48.4; ppl:  55.0; xent: 4.0; lr: 0.00177; sents:   12677; bsz: 2376/2581/158; 5577/6056 tok/s;   4546 sec;
[2024-07-30 11:18:29,568 INFO] Step 1260/10000; acc: 46.0; ppl:  62.3; xent: 4.1; lr: 0.00176; sents:   10521; bsz: 2345/2439/132; 5711/5941 tok/s;   4579 sec;
[2024-07-30 11:19:03,650 INFO] Step 1270/10000; acc: 46.2; ppl:  62.8; xent: 4.1; lr: 0.00175; sents:   10829; bsz: 2107/2355/135; 4945/5529 tok/s;   4613 sec;
[2024-07-30 11:19:38,053 INFO] Step 1280/10000; acc: 46.7; ppl:  60.7; xent: 4.1; lr: 0.00175; sents:   11094; bsz: 2253/2417/139; 5239/5621 tok/s;   4647 sec;
[2024-07-30 11:20:11,517 INFO] Step 1290/10000; acc: 46.1; ppl:  62.1; xent: 4.1; lr: 0.00174; sents:   10812; bsz: 2320/2482/135; 5545/5933 tok/s;   4681 sec;
[2024-07-30 11:20:45,156 INFO] Step 1300/10000; acc: 47.1; ppl:  59.1; xent: 4.1; lr: 0.00173; sents:   10939; bsz: 2234/2368/137; 5312/5632 tok/s;   4714 sec;
[2024-07-30 11:21:20,069 INFO] valid stats calculation
                           took: 34.89922738075256 s.
[2024-07-30 11:21:20,076 INFO] Train perplexity: 126.943
[2024-07-30 11:21:20,076 INFO] Train accuracy: 38.1315
[2024-07-30 11:21:20,076 INFO] Sentences processed: 1.48291e+06
[2024-07-30 11:21:20,076 INFO] Average bsz: 2274/2423/143
[2024-07-30 11:21:20,077 INFO] Validation perplexity: 131.806
[2024-07-30 11:21:20,077 INFO] Validation accuracy: 42.2012
[2024-07-30 11:21:20,077 INFO] Model is improving ppl: 138.221 --> 131.806.
[2024-07-30 11:21:20,077 INFO] Model is improving acc: 41.6915 --> 42.2012.
[2024-07-30 11:21:54,396 INFO] Step 1310/10000; acc: 46.4; ppl:  60.4; xent: 4.1; lr: 0.00173; sents:   10443; bsz: 2260/2415/131; 2611/2791 tok/s;   4783 sec;
[2024-07-30 11:22:27,613 INFO] Step 1320/10000; acc: 46.0; ppl:  62.3; xent: 4.1; lr: 0.00172; sents:   10655; bsz: 2363/2433/133; 5692/5860 tok/s;   4817 sec;
[2024-07-30 11:23:00,586 INFO] Step 1330/10000; acc: 46.6; ppl:  60.2; xent: 4.1; lr: 0.00171; sents:   10458; bsz: 2249/2350/131; 5458/5701 tok/s;   4850 sec;
[2024-07-30 11:23:34,778 INFO] Step 1340/10000; acc: 45.8; ppl:  62.8; xent: 4.1; lr: 0.00171; sents:   10312; bsz: 2239/2336/129; 5238/5467 tok/s;   4884 sec;
[2024-07-30 11:24:07,264 INFO] Step 1350/10000; acc: 46.7; ppl:  60.2; xent: 4.1; lr: 0.00170; sents:   10694; bsz: 2215/2352/134; 5455/5792 tok/s;   4916 sec;
[2024-07-30 11:24:41,057 INFO] Step 1360/10000; acc: 47.8; ppl:  56.0; xent: 4.0; lr: 0.00169; sents:   11996; bsz: 2317/2552/150; 5485/6041 tok/s;   4950 sec;
[2024-07-30 11:25:15,084 INFO] Step 1370/10000; acc: 45.8; ppl:  63.3; xent: 4.1; lr: 0.00169; sents:    9754; bsz: 2109/2328/122; 4958/5474 tok/s;   4984 sec;
[2024-07-30 11:25:49,282 INFO] Step 1380/10000; acc: 47.1; ppl:  57.7; xent: 4.1; lr: 0.00168; sents:   10342; bsz: 2277/2390/129; 5326/5590 tok/s;   5018 sec;
[2024-07-30 11:26:23,563 INFO] Step 1390/10000; acc: 48.5; ppl:  53.7; xent: 4.0; lr: 0.00168; sents:   11900; bsz: 2415/2507/149; 5636/5851 tok/s;   5053 sec;
[2024-07-30 11:26:56,565 INFO] Step 1400/10000; acc: 48.8; ppl:  53.0; xent: 4.0; lr: 0.00167; sents:   12110; bsz: 2383/2442/151; 5777/5920 tok/s;   5086 sec;
[2024-07-30 11:27:31,185 INFO] valid stats calculation
                           took: 34.603540897369385 s.
[2024-07-30 11:27:31,199 INFO] Train perplexity: 120.178
[2024-07-30 11:27:31,199 INFO] Train accuracy: 38.7594
[2024-07-30 11:27:31,199 INFO] Sentences processed: 1.59157e+06
[2024-07-30 11:27:31,199 INFO] Average bsz: 2275/2422/142
[2024-07-30 11:27:31,199 INFO] Validation perplexity: 125.526
[2024-07-30 11:27:31,199 INFO] Validation accuracy: 42.6284
[2024-07-30 11:27:31,199 INFO] Model is improving ppl: 131.806 --> 125.526.
[2024-07-30 11:27:31,200 INFO] Model is improving acc: 42.2012 --> 42.6284.
[2024-07-30 11:28:03,618 INFO] Step 1410/10000; acc: 47.8; ppl:  57.2; xent: 4.0; lr: 0.00166; sents:   11394; bsz: 2294/2378/142; 2737/2837 tok/s;   5153 sec;
[2024-07-30 11:28:36,647 INFO] Step 1420/10000; acc: 47.2; ppl:  57.8; xent: 4.1; lr: 0.00166; sents:   10811; bsz: 2265/2415/135; 5486/5850 tok/s;   5186 sec;
[2024-07-30 11:29:09,063 INFO] Step 1430/10000; acc: 47.4; ppl:  57.3; xent: 4.0; lr: 0.00165; sents:   11161; bsz: 2250/2357/140; 5552/5816 tok/s;   5218 sec;
[2024-07-30 11:29:43,239 INFO] Step 1440/10000; acc: 49.2; ppl:  52.6; xent: 4.0; lr: 0.00165; sents:   12476; bsz: 2305/2364/156; 5396/5535 tok/s;   5252 sec;
[2024-07-30 11:30:17,769 INFO] Step 1450/10000; acc: 49.2; ppl:  53.1; xent: 4.0; lr: 0.00164; sents:   13310; bsz: 2353/2545/166; 5453/5895 tok/s;   5287 sec;
[2024-07-30 11:30:52,285 INFO] Step 1460/10000; acc: 48.8; ppl:  53.9; xent: 4.0; lr: 0.00164; sents:   12491; bsz: 2273/2512/156; 5267/5823 tok/s;   5321 sec;
[2024-07-30 11:31:25,757 INFO] Step 1470/10000; acc: 47.8; ppl:  56.8; xent: 4.0; lr: 0.00163; sents:   11543; bsz: 2289/2408/144; 5471/5756 tok/s;   5355 sec;
[2024-07-30 11:31:59,481 INFO] Step 1480/10000; acc: 46.5; ppl:  60.8; xent: 4.1; lr: 0.00162; sents:   10480; bsz: 2175/2306/131; 5160/5469 tok/s;   5388 sec;
[2024-07-30 11:32:34,332 INFO] Step 1490/10000; acc: 47.4; ppl:  58.0; xent: 4.1; lr: 0.00162; sents:   11257; bsz: 2143/2404/141; 4919/5518 tok/s;   5423 sec;
[2024-07-30 11:33:07,833 INFO] Step 1500/10000; acc: 47.6; ppl:  57.7; xent: 4.1; lr: 0.00161; sents:   10842; bsz: 2239/2322/136; 5347/5546 tok/s;   5457 sec;
[2024-07-30 11:33:41,961 INFO] valid stats calculation
                           took: 34.11175322532654 s.
[2024-07-30 11:33:41,975 INFO] Train perplexity: 114.313
[2024-07-30 11:33:41,975 INFO] Train accuracy: 39.365
[2024-07-30 11:33:41,975 INFO] Sentences processed: 1.70734e+06
[2024-07-30 11:33:41,975 INFO] Average bsz: 2274/2420/142
[2024-07-30 11:33:41,975 INFO] Validation perplexity: 119.473
[2024-07-30 11:33:41,975 INFO] Validation accuracy: 43.1175
[2024-07-30 11:33:41,975 INFO] Model is improving ppl: 125.526 --> 119.473.
[2024-07-30 11:33:41,976 INFO] Model is improving acc: 42.6284 --> 43.1175.
[2024-07-30 11:33:42,173 INFO] Saving checkpoint /root/zindi-nmt/models/nllb-200-600M-zindi_V18_step_1500.pt
[2024-07-30 11:34:24,740 INFO] Step 1510/10000; acc: 48.5; ppl:  54.0; xent: 4.0; lr: 0.00161; sents:   11533; bsz: 2222/2423/144; 2312/2521 tok/s;   5534 sec;
[2024-07-30 11:34:58,801 INFO] Step 1520/10000; acc: 47.2; ppl:  56.7; xent: 4.0; lr: 0.00160; sents:   10485; bsz: 2248/2446/131; 5281/5745 tok/s;   5568 sec;
