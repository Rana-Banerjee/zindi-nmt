{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-30 09:58:48,886 INFO] Parsed 2 corpora from -data.\n",
      "[2024-07-30 09:58:48,886 INFO] Loading checkpoint from /root/zindi-nmt/models/nllb-200-600M-onmt.pt\n",
      "[2024-07-30 09:58:50,689 WARNING] configured transforms is different from checkpoint: +{'suffix', 'prefix', 'sentencepiece'}\n",
      "[2024-07-30 09:58:50,690 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.\n",
      "[2024-07-30 09:58:51,350 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']\n",
      "[2024-07-30 09:58:51,351 INFO] The decoder start token is: </s>\n",
      "[2024-07-30 09:58:51,397 INFO] Updating checkpoint vocabulary with new vocabulary\n",
      "[2024-07-30 09:58:51,399 INFO] Get special vocabs from Transforms: {'src': ['</s>', 'dyu_Latn'], 'tgt': ['', 'fra_Latn']}.\n",
      "[2024-07-30 09:58:52,122 INFO] The first 10 tokens of the vocabs are:['<s>', '<blank>', '</s>', '<unk>', 'an', '▁n', '▁m', '▁t', '▁k', '▁a']\n",
      "[2024-07-30 09:58:52,124 INFO] The decoder start token is: </s>\n",
      "[2024-07-30 09:58:52,189 INFO] Over-ride model option set to true - use with care\n",
      "[2024-07-30 09:58:52,190 INFO] Option: config , value: /root/zindi-nmt/models/config2.yaml overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: data , value: {'corpus_1': {'path_src': '/root/zindi-nmt/data/master_train_dyu.txt', 'path_tgt': '/root/zindi-nmt/data/master_train_fra.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}, 'valid': {'path_src': '/root/zindi-nmt/data/dyu_val_V1.txt', 'path_tgt': '/root/zindi-nmt/data/fra_val_V1.txt', 'transforms': ['sentencepiece', 'prefix', 'suffix', 'filtertoolong'], 'weight': 10, 'src_prefix': 'dyu_Latn', 'tgt_prefix': 'fra_Latn', 'src_suffix': '</s>', 'tgt_suffix': '', 'path_align': None}} overriding model: {}\n",
      "[2024-07-30 09:58:52,190 INFO] Option: skip_empty_level , value: warning overriding model: silent\n",
      "[2024-07-30 09:58:52,190 INFO] Option: save_data , value: /root/zindi-nmt/train/ overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: src_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: tgt_vocab , value: /root/zindi-nmt/models/dictionary.txt overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: src_seq_length , value: 130 overriding model: 150\n",
      "[2024-07-30 09:58:52,190 INFO] Option: tgt_seq_length , value: 192 overriding model: 150\n",
      "[2024-07-30 09:58:52,190 INFO] Option: src_prefix , value: dyu_Latn overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: tgt_prefix , value: fra_Latn overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: src_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: \n",
      "[2024-07-30 09:58:52,190 INFO] Option: tgt_subword_model , value: /root/zindi-nmt/models/flores200_sacrebleu_tokenizer_spm.model overriding model: \n",
      "[2024-07-30 09:58:52,191 INFO] Option: update_vocab , value: True overriding model: False\n",
      "[2024-07-30 09:58:52,191 INFO] Option: heads , value: 8 overriding model: 16\n",
      "[2024-07-30 09:58:52,191 INFO] Option: transformer_ff , value: 1024 overriding model: 4096\n",
      "[2024-07-30 09:58:52,191 INFO] Option: bucket_size , value: 65536 overriding model: 262144\n",
      "[2024-07-30 09:58:52,191 INFO] Option: prefetch_factor , value: 200 overriding model: 400\n",
      "[2024-07-30 09:58:52,191 INFO] Option: save_model , value: /root/zindi-nmt/models/nllb-200-600M-zindi_V18 overriding model: nllb\n",
      "[2024-07-30 09:58:52,191 INFO] Option: save_checkpoint_steps , value: 500 overriding model: 5000\n",
      "[2024-07-30 09:58:52,191 INFO] Option: keep_checkpoint , value: 3 overriding model: 50\n",
      "[2024-07-30 09:58:52,191 INFO] Option: train_from , value: /root/zindi-nmt/models/nllb-200-600M-onmt.pt overriding model: \n",
      "[2024-07-30 09:58:52,191 INFO] Option: reset_optim , value: all overriding model: none\n",
      "[2024-07-30 09:58:52,191 INFO] Option: batch_size , value: 2048 overriding model: 8192\n",
      "[2024-07-30 09:58:52,192 INFO] Option: accum_count , value: [8, 8, 8] overriding model: [4]\n",
      "[2024-07-30 09:58:52,192 INFO] Option: accum_steps , value: [0, 15000, 30000] overriding model: [0]\n",
      "[2024-07-30 09:58:52,192 INFO] Option: valid_steps , value: 100 overriding model: 5000\n",
      "[2024-07-30 09:58:52,192 INFO] Option: valid_batch_size , value: 32 overriding model: 4096\n",
      "[2024-07-30 09:58:52,192 INFO] Option: train_steps , value: 10000 overriding model: 100000\n",
      "[2024-07-30 09:58:52,192 INFO] Option: early_stopping , value: 15 overriding model: 0\n",
      "[2024-07-30 09:58:52,192 INFO] Option: optim , value: adam overriding model: \n",
      "[2024-07-30 09:58:52,192 INFO] Option: dropout , value: [0.7, 0.7, 0.7] overriding model: [0.1]\n",
      "[2024-07-30 09:58:52,192 INFO] Option: attention_dropout , value: [0.5, 0.5, 0.5] overriding model: [0.1]\n",
      "[2024-07-30 09:58:52,192 INFO] Option: dropout_steps , value: [0, 15000, 30000] overriding model: [0]\n",
      "[2024-07-30 09:58:52,192 INFO] Option: average_decay , value: 0.0005 overriding model: 0.0\n",
      "[2024-07-30 09:58:52,192 INFO] Option: learning_rate , value: 2.0 overriding model: 5e-05\n",
      "[2024-07-30 09:58:52,192 INFO] Option: decay_method , value: noam overriding model: none\n",
      "[2024-07-30 09:58:52,192 INFO] Option: warmup_steps , value: 100 overriding model: 4000\n",
      "[2024-07-30 09:58:52,193 INFO] Option: log_file , value: /root/zindi-nmt/logs/trainnllb-200-600M-zindi_V18.log overriding model: \n",
      "[2024-07-30 09:58:52,193 INFO] Option: report_every , value: 10 overriding model: 100\n",
      "[2024-07-30 09:58:52,193 INFO] Option: _all_transform , value: {'sentencepiece', 'suffix', 'filtertoolong', 'prefix'} overriding model: {'filtertoolong'}\n",
      "[2024-07-30 09:58:52,193 INFO] Building model...\n",
      "[2024-07-30 09:59:00,637 INFO] Switching model to float32 for amp/apex_amp\n",
      "[2024-07-30 09:59:00,637 INFO] Non quantized layer compute is fp16\n",
      "[2024-07-30 09:59:00,637 INFO] Updating vocabulary embeddings with checkpoint embeddings\n",
      "[2024-07-30 09:59:03,465 INFO] src: 0 new tokens\n",
      "[2024-07-30 09:59:09,137 INFO] tgt: 0 new tokens\n",
      "[2024-07-30 09:59:10,271 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(256206, 1024, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.7, inplace=False)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.7, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.7, inplace=False)\n",
      "        )\n",
      "        (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(256206, 1024, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.7, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0-11): 12 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.7, inplace=False)\n",
      "          (dropout_2): Dropout(p=0.7, inplace=False)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_values): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (linear_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (final_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (layer_norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=1024, out_features=256206, bias=True)\n",
      ")\n",
      "[2024-07-30 09:59:10,283 INFO] encoder: 337977344\n",
      "[2024-07-30 09:59:10,283 INFO] decoder: 126283982\n",
      "[2024-07-30 09:59:10,283 INFO] * number of parameters: 464261326\n",
      "[2024-07-30 09:59:10,286 INFO] Trainable parameters = {'torch.float32': 464261326, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2024-07-30 09:59:10,287 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
      "[2024-07-30 09:59:10,287 INFO]  * src vocab size = 256206\n",
      "[2024-07-30 09:59:10,287 INFO]  * tgt vocab size = 256206\n",
      "[2024-07-30 09:59:11,276 INFO] Starting training on GPU: [0]\n",
      "[2024-07-30 09:59:11,276 INFO] Start training loop and validate every 100 steps...\n",
      "[2024-07-30 09:59:11,276 INFO] Scoring with: ['sentencepiece', 'prefix', 'suffix', 'filtertoolong']\n",
      "[2024-07-30 09:59:14,739 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-07-30 09:59:18,504 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-07-30 09:59:22,352 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n",
      "[2024-07-30 09:59:25,811 INFO] Weighted corpora loaded so far:\n",
      "\t\t\t* corpus_1: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "!onmt_train -config /root/zindi-nmt/models/config2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert OpenNMT-py model to CTranslate2\n",
    "!ct2-opennmt-py-converter \\\n",
    "    --model_path \"/root/projects/zindi/nmt_train/models/nllb-200-600M-zindi_V18_step_500.pt\" \\\n",
    "    --output_dir \"/root/projects/zindi/nmt_train/models/ct-optim/\" \\\n",
    "    --quantization int8 \\\n",
    "    --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 1010165734 Jul 27 12:25 /root/projects/zindi/nmt_train/models/ct-optim/model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -lrta /root/projects/zindi/nmt_train/models/ct-optim/model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt-train-93Y4ewMI-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
